<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai - learning methods - multiclass classification with binary classification algorithm </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 非深層学習系の学習手法</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：texのミスを修正
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>2クラス分類アルゴリズムを利用した多クラス分類</h1>
    <h2>1対多方式</h2>
        <div><div class="hidden_show">
    <h3>アルゴリズム</h3>
      <ul>
        <li>
        各入力\(x_i\)に対し何らかの関数\(f(\bm{x}_i)\)で\(y_i\)を求めた後、\(y_i\)の大小により2クラスに分類するような分類器をn個用意する。</li>
        <li> \(k (\le n)\)番目のクラスの分類における\(f(\bm{x}_i)\)を\(f^k(\bm{x}_i)\)とする</li>
        <li> \(\bm{x_i}\)は各クラスの出力\(f^k(\bm{x}_i)\)が最大となるクラス（下式)に割り当てる。</li>
      </ul>
      $$
        g(\bm{x}_i) = \underset{k\in c}{\operatorname{argmax}} f^k(\bm{x}_i)
      $$

    <h3>長所</h3>
      <ul>
        <li> 実現が用意。 </li>
      </ul>
    <h3>短所</h3>
      <ul>
        <li> 比較の妥当性が明確でない。</li>
        <li> クラス間でサンプル数が変わる場合は特に偏った値となりがち。</li>
      </ul>

        </div></div>
    <h2>1対1方式：投票による分類</h2>
        <div><div class="hidden_show">
    <h3>アルゴリズム</h3>
    <ul>
      <li>クラス\(p\)に分類される教師データの組を\(c_p\)と表す。</li>
      <li>教師データ\(c_{pq} = c_p \cup c_q\)によって学習された、\(p\)または\(q\)への分類を行える分類器\(f^{pq}(\bm{x})\)をそれぞれ用意する。</li>
      <li>(\(n\)個のクラスに分ける分類問題では、上記分類器が\(n(n-1)/2\)個必要となる。)</li>
      <li>この全ての分類機に\(\bm{x}_i\)を入力し、それぞれの分類機で選択されたクラスへの投票を繰り返す。</li>
      <li>一番投票されたクラスに\(\bm{x}_i\)を割り当てる。</li>
    </ul>
    <h3>長所</h3>
      <ul>
        <li> １回の学習にかかる訓練コストは小さい。 </li>
      </ul>
    <h3>短所</h3>
      <ul>
        <li> 全ての分類機の学習が必要。</li>
        <li> 投票数が同じの時に分類が不可能となる。</li>
      </ul>

        </div></div>
    <h2>1対1方式：非循環有向グラフによる方法</h2>
        <div><div class="hidden_show">
    <ul>
      <li>まず\(\bm{x_i}\)をクラス\(p\)と\(q\)を分ける分類器\(f^{pq}(\bm{x})\)により分類する。</li>
      <li>この時、\(p\)と分類された場合は\(q\)と分類されなかった、\(q\)に分類された場合は\(p\)と分類されなかったとみなす。</li>
      <li>次に、\(p\)と分類されなかった\(\bm{x_i}\)について、\((r (\ne p), s(\ne p, \ne r)\))の組の分類器にかける。</li>
      <li>この結果を\(p\)に分類されず、かつ\(r\)に分類されなかった（あるいは、\(p\)に分類されず、かつ\(s\)に分類されなかった)とみなす。</li>
      <li>これを繰り返し、最終的に\(t\)以外とは分類されなかったものを\(t\)と分類されたとみなす。</li>
    </ul>
    <h3>長所</h3>
      <ul>
        <li> 投票形式よりも分類にかかる計算量は少ない。</li>
      </ul>
    <h3>短所</h3>
      <ul>
        <li> 全ての分類機の学習が必要。</li>
      </ul>


        </div></div>
    <h2>1対1方式：ペアワイズカップリング</h2>
        <div><div class="hidden_show">
    <h3>略称、英語名など</h3>
      pairwise coupling

    <h3>アルゴリズム</h3>
      <ul>
        <li>クラス\(i\)と\(j\)のペアに対する決定関数を\(f^{ij}(\bm{x})\)とする。</li>
        <li>条件付き確率\(p(Y = i|Y \in {i, j}, X = x)\)を以下の式\(\hat{p}^{ij}(\bm{x})\)でモデル化する。 </li>
      </ul>
      $$
        \hat{p}^{ij}(\bm{x}) = \varsigma(Af^{ij}(\bm{x})i + B)
      $$
      $$
        \varsigma(x) = \frac{1}{1 + e^{-x}}
      $$
      ここで、\(A \in \mathbb{R}, B \in \mathbb{R}\)はパラメータ。
      <ul>
        <li>パラメータを最尤推定法などで推定する。</li>
      </ul>
      <ul>
        <li> 任意の\(\bm{x}\)について\(\hat{p}^{ij}(\bm{x})\)を求める。</li>
        <li> 次に\(p(Y = i, | X = x) := p^i \)を推定する。また、集合\({i,j}\)における\(p_i\)の真の確率を\(p_{ij}\)とすると以下の式が成り立つ。</li>
        $$
          p^{ij} = \frac{p^i}{p^i + p^j}
        $$
        <li> \(\hat{p}^{ij}(\bm{x})\)と上記右辺の値の差がなるべく小さくなるように\(p^i\)を推定する。</li>
        <li> 各クラスペアの事例で重み付けした<a class="local" href="02_math/05_probability_and_statistics/02_kullback_leibler_divergence.html">KLダイバージェンス</a>を和\(s(ij)\)を最小化するように\(p^i\)を推定する。</li>
        $$
        s(ij) = \sum_{i \ne j} (n_i + n_j) \mathrm{KL}(\hat{p}^{ij}\|p^{ij}) = \sum_{i \ne j} (n_i + n_j) \mathrm{KL}(\hat{p}^{ij}\|\frac{p^i}{p^i + p^j})
        $$
        ここで、\(n_i\)はクラス\(i\)の事例数、\(\mathrm{KL}(\hat{p}^{ij}\|p^{ij})\)は\(\hat{p}^{ij}\)と\(p^{ij})\)のKLダイバージェンス。
      </ul>
        </div></div>
    <h2>誤り訂正出力符号</h2>
        <div><div class="hidden_show">
    <h3>アルゴリズム</h3>
      <ul>
        <li>分類したいものを特徴(符号)に分け、特徴ベクトルを作る。</li>
        <li>例えば、0〜9の数字を分類するときは、そのまま分類するのでなく、縦線を含むか、横線を含むか、などの特徴毎に２項分類する。</li>
        <li>特徴毎に２項分類の学習器を作る。</li>
        <li>任意の\(bm{x}\)について、全ての学習器で分類を行う。</li>
        <li>ハミング距離などを利用し、分類結果と特徴ベクトルが一番近いものに分類する。ハミング距離を利用する場合は、以下の式となる。</li>
      </ul>
    $$
    g(x) = \underset{k\in c}{\operatorname{argmin}} \sum_{i \in [m]}( 1 - \text{sgn}{(S_{ki}}f^i(x)))
    $$
      <ul>
        <li>各特徴軸に関して、ペアワイズカップリングの考え方を取り入れることもできる</li>
      </ul>
        </div></div>
    <h2>参考文献、サイト</h2>
        <div><div class="hidden_show">
    <ol>
        <li><a href="https://www.kspub.co.jp/book/detail/1529069.html">竹内一郎, 烏山昌幸, "サポートベクトルマシン",2015,講談社サイエンティフィク</a></li>
    </ol>

    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="03_ai/04_learning_methods/24_common_technique.html">共通技術</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="03_ai/05_deep_learning/01_deep_learning.html">深層学習の基礎</a>
</div>
</main>
</body>
</html>
