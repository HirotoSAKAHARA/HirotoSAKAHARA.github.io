<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - other - natural language processing - natural language processing </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : その他 - 自然言語処理</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>自然言語処理 </h1>
  <h2>英語名、略称など</h2>
    <div><div class="hidden_show">
    
  NLP, natural language processing

    </div></div>
  <h2>基本用語</h2>
    <div><div class="hidden_show">
    
  <h3>トークン</h3>
自然言語解析における文章の最小単位となる文字や文字列
    <h3>タイプ</h3>
単語の種類を表す。
    <h3>文章</h3>
まとまった内容を表す文。
    <h3>文書</h3>
複数の文書からなるデータ１件分
    <h3>コーパス</h3>
自然言語の文書を大量に集めたデータで、国立国語研究所などの様々な機関がまとめたデータのほか、青空文庫などもコーパスと言える。
    <h3>シソーラス</h3>
単語の上位／下位関係、部分／全体関係、同義関係、類義関係などによって、単語を分類し、体系づけた類語辞典、辞書。
    <h3>形態素</h3>
意味を持つ最小の単位。
    <h3>単語</h3>
単一または複数の形態素から構成される小さな単位
    <h3>わかち書き</h3>
文章において語の区切りに空白を挟んで記述すること。
    <h3>表層</h3>
原文の記述のこと。
    <h3>原形</h3>
活用する前の記述の事。
    <h3>特徴</h3>
文章や文書から抽出された情報のこと。
    <h3>辞書</h3>
単語のリストの事。
    
    </div></div>
  <h2>形態素解析 <span class="small">Morphological Analysis</span></h2>
    <div><div class="hidden_show">
    
  文法ルールや解析辞書データに基づいて文章を単語に分割し、それぞれに品詞を付与する処理。様々なライブラリが提供されている。
   <dl>
    <dt><a href="http://taku910.github.io/mecab/">MeCab</a></dt>
    <dd>  京都大学情報学研究科−日本電信電話株式会社コミュニケーション科学基礎研究所 共同研究ユニットプロジェクトを通じて開発されたオープンソース形態素解析エンジンで、言語, 辞書,コーパスに依存しない汎用的な設計を 基本方針としている。</dd>
    <dt><a href="http://mocobeta.github.io/janome/">Janome</a></dt>
    <dd> Janome(蛇の目)はpure pythonで書かれた、辞書内包(mecab-ipadic-2.7.0-20070801)の形態素解析器で、依存ライブラリなしでインストールできるpythonライブラリ<dd>
    <dt><a href="https://chasen-legacy.osdn.jp/">Chasen</a></dt>
    <dd> 茶筅システムは、奈良先端科学技術大学院大学情報科学研究科自然言語処理学講座(松本研究室)が、広く自然言語処理研究に資するため無償のソフトウェアとして開発されたもの。</dd>
    <dt><a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN">JUMAN</a></dt>
    <dd>JUMANは、京都大学 大学院情報学研究科 知能情報学専攻 知能メディア講座 言語メディア分野（黒橋・河原・村脇研究室）によって、計算機による日本語の解析の研究を目指す多くの研究者に共通に使える形態素解析ツールを提供するために開発された。</dd>
  </dl>
   
    </div></div>
  <h2>Ngram</h2>
    <div><div class="hidden_show">
    
  与えられたテキストから、それぞれの語に対して隣接するn個の語の列を作って解析する方法。
    単語単位の場合もある。
    <ul>
<li>0-gram(ヌルグラム) 事前の単語を考慮しない</li>
<li>1-gram(ヌ二グラム) 1つ前の単語を考慮する。</li>
<li>2-gram(ヌ二グラム) 2つ前と2つまえの単語を考慮する。</li>
    </ul>

    統計的言語モデルからニューラルネットワーク言語モデル（リカレント．．．nnlm、rnnlm)

    </div></div>
  <h2>BOW <span class="small"> : Bags of Words</span></h2>
    <div><div class="hidden_show">
    
  文章中の各単語の分布を記録したベクトルとして表現する。ベクトル表現には以下のようなものがある。
    <h3>カウント表現</h3>
文章中の各単語の出現数を記録する。

    <h3>バイナリ表現</h3>
文章中に各単語が存在するかどうかを記録する。

    <h3>tf-idf表現</h3>
単語の出現頻度である\(tf\)(term frequency)と単語の希少性を示す\(idf\)(inverse document frequency:逆文書頻度)に基づいて計算される。
多くの文書に出現する一般的な言葉の重要度を下げる。
$$
tfidf_{i,j} = tf_{i,j}\cdot idf_i
$$
$$
tf_{i,j} = \frac{n_{i,j}}{\sum_k{n_{k,j}}}
$$
$$
idf_i = \log{\frac{|D|}{|{d: d \ni t_i}|}}
$$
ここで、\(n_{i_j}\)は文書\(d_j\)における単語\(t_i\)の出現回数、\(\sum_k{n_{k,j}}\)は文書\(d_j\)における全ての単語の出現回数の和。\(|D|\)は総文書数、\(|{d: d \ni t_i}|\)は単語\(t_i\)を含む文書数。

    <h3>one hot vector表現</h3>
犬、猫、うさぎのような数値でない値を取る説明変数において、犬:(1,0,0),猫(0,1,0),うさぎ(0,0,1)のような対応する次元が1でその他が0のベクトルで表す。
次元数は言葉の種類の総数となる。

    </div></div>
  <h2>類似度</h2>
    <div><div class="hidden_show">
    
  ２つのデータがどれだけ似ているかの指標
    <h3>cos類似度</h3>
ベクトルがどれだけ近いかを判断する指標
$$
cos(\vec{q}, \vec{d}) = \frac{\vec{q}\vec{d}}{|\vec{q}||\vec{d}|} = \frac{\sum_{i=1}q_id_i}{\sqrt{\sum_{i=1}q_i^2}\sqrt{\sum_{i=1}d_i^2}}
$$
    <h3>Jaccard類似度</h3>
集合動詞の類似度を判断する指標
    <h3>ピアソンの積率相関係数</h3>
    <q>2つの量的変数間の直線的関連の変数を表す係数。n組のデータ(\(x_1, y_1\)),(\(x_2, y_2\)) \(\cdots\) (\(x_n, y_n\))があり、それぞれの平均を\(\bar{x}, \bar{y}\)とした時、ピアソンの積率相関係数は以下の式で表される。</q>[1]
    $$
r_{xy} = \frac{{\displaystyle \sum_{i = 1}^n (x_i - \overline{x})
(y_i - \overline{y})}}{\sqrt{{\displaystyle \sum_{i = 1}^n 
(x_i - \overline{x})^2}} \sqrt{{\displaystyle \sum_{i = 1}^n 
(y_i - \overline{y})^2}}} = \frac{s_{xy}}{s_xs_y}
    $$
    <h3>ケンドールの順位相関係数</h3>
対応する２つの変量\(p_i(x_i, y_i)\)がある時、\(p_s\)と\(p_t\) (\(s \lt t\)) のときの、\(x_s\)と\(x_t\), \(y_s\)と\(y_t\)の大小の向きに関する類似度を計算する相関係数。
    <h3>スピアマンの順位相関係数</h3>
変量の値をその大きさ順に順位に変換し、あるいは順位しかわかっていないデータの順位により、ピアソンの相関係数を計算したもの。

    </div></div>
  <h2>単語文書行列<span class="small">term-document matrix</span></h2>
    <div><div class="hidden_show">
    
  文書に出現する単語の頻度を表形式で表したもの。tf, idfにより単語の重みを変えると、重みあり単語文書行列になる。

    </div></div>
  <h2>トピックモデル</h2>
    <div><div class="hidden_show">
    
  文章が複数のトピックで成り立っており、文書中の単語はそれらのトピックにより確率的に生成されていると仮定するモデル。
    <dl>
<dt>潜在的ディリクレ配分法</dt>
<dd>
トピックモデルにおいて、トピック分布にディリクレ分布を仮定し、ベイズ推定する手法。
ある現象が起きたときに次の現象が起きる確率であるガンマ分布を全ての単語で計算し、
確率密度関数を作成する。 適応的にパラメータが変わるノンパラメトリックモデルである。
</dd>
    </dl>

    </div></div>
  <h2>深層学習を使った自然言語処理</h2>
    <div><div class="hidden_show">
    
  深層学習を使わないベクトル表現では、ベクトルの次元が大きくなりすぎたり、意味を扱えなかったりするが、
深層学習を使うと、誤差逆伝播法により、わずか数百次元のベクトルで単語の意味を表現できるようになる。

    </div></div>
  <h2>word2vec</h2>
    <div><div class="hidden_show">
    
  "king" - "man" + "woman" = "Queen" <br>
単語埋め込み(Embedding)モデル。単語のベクトルを用いて、単語の足し算、引き算が可能となる。同じように文法も解ける。
BOWや単語文書行列では、それぞれの単語が0,1の入力層が数万個で表示されていたのが、まとめてベクトルとすると、
ベクトルが学習によって意味を表すようになってきた。
自分以外の周りの単語を予測するスキップグラムとその逆のCBOWで学習させると中間層に意味の表層ができる。

    </div></div>
  <h2>doc2vec</h2>
    <div><div class="hidden_show">
    
  word2vecを応用した文章をベクトル化する技術で、文の語順も特徴として考慮に入れることができる。

    </div></div>
  <h2>Sequence-to-Sequence <span class="small">: Encoder-Decoder、系列変換モデル</span></h2>
    <div><div class="hidden_show">
    
  ソースとターゲットで２つのRNN(LSTM)を使用する手法
ソースに系列（文章など)を入れ、ソース側で双方向リカレントニューラルネットワークを使用して、
束ねてベクトル情報に圧縮してターゲットに入力し、ターゲット側で新たな系列を生成する手法。

ソースからの複数の出力のうちのどれを使うのかをattentionとして出力側が決定する。
中間層の一工夫として、入力系列(文章)は逆順に入れると精度が上がることが報告されている。

    </div></div>
  <h2>Attention Mechanism <span class="small">注意機構</span></h2>
    <div><div class="hidden_show">
    
  Sequence-to-Sequenceモデルの２つのRNN間の情報伝達を改善する手法で、長いネットワークにおいて、
最初に入力された情報が復号化器まで伝播しづらくなるのを解決するために、直接的に入力情報を出力時に利用する。
具体的には、各単語に対して、各時刻で出力されたベクトルの重み付き平均を計算し、それを利用する。

    </div></div>
  <h2>参考文献、サイト</h2>
    <div><div class="hidden_show">
    
      <ol>
    <li><cite><a href="https://bellcurve.jp/statistics/glossary/1233.html">BellCurve, "ピアソンの積率相関係数", 統計WEB</a>
  </ol>
</div>

    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="6_failure_detection/1_failure_detection/1_failure_detection.html">異常検知</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="7_other/2_voice_recognition/1_voice_recognition.html">音声認識</a>
</div>
</main>
</body>
</html>
