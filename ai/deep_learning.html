<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - AI - deep learning</title>
<base href="../" />

<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
      macros: {
      x: "{\\times}",
      bm: ["{\\boldsymbol{#1}}",1],
      dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
      },
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->

<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - AI
      - deep learning
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h2>深層学習の基本</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-neural-network"/>
    <div class="hidden_show">
    脳の中の神経細胞を模擬したアルゴリズムで、
    シナプスの結合によりネットワークを形成した人口ニューロンが持つ結合加重ベクトルを学習によりを変化させ、
    様々な問題を解決できるようなモデルを作る。

   <h4>基本的なNeural Network</h4>
      <h5>ニューロン</h5>
        <a href="ai/index.html#perceptron">単純パーセプトロン</a>の出力に活性化関数と呼ばれる非線形の関数を加え、複雑な問題を解けるように工夫したもの。多層パーセプトロンに使用する。

        <div>
          <img src="out\ai\neuron.png" alt="ニューロン"/>
        </div>

      <h5>多層パーセプトロン</h5>
        ニューロンを組み合わせることで、複雑な問題を解けるよう改良したもの。
         <div>
          <img src="out\ai\multi_layer_perceptron.png" alt="ニューロン"/>
        </div>
      <h5>Deep Learning</h5>
        多層パーセプトロン等のニューラルネットワークの中で、３層以上のもの。
        入力層\(x\)、1層以上の中間層(隠れ層)\(h\)、出力層\(y\)からなる。
    
    <h4>学習の手順</h4>
    ニューラルネットワークの学習の手順を以下に示す。
    <div>
      <img src="out\ai\procedure.png" alt="手順"/>
    </div>
    <h4>課題</h4>  
      <h5>the gradient vanishing problem(勾配消失問題)</h5>
        誤差勾配の計算時には、各層の微分が行われ、かつ下層の勾配が上層に影響を与える。
        活性化関数で一般的であったシグモイド関数を使うと、一回の計算で微分値が1/4以下になるので、
        層を増やすと上層での勾配がなくなり、学習が行われない。

      <h5>過適合</h5>
        学習データに適合し、汎化性能が局所解に陥ることで、

      <h5>次元の呪い</h5>
        推定すべきパラメータが多くなると、次元が多くなって、パラメータ空間の探索が難しくなり、
        オーバーフィッティングなどが起きやすくなること。
      


    <h4>ブレイクスルー</h4>
      <h5>auto encoder(自己符号化器)</h5>
        3層のネットワークを考え、中間層のユニット数が入力層と出力層よりも小さく、かつ出力層で入力を再現できるような重み付けを行うこと。
        この時、中間層は入力層の次元を圧縮できたことになる。自己符号化器を多層に重ねることで勾配消失問題を解決し、ディープニューラルネットワークが可能となった。
        ただし現在では活性化関数の工夫により勾配消失問題を解決していることからあまり使われていない。
   

  </div></div>
      
  <h3>基本技術</h3>
    <div class="hidden_box">
    <label for="label-basics-of-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-basics-of-neural-network"/>
    <div class="hidden_show">

      
    
    <h4>活性化関数（基本）</h4>
    中間層や出力層で、入力を足し合わせたのちに通す関数。微分可能かつ非線形であることが重要。

      <h5>ロジスティックシグモイド関数</h5>
        入力は\((-\inf,\inf)\)出力は\((0,1)\)である関数。
        入力値が飽和する。出力が滑らかであることなどが特徴。
        微分値が\((0,0.25)\)しかとらず勾配消失の原因となった
        $$ f(u) = \frac{1}{1 + e^{-u}} $$
      
      <h5>ハイパータンジェント</h5>
        入力は\((-\inf,\inf)\)出力は\((-1, 1)\)である関数。
        入力値が飽和する。出力が滑らかであることなどが特徴。
        微分値は\((0,1.0)\)なの、シグモイドよりも勾配消失しにくいが、
        高々１なので積み重ねると勾配消失が発生する。
        $$ f(u) = \tanh(u) $$

      <h5>ReLU (Rectified Linear Unit 整流線型ユニット)</h5>
        簡単でかつ収束も早い。u = 0の時に少しだけ問題となる。
        $$
          f(u) = 
          \left\{
            \begin{array} {11}
              0 & (u \leq 0) \\
              u & (u > 0)
            \end{array} 
          \right.
        $$

    <h4>出力ユニット</h4>
      最終層のニューロンの出力を意味のあるものに変換する関数。
      　<h5>ロジスティックシグモイド関数</h5>
          0から1までの確率で出力される
      　<h5>Softmax関数<span class="small">ソフトマックス関数</span></h5>
          多クラス分類に使用する。複数の候補を取得する。それぞれのyは0～1になり、すべて足すと1になる(確率の要件を満たす)
          $$
            y_i = \frac{e^{x_i}}{\sum^n_{k=1}e^{x_k}} 
          $$
      　<h5>Gaussian Mixture Model (GMM、ガウス混合分布)</h5>
          ある点が複数のガウス分布のうちどこに属しているのかを確率的に表現する。
      　<h5>Radial Basis Function (RBF, 動径基底関数)</h5>
          ある点cからの距離のみに基づいて値が決まる関数である動径関数をcを変えて集め、それらの線形和をとる。

    <h4>cost function(コスト関数、loss function、損失関数、error function、誤差関数、objective function、目的関数)</h4>
      教師データと出力との差を定量的に示すための関数。
      <h5>二乗誤差</h5>
      出力と正解の各成分の差の二乗和による誤差

      <h5>クロスエントロピー誤差</h5>
      二乗誤差よりも収束がはるかに速い

    <h4>勾配降下</h4>
    　得られた誤差の値からニューロンの重みを最適化するための手法。各手法で利用される誤差勾配は誤差逆伝播法により求める
      <dl>
        <dt>バッチ学習法</dt>
        <dd></dd>
        <dt>オンライン学習法</dt>
        <dd></dd>
        <dt>確率的勾配降下法</dt>
        <dd>勾配のαを変えて更新幅を決める、ミニバッチを用いる</dd>
        <dt>モーメント法</dt>
        <dd>1時刻前の更新幅を利用して現在の更新幅を決める</dd>
        <dt>ニュートン法</dt>
        <dd>ニュートン法と同じくヘシアン（２次微分）を用いる最適化手法。ただし、ヘシアン作るのは難しい</dd>
        <dt>Adgrad法</dt>
        <dd>ヘシアンの近似として、ヘシアンの対角行列のみをつかう。</dd>
        <dt>Adadelta法</dt>
        <dd>ヘシアンの近似として、逐次自乗平均を使う。</dd>
        <dt>RMSprop法</dt>
        <dd>逐次平均ではなくて、過去の値に重みを増やす</dd>
        <dt>Adam 法</dt>
        <dd>モーメントも使う</dd>
      </dl>

    <h4>誤差逆伝播法による勾配の計算</h4>
      教師データと出力により得られた誤差の値から、各ニューロンの誤差勾配を求める手法。
      <h5>導出</h5>
        \(l-1\)層目の\(i\)番目のニューロンからの入力を受ける\(l\)層の\(j\)番目のニューロン\(w_{ji}^{(l)}\)の重みのコスト関数\(E_n\)に対する誤差勾配は、
        そのニューロンの出力に対する式ととらえると以下のように変形できる。
        $$
        \frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial u_j^{(l)}}\frac{\partial u_j^{(l)}}{\partial w_{ji}^{(l)}}
        $$
        また、その出力が\(l + 1\)番目の各ニューロンに与える影響を考えると、右辺第１項は以下のように式変形できる。
        $$
        \frac{\partial E_n}{\partial u_j^{(l)}} = \sum_k{\frac{\partial E_n}{\partial u_k^{(l+1)}}\frac{\partial u_k^{(l+1)}}{\partial u_j^{(l)}}}
        $$
        ここで、\( \delta_j^{(l)} \equiv \frac{\partial E_n}{\partial u_j^{(l)}} \) と置き、
        \(u_k^{(l+1)} = \sum_j{w_{kj}^{(l+1)}z_j^{(l)}} = \sum_j{w_{kj}^{(l+1)}f(u_j^{(l)})}\)の関係を使うと、上式は以下のように変形できる。 
        $$
        \delta_j^{(l)}　=　\sum_k{\delta_j^{(l+1)}(w_{kj}^{(l+1)}f'(u_j^{(l)}))}
        $$
        これは、次の層の全ニューロンの微分値が求まると、ニューロンの微分値が求まることを意味している。
        また、初めの式の右辺第２項目は\(u_j^{(l)} = \sum_i{w_{ji}^{(l)}z_i^{(l-1)}}\)の関係から簡単に以下の式で表される。
        $$
        \frac{\partial u_j^{(l)}}{\partial w_{ji}^{(l)}} = z_i^{(l-1)}
        $$
        よって、これらの式を出力層から順に利用していくことで、すべての重みに対する誤差勾配を求めることができる。
    </div></div>


  <h3>応用技術</h3>
    <div class="hidden_box">
    <label for="label-advanced-of-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-advanced-of-neural-network"/>
    <div class="hidden_show">

    <h4>活性化関数（応用）</h4>
      <h5>parametric ReLU</h5>
      $$
        f(u) = 
        \left\{
          \begin{array} {11}
            \alpha u & (u \leq 0) \\
            u & (u > 0)
          \end{array} 
        \right.
      $$

      <h5>leaky ReLU</h5>
      \(\alpha = 0.01\)のparametric ReLU
      <h5>maxout</h5>
      K個の異なるユニットが異なる重みとバイアスを持ち、それらの最大値を出力とする。
      性能は良いがパラメータ数が多いため、それほどよく使われるわけではない。
      $$
      u_{jk} = \sum_i{w_{jik}z_i}+b_{jk} (k = 1,...,K)
      f(u_j) = \max_{k=1,...,K}u_{jk}
      $$
      <h5>ソフトプラス</h5>
      ReLUの連続近似、ソフトプラスを微分するとシグモイド関数になる。
      $$
      f(u) = \log{(1 + e^u)}
      $$
    
      <h5>ELU</h5>
      ReLUに比べてあまり広く使われていないがReLUよりも高速だと言われる。

      $$
      f(u) = 
      \left\{
        \begin{array} {11}
            \alpha (e^u - 1) & (u \leq 0) \\
            u & (u > 0)
        \end{array} 
      \right.
      $$

      <h5>SELU</h5>
      ELUを\(\lambda\)倍する。

    <h4>過適合の緩和</h4>
    <h5>Dropout <span class="small"> : ドロップアウト</h5>
      途中のノードを確率的に無効化（ニューロンの重みを０に）することで、オーバーフィッティングを避ける。
      中間層だけでなく、入力層でもドロップアウトすることで性能が良くできる場合がある。
      RNNの場合、フィードフォワードの分はドロップアウトした方がよいが、時系列のドロップアウトはしない方がよい。

    <h5>バッチ正則化(2014)</h5>
        この技術を使うことで画像認識の誤り率が5%を切った。
        ミニバッチ毎に正則化を行う。
        初期化((例)ザビエルの初期化)が不要となった。また、学習係数を大きくすることができるようになった。場合によってはドロップアウト不要
        バッチ毎に以下の計算を行う。
        <pre> 正規化データ = (あるデータ　－　平均) / (標準偏差) </pre>

    <h5>スパースモデリング</h5>
      デシメーションと重み付けで不要な重みを0にする。
      劣決定系のモデルの中でスパース（０が多い）解を算出する。
     
  </div></div>

  <h3>具体的なアルゴリズム</h3>
    <div class="hidden_box">
    <label for="label-algorithm-of-nn">[+]　　　</label>
    <input type="checkbox" id="label-algorithm-of-nn"/>
    <div class="hidden_show">
    ユニット数・層数・層間の結合、カーネルの幅などにより、様々なネットワークがある。

     <h4>ネオコグニトロン</h4>
       ニューラルネットワークがメジャーになる前に考えられた構成。add-if silentと呼ばれる学習方法を用いる。
     <h4>LeNet</h4>
       畳み込みニューラルネットワークの起源。誤差逆伝播法を用いる。
     <h4>AlexNet(2012)</h4>
       224x224のRGB３原色の画像を認識, 2枚のGPUボードで実装(中は同じ)
     <h4>LuNet</h4>
       手書き文字の認識で使用される。
     <h4>GoogLeNet</h4>
     <h4>VGG</h4>
     <h4>ResNet(2015)</h4>
       層を飛ばして出力するSkip Connectができる。カーネルが3x3で同じ形で何度も繰り返す。
     <h4>ResNet16</h4>
     入力+conv64+conv64+maxpool+conv128+conv128+maxpool+conv256+conv256+maxpool+conv512+conv512+maxpool+FC4096(完全結合)+FC4096+FC1000+softmax

     <h4>DensNet(2018)</h4>
       複数の階層を飛ばすSkip Connectがある。大脳新皮質のつながりに近くなる。
    </div></div>

  <h3>DNNフレームワーク</h3>
    <div class="hidden_box">
    <label for="label-framework">[+]　　　</label>
    <input type="checkbox" id="label-framework"/>
    <div class="hidden_show">

    <dl>
      <dt>Theano</dt>
      <dd>pythonのDNNライブラリ。歴史的意義があるが、2017/11/15に1.0.0がでてから更新されていない。</dd>
      <dt>Caffe</dt>
      <dd> UCバークレーで開発された Deep Learning Framework。
           Caffe Framework modelsが開発され、他のフレームワークでも使用できる
      </dd>
      <dt>Torch</dt>
      <dd>Lua言語をベースとしたスクリプト言語の機械学習ライブラリ。2018年からは開発は行われていない</dd>
      <dt>pyTorch</dt>
      <dd>Pythonで使用できるTorch。2019年に開発された。</dd>
      <dt>chainer</dt>
      <dd>preferred network社が開発している和製フレームワーク。
        2019/12/05 preferred network社はPyTorchに研究開発基板を移行を発表。
        <a href="https://preferred.jp/ja/news/pr20191205/">https://preferred.jp/ja/news/pr20191205/</a></dd>
      <dt>TensorFlow</dt>
      <dd>googleが開発、圧倒的に人気がある</dd>
      <dt> keras </dt>
      <dd>tensor flow, theanoのフロントエンド。人気がある</dd>
      <dt>caffe2</dt>
      <dd>caffeの改良版</dd>
      <dt>TensorFlow.js</dt>
      <dd>TensorFlowのjava script版</dd>
    </dl>
  </div> </div>

<h3>用語</h3>
  <div class="hidden_box">
  <label for="label-words">[+]　　　</label>
  <input type="checkbox" id="label-words"/>
  <div class="hidden_show">
   
  <dl>
    <dt>Artificial Neural Network(ANN, 人口ニューラルネットワーク)</dt>
    <dd>コンピュータの中のニューラルネットワーク, BNNの対比</dd>
    <dt>Biologinal Neural Network(BNN, 生物学的ニューラルネットワーク)</dt>
    <dd>
    人間や動物が生物学的に持っているニューラルネットワーク </dd>
    <dt>Convolusional Neural Network(CNN, 畳み込みニューラルネットワーク)</dt>
    <dd> 「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク
    </dd>
    <dt>Deep Neural Network(DNN, 深層学習)</dt>
    <dd>入力層＋中間層＋出力層からなるニューラルネットのなかで、中間層が２層以上のもの。
    <dt>Feedforward Neural Network(順伝播型ニューラルネットワーク)</dt>
    <dd>層状に並べたユニットが隣接層間でのみ結合した構造をもち、情報が入力側から出力側へ一方向に伝播する。
        multi layer perceptron（多層パーセプトロン）とも呼ぶ。
    </dd>
    <dt>変分オートエンコーダ</dt>
    <dd>内容</dd>
  </dl>
  </div></div>


<h3>参考文献、サイト</h3>
  <dd>
  <dt> <cite><a href="https://pdfs.semanticscholar.org/8bdf/dc2c2777b395c086810c03a8cdeccc55c4db.pdf">Wolpert, D.H., Macready, W.G. "No Free Lunch Theorems for Search", Technical Report SFI-TR-95-02-010,1995, Santa Fe Institute </a></cite></dt>
  <dd>ノーフリーランチ定理</dd>
  <dt><cite><a href="http://bookclub.kodansha.co.jp/product?item=0000147653">岡谷貴之,"深層学習(機械学習プロフェッショナルシリーズ)", 2015, 講談社</a></cite></dt>
  <dd>深層学習の基礎を網羅している本。</dd>
  <dt><cite>Akiba, Takuya, Shuji Suzuki, and Keisuke Fukuda "Extremely Large Minibatch Sgd: Training Resnet-50 on Imagenet in 15 Miniutes", 2017, arXiv Preprint.</cite>
  </dt>
  <dd></dd>
  Yee Wyne Teh NIPS 2017

  <dt> [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) 再解釈論文 (2016)</dt>
  <dd> 変分オートエンコーダを再解釈した論文</dd>


<div class = "end_of_page_margin"></div>
<div class = "end_of_page">
<a class="prev" href="ai/applications.html" >前：各分野における応用技術</a>
<a class="upper" href="index.html" >上：ホーム</a>
<a class="next" href="ai/cnn.html">次：畳み込みニューラルネットワーク</a>
</div>

  </main>
</body>
</html>
