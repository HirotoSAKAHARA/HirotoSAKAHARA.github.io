<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai - newral network</title>
<base href="../" />


<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
      macros: {
      x: "{\\times}",
      bm: ["{\\boldsymbol{#1}}",1],
      dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
      },
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - 
    <a href="index.html">AI</a>
      - neural network 
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：RNN、LSTMの記述を追加
</div>

 <div class="navigation">
<ul>
  <li><a href="index.html">home</a></li>
  <li><a href="mobile_robot/index.html">移動ロボット</a></li>
  <li><a href="ai/index.html">AI</a></li>
  <li><a href="math/index.html">数学</a></li>
  <li><a href="other/index.html">その他</a></li>
</ul>
</div>

<!--*********************************本文*********************************-->

<h1>Deep Learning (深層学習)</h1>

<h2>Neural Network</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-neural-network"/>
    <div class="hidden_show">
    脳の中の神経細胞を模擬したアルゴリズムで、
    シナプスの結合によりネットワークを形成した人口ニューロンが持つ結合加重ベクトルを学習によりを変化させ、
    様々な問題を解決できるようなモデルを作る。

   <h4>基本的なNeural Network</h4>
      <h5>ニューロン</h5>
        <a href="ai/index.html#perceptron">単純パーセプトロン</a>の出力に活性化関数と呼ばれる非線形の関数を加え、複雑な問題を解けるように工夫したもの。多層パーセプトロンに使用する。

        <div>
          <img src="out\ai\neuron.png" alt="ニューロン"/>
        </div>

      <h5>多層パーセプトロン</h5>
        ニューロンを組み合わせることで、複雑な問題を解けるよう改良したもの。
         <div>
          <img src="out\ai\multi_layer_perceptron.png" alt="ニューロン"/>
        </div>
      <h5>Deep Learning</h5>
        多層パーセプトロン等のニューラルネットワークの中で、３層以上のもの。
        入力層\(x\)、1層以上の中間層(隠れ層)\(h\)、出力層\(y\)からなる。
    
    <h4>学習の手順</h4>
    ニューラルネットワークの学習の手順を以下に示す。
    <div>
      <img src="out\ai\procedure.png" alt="手順"/>
    </div>
    <h4>課題</h4>  
      <h5>the gradient vanishing problem(勾配消失問題)</h5>
        誤差勾配の計算時には、各層の微分が行われ、かつ下層の勾配が上層に影響を与える。
        活性化関数で一般的であったシグモイド関数を使うと、一回の計算で微分値が1/4以下になるので、
        層を増やすと上層での勾配がなくなり、学習が行われない。

      <h5>過適合</h5>
        学習データに適合し、汎化性能が局所解に陥ることで、

      <h5>次元の呪い</h5>
        推定すべきパラメータが多くなると、次元が多くなって、パラメータ空間の探索が難しくなり、
        オーバーフィッティングなどが起きやすくなること。
      


    <h4>ブレイクスルー</h4>
      <h5>auto encoder(自己符号化器)</h5>
        3層のネットワークを考え、中間層のユニット数が入力層と出力層よりも小さく、かつ出力層で入力を再現できるような重み付けを行うこと。
        この時、中間層は入力層の次元を圧縮できたことになる。自己符号化器を多層に重ねることで勾配消失問題を解決し、ディープニューラルネットワークが可能となった。
        ただし現在では活性化関数の工夫により勾配消失問題を解決していることからあまり使われていない。
   

  </div></div>
      
  <h3>基本技術</h3>
    <div class="hidden_box">
    <label for="label-basics-of-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-basics-of-neural-network"/>
    <div class="hidden_show">

      
    
    <h4>活性化関数（基本）</h4>
    中間層や出力層で、入力を足し合わせたのちに通す関数。微分可能かつ非線形であることが重要。

      <h5>ロジスティックシグモイド関数</h5>
        入力は\((-\inf,\inf)\)出力は\((0,1)\)である関数。
        入力値が飽和する。出力が滑らかであることなどが特徴。
        微分値が\((0,0.25)\)しかとらず勾配消失の原因となった
        $$ f(u) = \frac{1}{1 + e^{-u}} $$
      
      <h5>ハイパータンジェント</h5>
        入力は\((-\inf,\inf)\)出力は\((-1, 1)\)である関数。
        入力値が飽和する。出力が滑らかであることなどが特徴。
        微分値は\((0,1.0)\)なの、シグモイドよりも勾配消失しにくいが、
        高々１なので積み重ねると勾配消失が発生する。
        $$ f(u) = \tanh(u) $$

      <h5>ReLU (Rectified Linear Unit 整流線型ユニット)</h5>
        簡単でかつ収束も早い。u = 0の時に少しだけ問題となる。
        $$
          f(u) = 
          \left\{
            \begin{array} {11}
              0 & (u \leq 0) \\
              u & (u > 0)
            \end{array} 
          \right.
        $$

    <h4>出力ユニット</h4>
      最終層のニューロンの出力を意味のあるものに変換する関数。
      　<h5>ロジスティックシグモイド関数</h5>
          0から1までの確率で出力される
      　<h5>ソフトマックス関数</h5>
          多クラス分類に使用する。複数の候補を取得する。それぞれのyは0～1になり、すべて足すと1になる(確率の要件を満たす)
      　<h5>Gaussian Mixture Model (GMM、ガウス混合分布)</h5>
          ある点が複数のガウス分布のうちどこに属しているのかを確率的に表現する。
      　<h5>Radial Basis Function (RBF, 動径基底関数)</h5>
          ある点cからの距離のみに基づいて値が決まる関数である動径関数をcを変えて集め、それらの線形和をとる。

    <h4>cost function(コスト関数、loss function、損失関数、error function、誤差関数、objective function、目的関数)</h4>
      教師データと出力との差を定量的に示すための関数。
      <h5>二乗誤差</h5>
      出力と正解の各成分の差の二乗和による誤差

      <h5>クロスエントロピー誤差</h5>
      二乗誤差よりも収束がはるかに速い

    <h4>勾配降下</h4>
    　得られた誤差の値からニューロンの重みを最適化するための手法。各手法で利用される誤差勾配は誤差逆伝播法により求める
      <dl>
        <dt>バッチ学習法</dt>
        <dd></dd>
        <dt>オンライン学習法</dt>
        <dd></dd>
        <dt>確率的勾配降下法</dt>
        <dd>勾配のαを変えて更新幅を決める、ミニバッチを用いる</dd>
        <dt>モーメント法</dt>
        <dd>1時刻前の更新幅を利用して現在の更新幅を決める</dd>
        <dt>ニュートン法</dt>
        <dd>ニュートン法と同じくヘシアン（２次微分）を用いる最適化手法。ただし、ヘシアン作るのは難しい</dd>
        <dt>Adgrad法</dt>
        <dd>ヘシアンの近似として、ヘシアンの対角行列のみをつかう。</dd>
        <dt>Adadelta法</dt>
        <dd>ヘシアンの近似として、逐次自乗平均を使う。</dd>
        <dt>RMSprop法</dt>
        <dd>逐次平均ではなくて、過去の値に重みを増やす</dd>
        <dt>Adam 法</dt>
        <dd>モーメントも使う</dd>
      </dl>

    <h4>誤差逆伝播法による勾配の計算</h4>
      教師データと出力により得られた誤差の値から、各ニューロンの誤差勾配を求める手法。
      <h5>導出</h5>
        \(l-1\)層目の\(i\)番目のニューロンからの入力を受ける\(l\)層の\(j\)番目のニューロン\(w_{ji}^{(l)}\)の重みのコスト関数\(E_n\)に対する誤差勾配は、
        そのニューロンの出力に対する式ととらえると以下のように変形できる。
        $$
        \frac{\partial E_n}{\partial w_{ji}^{(l)}} = \frac{\partial E_n}{\partial u_j^{(l)}}\frac{\partial u_j^{(l)}}{\partial w_{ji}^{(l)}}
        $$
        また、その出力が\(l + 1\)番目の各ニューロンに与える影響を考えると、右辺第１項は以下のように式変形できる。
        $$
        \frac{\partial E_n}{\partial u_j^{(l)}} = \sum_k{\frac{\partial E_n}{\partial u_k^{(l+1)}}\frac{\partial u_k^{(l+1)}}{\partial u_j^{(l)}}}
        $$
        ここで、\( \delta_j^{(l)} \equiv \frac{\partial E_n}{\partial u_j^{(l)}} \) と置き、
        \(u_k^{(l+1)} = \sum_j{w_{kj}^{(l+1)}z_j^{(l)}} = \sum_j{w_{kj}^{(l+1)}f(u_j^{(l)})}\)の関係を使うと、上式は以下のように変形できる。 
        $$
        \delta_j^{(l)}　=　\sum_k{\delta_j^{(l+1)}(w_{kj}^{(l+1)}f'(u_j^{(l)}))}
        $$
        これは、次の層の全ニューロンの微分値が求まると、ニューロンの微分値が求まることを意味している。
        また、初めの式の右辺第２項目は\(u_j^{(l)} = \sum_i{w_{ji}^{(l)}z_i^{(l-1)}}\)の関係から簡単に以下の式で表される。
        $$
        \frac{\partial u_j^{(l)}}{\partial w_{ji}^{(l)}} = z_i^{(l-1)}
        $$
        よって、これらの式を出力層から順に利用していくことで、すべての重みに対する誤差勾配を求めることができる。
    </div></div>


  <h3>応用技術</h3>
    <div class="hidden_box">
    <label for="label-advanced-of-neural-network">[+]　　　</label>
    <input type="checkbox" id="label-advanced-of-neural-network"/>
    <div class="hidden_show">

    <h4>活性化関数（応用）</h4>
      <h5>parametric ReLU</h5>
      $$
        f(u) = 
        \left\{
          \begin{array} {11}
            \alpha u & (u \leq 0) \\
            u & (u > 0)
          \end{array} 
        \right.
      $$

      <h5>leaky ReLU</h5>
      \(\alpha = 0.01\)のparametric ReLU
      <h5>maxout</h5>
      K個の異なるユニットが異なる重みとバイアスを持ち、それらの最大値を出力とする。
      性能は良いがパラメータ数が多いため、それほどよく使われるわけではない。
      $$
      u_{jk} = \sum_i{w_{jik}z_i}+b_{jk} (k = 1,...,K)
      f(u_j) = \max_{k=1,...,K}u_{jk}
      $$
      <h5>ソフトプラス</h5>
      ReLUの連続近似、ソフトプラスを微分するとシグモイド関数になる。
      $$
      f(u) = \log{(1 + e^u)}
      $$
    
      <h5>ELU</h5>
      ReLUに比べてあまり広く使われていないがReLUよりも高速だと言われる。

      $$
      f(u) = 
      \left\{
        \begin{array} {11}
            \alpha (e^u - 1) & (u \leq 0) \\
            u & (u > 0)
        \end{array} 
      \right.
      $$

      <h5>SELU</h5>
      ELUを\(\lambda\)倍する。

    <h4>過適合の緩和</h4>
    <h4>ドロップアウト</h4>
      途中のノードを確率的に無効化（ニューロンの重みを０に）することで、オーバーフィッティングを避ける。
      中間層だけでなく、入力層でもドロップアウトすることで性能が良くできる場合がある。
      RNNの場合、フィードフォワードの分はドロップアウトした方がよいが、時系列のドロップアウトはしない方がよい。

    <h4>バッチ正則化(2014)</h4>
        この技術を使うことで画像認識の誤り率が5%を切った。
        ミニバッチ毎に正則化を行う。
        初期化((例)ザビエルの初期化)が不要となった。また、学習係数を大きくすることができるようになった。場合によってはドロップアウト不要
        バッチ毎に以下の計算を行う。
        <pre> 正規化データ = (あるデータ　－　平均) / (標準偏差) </pre>

    <h4>スパースモデリング</h4>
      デシメーションと重み付けで不要な重みを0にする。
      劣決定系のモデルの中でスパース（０が多い）解を算出する。
     
  </div></div>

  <h3>具体的なアルゴリズム</h3>
    <div class="hidden_box">
    <label for="label-algorithm-of-nn">[+]　　　</label>
    <input type="checkbox" id="label-algorithm-of-nn"/>
    <div class="hidden_show">
    ユニット数・層数・層間の結合、カーネルの幅などにより、様々なネットワークがある。

     <h4>ネオコグニトロン</h4>
       ニューラルネットワークがメジャーになる前に考えられた構成。add-if silentと呼ばれる学習方法を用いる。
     <h4>LeNet</h4>
       畳み込みニューラルネットワークの起源。誤差逆伝播法を用いる。
     <h4>AlexNet(2012)</h4>
       224x224のRGB３原色の画像を認識, 2枚のGPUボードで実装(中は同じ)
     <h4>LuNet</h4>
       手書き文字の認識で使用される。
     <h4>GoogLeNet</h4>
     <h4>VGG</h4>
     <h4>ResNet(2015)</h4>
       層を飛ばして出力するSkip Connectができる。カーネルが3x3で同じ形で何度も繰り返す。
     <h4>ResNet16</h4>
     入力+conv64+conv64+maxpool+conv128+conv128+maxpool+conv256+conv256+maxpool+conv512+conv512+maxpool+FC4096(完全結合)+FC4096+FC1000+softmax

     <h4>DensNet(2018)</h4>
       複数の階層を飛ばすSkip Connectがある。大脳新皮質のつながりに近くなる。
    </div></div>

  <h3>DNNフレームワーク</h3>
    <div class="hidden_box">
    <label for="label-framework">[+]　　　</label>
    <input type="checkbox" id="label-framework"/>
    <div class="hidden_show">

    <dl>
      <dt>Theano</dt>
      <dd>pythonのDNNライブラリ。歴史的意義があるが、2017/11/15に1.0.0がでてから更新されていない。</dd>
      <dt>Caffe</dt>
      <dd> UCバークレーで開発された Deep Learning Framework。
           Caffe Framework modelsが開発され、他のフレームワークでも使用できる
      </dd>
      <dt>Torch</dt>
      <dd>Lua言語をベースとしたスクリプト言語の機械学習ライブラリ。2018年からは開発は行われていない</dd>
      <dt>pyTorch</dt>
      <dd>Pythonで使用できるTorch。2019年に開発された。</dd>
      <dt>chainer</dt>
      <dd>preferred network社が開発している和製フレームワーク。
        2019/12/05 preferred network社はPyTorchに研究開発基板を移行を発表。
        <a href="https://preferred.jp/ja/news/pr20191205/">https://preferred.jp/ja/news/pr20191205/</a></dd>
      <dt>TensorFlow</dt>
      <dd>googleが開発、圧倒的に人気がある</dd>
      <dt> keras </dt>
      <dd>tensor flow, theanoのフロントエンド。人気がある</dd>
      <dt>caffe2</dt>
      <dd>caffeの改良版</dd>
      <dt>TensorFlow.js</dt>
      <dd>TensorFlowのjava script版</dd>
    </dl>
  </div> </div>

<h3>用語</h3>
  <div class="hidden_box">
  <label for="label-words">[+]　　　</label>
  <input type="checkbox" id="label-words"/>
  <div class="hidden_show">
   
  <dl>
    <dt>Artificial Neural Network(ANN, 人口ニューラルネットワーク)</dt>
    <dd>コンピュータの中のニューラルネットワーク, BNNの対比</dd>
    <dt>Biologinal Neural Network(BNN, 生物学的ニューラルネットワーク)</dt>
    <dd>
    人間や動物が生物学的に持っているニューラルネットワーク </dd>
    <dt>Convolusional Neural Network(CNN, 畳み込みニューラルネットワーク)</dt>
    <dd> 「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク
    </dd>
    <dt>Deep Neural Network(DNN, 深層学習)</dt>
    <dd>入力層＋中間層＋出力層からなるニューラルネットのなかで、中間層が２層以上のもの。
    <dt>Feedforward Neural Network(順伝播型ニューラルネットワーク)</dt>
    <dd>層状に並べたユニットが隣接層間でのみ結合した構造をもち、情報が入力側から出力側へ一方向に伝播する。
        multi layer perceptron（多層パーセプトロン）とも呼ぶ。
    </dd>
    <dt>変分オートエンコーダ</dt>
    <dd>内容</dd>
  </dl>
  </div></div>

<h2>convolutional neural network(cnn, 畳み込みニューラルネットワーク)</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-cnn">[+]　　　</label>
    <input type="checkbox" id="label-cnn"/>
    <div class="hidden_show">

  「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク。
  全結合でなく畳み込みを使うことでパラメータが少なくなる。
    <h4>ニューラルネットワークの問題</h4>
      <h5>計算量の問題</h5>
      例えば、
        全結合：224*224*3*96*55*55
        畳み込みニューラルネットワーク：11*11*3*96*55*55
      <h5>credit assignment problems(信用割当問題)</h5>
        全結合をすると、責任の所在が曖昧になる


    </div></div>
  <h3>用語</h3>
    <div class="hidden_box">
    <label for="label-cnn-words">[+]　　　</label>
    <input type="checkbox" id="label-cnn-words"/>
    <div class="hidden_show">

  <dl>
    <dt>全結合層</dt>
    <dd>全ての上位層から全ての下位層にネットワークが伝播する。責任の所在がわからなくなる。</dd>
    <dt>畳み込み層</dt>
    <dd>入力データの１部分に注目し、その部分画像の特徴を調べる層,カーネルと一部分の内積を求める。特徴はデータや損失関数により自動的に定まるが、入力層に近いところでは点や線といった低次元概念、出力層に近いところでは目や鼻といった高次元概念の注目をする。各特徴に１つのフィルターを使う</dd>
    <dt>プーリング層</dt>
    <dd>畳み込み層の出力を縮約し、データ量を削減する層、maxプーリングやaverageプーリングなどでデータの圧縮を実現する</dd>
    <dt>カーネル</dt>
    <dd>畳み込みに使用する重み行列(フィルタ)</dd>
    <dt>maxプーリング</dt>
    <dd>元行列の最大値を出力するプーリング</dd>
    <dt>サブサンプリング</dt>
    <dd></dd>
    <dt>averageプーリング</dt>
    <dd>元行列の平均値を出力するプーリング</dd>
    <dt>filters(keras)</dt>
    <dd>生成する特徴マップの数、filterはランダムに選ばれる</dd>
    <dt>kernel_size(keras)</dt>
    <dd>カーネル行列の大きさ</dd>
    <dt>strides(keras)</dt>
    <dd>カーネルを動かす距離</dd>
    <dt>padding(keras)</dt>
    <dd>入力画像の縮小を抑えるため、入力画像の周辺を0で埋めること</dd>
    <dt>pool_size(keras)</dt>
    <dd>一度にpoolingを適用する領域のサイズ</dd>
    <dt>global average pooling</dt>
    <dd>1つの特徴マップに 1 つのクラスを対応させることで分類を行う</dd>
    <dt>次元の祝福</dt>
    <dd>弁別可能性が高まることや、ミクロとマクロを結びつけることでシンプルに書けること。</dd>
  </dl>
  </div></div>

  <h3>3層パーセプトロンの関数近似としての完全性</h3>
    <div class="hidden_box">
    <label for="label-3perceptron">[+]　　　</label>
    <input type="checkbox" id="label-3perceptron"/>
    <div class="hidden_show">

    ３層あればどんな関数も作成できることが証明できる。
    <h4>万能近似定理</h4>
    どんな関数を学習するかにかかわらず、大きなmlpであれば、その関数を表現できるが、訓練可能であることを保証しない。
    <h4>ノーフリーランチ定理</h4>
      <q>コスト関数の局地を探索するあらゆるアルゴリズムは、全ての可能なコスト関数に適用した結果を平均すると同じ性能となる。</q>[1]という組み合わせ最適化の領域の定理。
    <h4>層の数と表現力</h4>
      浅いネットワークであれば、一層あたりのニューロンが増えてしまう。
      </div></div>

  <h3>実装</h3>
    <div class="hidden_box">
    <label for="label-cnn-implementation">[+]　　　</label>
    <input type="checkbox" id="label-cnn-implementation"/>
    <div class="hidden_show">

    <h4>python + keras</h4>
<!--==============================コード==============================-->
<pre> <code> # モデルの定義
model = sequential()

// 畳み込み層の挿入
model.add(conv2d(filters=32, kernel_size=(3, 3), input_shape=(28,28,1))) 

// 活性化関数の挿入
model.add(activation('relu')) 

// プーリング層の挿入
model.add(maxpooling2d(pool_size=(2, 2)))

// 入力にドロップアウトを適用(ランダムに入力ユニットを0とする)
model.add(dropout(0.25))
// 入力を平坦化
model.add(flatten())
// 全結合レイヤーの挿入
model.add(dense(128))

// アクティベーション層の導入(softmax)
model.add(activation('softmax'))

// 学習のためのモデルを設定
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])

// 固定回数の施行で学習
model.fit(x_train, y_train,
          batch_size=128,
          epochs=1,
          verbose=1,
          validation_data=(x_test, y_test)) </code> </pre>
<!--==================================================================-->
    </div></div>


<h2>RNN : <span class="small"> : recurrent neural network, 再帰型ニューラルネットワーク</span></h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-rnn-outline">[+]　　　</label>
    <input type="checkbox" id="label-rnn-outline"/>
    <div class="hidden_show">

    <h4>特徴</h4>
      フィードバック結合をもつ。漸化式のような構造を持つ。
      最も簡単な構造のRNNの式は以下の式で表され、
      構造は、入力層\(x_i\)、1層以上の中間層(隠れ層)\(h_i\)、出力層\(y_i\)からなる下図で示され、
      １時刻前の中間層の出力を次の中間層に利用することで、それまでの状態変化を全て反映する。
      $$
        h_t = f(Ux_t + Wh_{t-1} + b_h)
      $$
      $$
        y_t = g(Vh_t + b_y)
      $$

      <img src="out\ai\RNN_base.png" alt="RNN"/>
    <h4>応用分野</h4>
      手書き文字認識、音声認識、手書き文字生成、系列学習、機械翻訳、画像脚注付け、構文解析、プログラムコード生成
    <h4>RNNの種類と応用例</h4>
      入出力の数により幾つかの種類に分けることができる。
      <dl>
        <dt>one-to-one</dt>
        <dd>画像分類</dd>
        <dt>one-to-many</dt>
        <dd>中間層が時間発展で変わっていく : 画像脚注付き</dd>
        <dt>many-to-one<dt>
        <dd>入力が沢山で出力が１つ：感性分析(レビュー文を見せて星の数を予想する)</dd>
        <dt>many-to-many<dt>
        <dd>入力と出力がたくさん（通常のrnn)：機械翻訳</dd>
        <dt>many-to-many(逐次)</dt>
        <dd>ビデオ分類</dd>
      </dl>

      <h5>Elman Net<span class="small">エルマンネット</span></h5>
      (入力層+context)＋中間層＋出力層となっており、contextは1時刻前の中間層（内部状態）をコピーする。

      <h5>Jordan Net<span class="small"> : ジョーダンネット</span></h5>
      (入力層＋context)＋中間層＋出力層となっており、contextは1時刻前の出力層をコピーする。

    <h4>課題</h4>

      <h5>long term dependency(長距離依存)</h5>
        いくつか前の情報に依存する時の対策は難しく、
        再帰結合係数行列の時間発展に関して指数的に作用（勾配爆発）

      <h5> the gradient exploring problem(勾配爆発問題)</h5>
        RNNの逆伝播において、重み行列を掛けていくことにより、
        勾配が指数関数的に大きくなってしまうこと。

      </div></div>

    <h3>LSTM<span class="small"> : long short-term memory、長・短期記憶 </span></h3>
      <div class="hidden_box">
      <label for="label-rnn-lstm">[+]　　　</label>
      <input type="checkbox" id="label-rnn-lstm"/>
      <div class="hidden_show">
       <h4>概要</h4>
         RNNの長距離依存の問題を解決したアーキテクチャ。
         ネットワークの１つのニューロンをセル、入力ゲート、出力ゲート、及び忘却ゲートで再構築する。
         各ゲートの入出力も必要なので、結合係数は５倍になる。
         $$
          f_t = \sigma(W_fx_t + U_fy_{t-1} + b_f)
         $$
         $$
          i_t = \sigma(W_ix_t + U_iy_{t-1} + b_i)
         $$
         $$
          o_t = \sigma(W_ox_t + U_iy_{t-1} + b_o)
         $$
         $$
          c_t = f_t \circ h_{t-1} + i_t \circ \tanh(W_cx_t + U_cy_{t-1} + b_c)
         $$
         $$
          y_t = o_t \circ \tanh(c_t)
         $$

        <img src="out\ai\LSTM.png" alt="LSTM" width=100%/>

        <h5>ゲートのコントロールはだれ</h5>
          一つ前の中間層、一つ前の出力、現時刻の入力で結合係数の差により、入力、出力、忘却のそれぞれのゲートの制御の差が出る。

     </div></div>


  <h3>応用技術</h3>
    <div class="hidden_box">
    <label for="label-rnn-advanced">[+]　　　</label>
    <input type="checkbox" id="label-rnn-advanced"/>
    <div class="hidden_show">

    <h4> back propagation through time(bptt)</h4>
    　時間伝播において、過去のネットワークを含めて誤差逆伝播を考える。　
      時刻０からの過去の入力を考えたものは完全bpttと呼び、時刻幅を考えたものをtrancated(切断) bpttと呼ぶ

    <h4>teacher forcing(教師強制)</h4>
      ジョーダンネットにおいて、学習初期には、出力の精度が悪いので、教師情報を入れる。
      back propagation through time の代替として開発された。


      <h4>双方向リカレントニューラルネットワーク</h4>
        birnn、事後にデータが取得できた場合、次の時刻のデータから予測するということがあり得る
        例えば、英語の単語(the apple)のtheはappleを確認してからtheの発音を決定できる。
        
      <h4>sequence-to-sequence</h4>
        入力用と出力用の2つのrnn(lstm)を使用する。
        中間層の一工夫として、入力系列(文章)は逆順に入れると精度が上がることが報告されている。

      <h4>encoder-decoder(エンコーダデコーダ)</h4>
        ソースとターゲットで２つのリカレントネットワークを使用する。
        ソース言語の方で双方向リカレントニューラルネットワークを使用して、束ねてからターゲットにデータを入れる。
        ソースからの複数の出力のうちのどれを使うのかをattentionとして出力側が決定する。
        このことで、文章が長くなっても精度が下がらなくなった。

      <h4>勾配クリップ</h4>
        勾配の幅に制限を付けることで勾配爆発を抑えることで、学習が可能となる。

      <h4>gru</h4>
        入力はそれ自体か内部状態と合わさったものが内部入力となり、リセットゲートにより決定される。
        過去の内部状態かそれに加え内部入力が合わさって作られたものかが新規内部状態になり、更新ゲートで決定される。
        内部状態によりリセットゲートが、内部状態により更新ゲートが決まる。
        
      <h4>メモリ拡張</h4>
        rnnに外部メモリを追加すると更に知的なことができる
        <dl>
          <dt>q and a</dt>
          <dd></dd>
          <dt>プログラムの自動作成</dt>
          <dd></dd>
          <dt>アルゴリズムの学習</dt>
          <dd></dd>
          <dt>ビデオq&a</dt>
          <dd></dd>
          <dt>メモリネット</dt>
          <dd></dd>
          <dt>ポインターネット</dt>
          <dd></dd>
        </dl>
    <h4>ニューラルチューリングマシン</h4>
      チューリングマシンでできることをニューラルネットでできるようになった。
    </div></div>

<h2>生成モデル</h2>
  <h3>evidence of lower bound (elbo, 変分下限)</h3>

  <h3>baysian nn (ベイズ流ニューラルネットワーク)</h3>

  <h3>deep generative models</h3>

  <h3>variational autoencoders (vae 変分オートエンコーダ)</h3>
    <div class="hidden_box">
    <label for="label-vae">[+]　　　</label>
    <input type="checkbox" id="label-vae"/>
    <div class="hidden_show">

    生成モデルの枠組みで自己符号化器 (autoencoders) を解釈したもので、 生成モデルで言う潜在変数を観測データの符号と見做す. すなわち、観測データから潜在変数を推定する手続きが符号化であり、その逆が復号化である.
    グラフを使って似た画像を並べて、潜在変数を解釈することができる。
    <h4> ベイズ推論 </h4>
      条件付き確率の出力から、条件を算出する。

    <h4>変分ベイズ法 (変分推論、変分近似法)</h4>
      データ(x)と未知パラメータ(z)と潜在変数(θ)の確率分布がある場合、
      KL距離(KL Divergence)の最小化を解析的手法で求めることで最適値が求まる。

    <h4>リパラメトライゼーショントリック</h4>
      x -> z -> xを求める過程でzは確率変数でなく
      真値(平均)とノイズの組み合わせだとみなして解く。
      RNNにも応用できて、中間の文章が生成できる。

    </div></div>

  <h3>generative adversarial networks (GAN =敵対ネットワーク)</h3>
  <div class="hidden_box">
    <label for="label-gan">[+]　　　</label>
    <input type="checkbox" id="label-gan"/>
    <div class="hidden_show">
    <h4>基本</h4>
      DiscrereminatorとGeneratorの2つのネットワークが競い合う。
      Generatorはノイズから画像を生成する。
      Generatorが作ったデータをDiscreminatorの入力とし、Discriminatorはデータを判断する。

    <h4>応用</h4>
      <ul>
        <li>眼鏡つき男性画像１　ー　眼鏡なし男性画像２　＋　眼鏡なし女性画像 = 眼鏡つき女性画像</li>
        <li>模型図から実画像を生成</li>
        <li>馬をシマウマに</li>
        <li>ゴッホ風、モネ風の画像生成</li>
        <li>自動着色</li>
        <li>光源を変更</li>
     </ul>

     </div></div>

  <h3>評価</h3>
    <div class="hidden_box">
    <label for="label-evaluate-vae">[+]　　　</label>
    <input type="checkbox" id="label-evaluate-vae"/>
    <div class="hidden_show">

    <h4>動的評価</h4>
      通常は訓練データとテストデータは完全に分けて評価をするが、
      文脈においては、特定の単語（人名など）と偏ったデータになりがちなので、
      テストデータでも１回だけ学習をすること。

    </div></div>

<h2> NNによる強化学習</h2>
  <h3>基本</h3>
    <div class="hidden_box">
    <label for="label-reinforcement-nn">[+]　　　</label>
    <input type="checkbox" id="label-reinforcement-nn"/>
    <div class="hidden_show">

    Q-LearningをDNNに採用。入力はテレビ画面。出力はジョイスティック(18通り)。
    教師信号はなし。
    画期的な技術というよりもコンピュータの能力、データ規模の拡大、
    アルゴリズムの改良、エコシステムの拡大

    </div></div>

  <h3>DQN</h3>
    <div class="hidden_box">
    <label for="label-dqn">[+]　　　</label>
    <input type="checkbox" id="label-dqn"/>
    <div class="hidden_show">

    損失関数をQLearningに入れる。

    </div></div>

  <h3>改良版アルゴリズム</h3>
    <div class="hidden_box">
    <label for="label-rainbow">[+]　　　</label>
    <input type="checkbox" id="label-rainbow"/>
    <div class="hidden_show">
     <dl>
       Rainbow, Hessel et al 2017
     </dl>
     <dd> ポリシーベース、状態価値関数、モデルベースの強化学習 </dd>

    </div></div>

<h2>その他</h2>
  <h3>self organizing map <span class="small"> :  SOM, 自己組織化</span></h3>
    <div class="hidden_box">
    <label for="label-som">[+]　　　</label>
    <input type="checkbox" id="label-som"/>
    <div class="hidden_show">

    <dl>
      <dt>マルスバーグの自己組織化</dt>
      <dd>
        Todo : 内容
      </dd>
      <dt>コホネンの自己組織化</dt>
      <dd>
        ランダムベクトルからなる行列を用意する。入力ノード群を用意し、
        そのノードに一番近い点とそのノードの隣接点に暴露することを繰り返すことで分類ができる。
       (結果的に入力ノードが消える場合もある）
      </dd>
      <dt>リンスカーの受容野形成の自己組織化</dt>
      <dd>
        Todo : 内容
      </dd>
      <dt>オジャのSOMの自己組織化</dt>
      <dd>
        Todo : 内容
      </dd>
    </dl>
    </div></div>

  <h3>深層学習を用いた自然言語処理</h3>
    <div class="hidden_box">
    <label for="label-nlp">[+]　　　</label>
    <input type="checkbox" id="label-nlp"/>
    <div class="hidden_show">

    <h4>利点</h4>
      深層学習を使わないベクトル表現では、ベクトルの次元が大きくなりすぎたり、意味を扱えなかったりするが、
      深層学習を使うと、誤差逆伝播法により、わずか数百次元のベクトルで単語の意味を表現できるようになる。

    <h4>word2vec</h4>
      "king" - "man" + "woman" = "Queen" <br>
      単語埋め込み(Embedding)モデル。単語のベクトルを用いて、単語の足し算、引き算が可能となる。同じように文法も解ける。
      BOWや単語文書行列では、それぞれの単語が0,1の入力層が数万個で表示されていたのが、まとめてベクトルとすると、
      ベクトルが学習によって意味を表すようになってきた。
      自分以外の周りの単語を予測するスキップグラムとその逆のCBOWで学習させると中間層に意味の表層ができる。

    <h4>doc2vec</h4>
      word2vecを応用した文章をベクトル化する技術で、文の語順も特徴として考慮に入れることができる。



    </div></div>

<h2>参考文献、サイト</h2>
  <dd>
  <dt> <cite><a href="https://pdfs.semanticscholar.org/8bdf/dc2c2777b395c086810c03a8cdeccc55c4db.pdf">Wolpert, D.H., Macready, W.G. "No Free Lunch Theorems for Search", Technical Report SFI-TR-95-02-010,1995, Santa Fe Institute </a></cite></dt>
  <dd>ノーフリーランチ定理</dd>
  <dt><cite><a href="http://bookclub.kodansha.co.jp/product?item=0000147653">岡谷貴之,"深層学習(機械学習プロフェッショナルシリーズ)", 2015, 講談社</a></cite></dt>
  <dd>深層学習の基礎を網羅している本。</dd>
  <dt><cite>Akiba, Takuya, Shuji Suzuki, and Keisuke Fukuda "Extremely Large Minibatch Sgd: Training Resnet-50 on Imagenet in 15 Miniutes", 2017, arXiv Preprint.</cite>
  </dt>
  <dd></dd>
  Yee Wyne Teh NIPS 2017

  <dt> [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) 再解釈論文 (2016)</dt>
  <dd> 変分オートエンコーダを再解釈した論文</dd>

<div class = "end_of_page"></div>

  </main>
</body>
</html>
