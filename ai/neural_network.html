<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai - newral network</title>
<base href="../" />


<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
      macros: {
      x: "{\\times}",
      bm: ["{\\boldsymbol{#1}}",1],
      dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
      },
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - 
    <a href="index.html">AI</a>
      - neural network 
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：活性化関数について追記
</div>

 <div class="navigation">
<ul>
  <li><a href="index.html">home</a></li>
  <li><a href="mobile_robot/index.html">移動ロボット</a></li>
  <li><a href="ai/index.html">AI</a></li>
  <li><a href="math/index.html">数学</a></li>
  <li><a href="other/index.html">その他</a></li>
</ul>
</div>

<!--*********************************本文*********************************-->

<h1>Deep Learning (深層学習)</h1>

<h2>Neural Network</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-neural-network">[+]</label>
    <input type="checkbox" id="label-neural-network"/>
    <div class="hidden_show">
    脳の中の神経細胞を模擬したアルゴリズムで、
    シナプスの結合によりネットワークを形成した人口ニューロンが持つ結合加重ベクトルを学習によりを変化させ、
    様々な問題を解決できるようなモデルを作る。

    <h4>過去の課題</h4>  
      <h5>the gradient vanishing problem(勾配消失問題)</h5>
        活性化関数の変化の際にチェインルールが適用されるが、
        この計算の際に微分が使われる。
        シグモイド関数を使うと、一回の計算で勾配が1/4以下になるので、層を増やすと勾配がなくなってしまう。
        ハイパータンジェントやReLuでは勾配消失問題が起きにくい。
      <h5>credit assignment problems(信用割当問題)</h5>
        全結合をすると、責任の所在が曖昧になる
  
      <h5>次元の呪い</h5>
        推定すべきパラメータが多くなると、次元が多くなって、パラメータ空間の探索が難しくなり、
        オーバーフィッティングなどが起きやすくなること。
      
    <h4>ブレイクスルー</h4>
        <h5>auto encoder(自己符号化器)</h5>
        3層のネットワークを考え、中間層のユニット数が入力層と出力層よりも小さく、かつ出力層で入力を再現できるような重み付けを行うこと。
        この時、中間層は入力層の次元を圧縮できたことになる。自己符号化器を多層に重ねることで勾配消失問題を解決し、ディープニューラルネットワークが可能となった。
        ただし現在では活性化関数の工夫により勾配消失問題を解決していることからあまり使われていない。

      <h4>Neural Networkの種類</h4>
      <dl>
        <dt>Artificial Neural Network(ANN, 人口ニューラルネットワーク)</dt>
        <dd>コンピュータの中のニューラルネットワーク, BNNの対比</dd>
        <dt>Biologinal Neural Network(BNN, 生物学的ニューラルネットワーク)</dt>
        <dd>
        人間や動物が生物学的に持っているニューラルネットワーク </dd>
        <dt>Convolusional Neural Network(CNN, 畳み込みニューラルネットワーク)</dt>
        <dd> 「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク
        </dd>
        <dt>Deep Neural Network(DNN, 深層学習)</dt>
        <dd>入力層＋中間層＋出力層からなるニューラルネットのなかで、中間層が２層以上のもの。
        <dt>Feedforward Neural Network(順伝播型ニューラルネットワーク)</dt>
        <dd>層状に並べたユニットが隣接層間でのみ結合した構造をもち、情報が入力側から出力側へ一方向に伝播する。
            multi layer perceptron（多層パーセプトロン）とも呼ぶ。
        </dd>
        <dt>Recurrent Neural Network (RNN)</dt>
        <dd> 前の時間の結果を入力の一部としたニューラルネットワーク</dd>
        <dt>Reinforcement Learning強化学習</dt>
        <dd> </dd>
        <dt>GAN, VAE</dt>
        <dd>内容</dd>
        <dt>変分オートエンコーダ</dt>
        <dd>内容</dd>
      </dl>
    </div></div>
  
  <h3>基本技術</h3>
    <div class="hidden_box">
    <label for="label-basics-of-neural-network">[+]</label>
    <input type="checkbox" id="label-basics-of-neural-network"/>
    <div class="hidden_show">

    <h4>パーセプトロン(単純パーセプトロン)</h4>
      バイアス +Σ（入力 × 重み）= a を計算し、aが0以上の場合は1、以下の場合は0を出力
      この単純化のため排他的論理和の問題は解けない。(１本の判別直線で分離できない問題は解けない)
      微分可能な活性化関数と２層以上の中間層、あるいは、スキップな構造が必要となる。
      <div>
      <img src="out\ai\perceptron\perceptron.png" alt="パーセプトロン"/>
      </div>

    <h4>活性化関数</h4>
    中間層や出力層で、入力を足し合わせたのちに通す関数。非線形であることが重要。
      <h5>ロジスティックシグモイド関数</h5>
      入力は\((-\inf,\inf)\)出力は\((0,1)\)である関数。
      入力値が飽和する。出力が滑らかであることなどが特徴。
      微分値は\((0,0.25)\)しかとらず問題となった。
      $$ f(u) = \frac{1}{1 + e^{-u}} $$
      
      <h5>ハイパータンジェント</h5>
      入力は\((-\inf,\inf)\)出力は\((-1, 1)\)である関数。
      入力値が飽和する。出力が滑らかであることなどが特徴。
      微分値は\((0,1.0)\)なので、シグモイドよりも良い。
      $$ f(u) = \tanh(u) $$

      <h5>ReLU (Rectified Linear Unit 整流線型ユニット)</h5>
      簡単でかつ収束も早い。u = 0の時に少しだけ問題となる。
      $$
        f(u) = 
        \left\{
          \begin{array} {11}
              0 & (u \leq 0) \\
              u & (u > 0)
          \end{array} 
        \right.
      $$
      <h5>parametric ReLU</h5>

      $$
        f(u) = 
        \left\{
          \begin{array} {11}
              \alpha u & (u \leq 0) \\
              u & (u > 0)
          \end{array} 
        \right.
      $$

      <h5>leaky ReLU</h5>
      \(\alpha = 0.01)\)のparametric ReLU
      <h5>maxout</h5>
      K個の異なるユニットが異なる重みとバイアスを持ち、それらの最大値を出力とする。
      性能は良いがパラメータ数が多いため、それほどよく使われるわけではない。
      $$
      u_{jk} = \sum_i{w_{jik}z_i}+b_{jk} (k = 1,...,K)
      f(u_j) = \max_{k=1,...,K}u_{jk}
      $$
      <h5>ソフトプラス</h5>
      ReLUの連続近似、ソフトプラスを微分するとシグモイド関数になる
      <h5>ハードtanh</h5>
      <h5>ELU</h5>

    <h4>誤差逆伝搬</h4>
      
    <h4>cost function(コスト関数、loss function、損失関数、error function、誤差関数、objective function、目的関数)</h4>
      <h5>二乗誤差</h5>
      出力と正解の各成分の差の二乗和による誤差

      <h5>クロスエントロピー誤差</h5>
      二乗誤差よりも収束がはるかに速い
    <h4>出力ユニット</h4>
    　<h5>ロジスティックシグモイド関数</h5>
        0から1までの確率で出力される
    　<h5>ソフトマックス関数</h5>
        多クラス分類に使用する。複数の候補を取得する。それぞれのyは0～1になり、すべて足すと1になる(確率の要件を満たす)
    　<h5>Gaussian Mixture Model (GMM、ガウス混合分布)</h5>
        ある点が複数のガウス分布のうちどこに属しているのかを確率的に表現する。
    　<h5>Radial Basis Function (RBF, 動径基底関数)</h5>
        ある点cからの距離のみに基づいて値が決まる関数である動径関数をcを変えて集め、それらの線形和をとる。
      
    </div></div>

 <h3>応用技術</h3>
    <div class="hidden_box">
    <label for="label-advanced-of-neural-network">[+]</label>
    <input type="checkbox" id="label-advanced-of-neural-network"/>
    <div class="hidden_show">

    <h4>ドロップアウト</h4>
    途中のノードを確率的に無効化（ニューロンの重みを０に）することで、オーバーフィッティングを避ける。
    中間層だけでなく、入力層でもドロップアウトすることで性能が良くできる場合がある。
    RNNの場合、フィードフォワードの分はドロップアウトした方がよいが、時系列のドロップアウトはしない方がよい。

    <h4>バッチ正則化(2014)</h4>
      この技術を使うことで画像認識の誤り率が5%を切った。
      ミニバッチ毎に正則化を行う。
      初期化((例)ザビエルの初期化)が不要となった。また、学習係数を大きくすることができるようになった。場合によってはドロップアウト不要
      バッチ毎に以下の計算を行う。
      <pre>
      正規化データ = (あるデータ　－　平均) / (標準偏差)
      </pre>

    
    <h4>スパースモデリング</h4>
    デシメーションと重み付けで不要な重みを0にする。
    劣決定系のモデルの中でスパース（０が多い）解を算出する。
   

  <h3>勾配降下</h3>
  <dl>
    <dt>バッチ学習法</dt>
    <dd></dd>
    <dt>オンライン学習法</dt>
    <dd></dd>
    <dt>確率的勾配降下法</dt>
    <dd>勾配のαを変えて更新幅を決める、ミニバッチを用いる</dd>
    <dt>モーメント法</dt>
    <dd>1時刻前の更新幅を利用して現在の更新幅を決める</dd>
    <dt>ニュートン法</dt>
    <dd>ニュートン法と同じくヘシアン（２次微分）を用いる最適化手法。ただし、ヘシアン作るのは難しい</dd>
    <dt>Adgrad法</dt>
    <dd>ヘシアンの近似として、ヘシアンの対角行列のみをつかう。</dd>
    <dt>Adadelta法</dt>
    <dd>ヘシアンの近似として、逐次自乗平均を使う。</dd>
    <dt>RMSprop法</dt>
    <dd>逐次平均ではなくて、過去の値に重みを増やす</dd>
    <dt>Adam 法</dt>
    <dd>モーメントも使う</dd>
  </dl>



    </div></div>
  <h3>具体的なアルゴリズム</h3>
    <div class="hidden_box">
    <label for="label-algorithm-of-nn">[+]</label>
    <input type="checkbox" id="label-algorithm-of-nn"/>
    <div class="hidden_show">
     ユニット数・層数・層間の結合、カーネルの幅などにより、様々なネットワークがある。

     <h4>ネオコグニトロン</h4>
       ニューラルネットワークがメジャーになる前に考えられた構成。add-if silentと呼ばれる学習方法を用いる。
     <h4>LeNet</h4>
       畳み込みニューラルネットワークの起源。誤差逆伝播法を用いる。
     <h4>AlexNet(2012)</h4>
       224x224のRGB３原色の画像を認識, 2枚のGPUボードで実装(中は同じ)
     <h4>LuNet</h4>
       手書き文字の認識で使用される。
     <h4>GoogLeNet</h4>
     <h4>VGG</h4>
     <h4>ResNet(2015)</h4>
       層を飛ばして出力するSkip Connectができる。カーネルが3x3で同じ形で何度も繰り返す。
     <h4>ResNet16</h4>
     入力+conv64+conv64+maxpool+conv128+conv128+maxpool+conv256+conv256+maxpool+conv512+conv512+maxpool+FC4096(完全結合)+FC4096+FC1000+softmax

     <h4>DensNet(2018)</h4>
       複数の階層を飛ばすSkip Connectがある。大脳新皮質のつながりに近くなる。
    </div></div>

  <h3>DNNフレームワーク</h3>
    <div class="hidden_box">
    <label for="label-framework">[+]</label>
    <input type="checkbox" id="label-framework"/>
    <div class="hidden_show">

    <dl>
    <dt>Theano</dt>
    <dd>pythonのDNNライブラリ。歴史的意義があるが、2017/11/15に1.0.0がでてから更新されていない。</dd>
    <dt>Caffe</dt>
    <dd> UCバークレーで開発された Deep Learning Framework。
         Caffe Framework modelsが開発され、他のフレームワークでも使用できる
    </dd>
    <dt>Torch</dt>
    <dd>Lua言語をベースとしたスクリプト言語の機械学習ライブラリ。2018年からは開発は行われていない</dd>
    <dt>pyTorch</dt>
    <dd>Pythonで使用できるTorch。2019年に開発された</dd>
    <dt>chainer</dt>
    <dd>preffered network社が開発している和製フレームワークしている</dd>
    <dt>tensorflow</dt>
    <dd>googleが開発、圧倒的に人気がある</dd>
    <dt> keras </dt>
    <dd>tensor flow, theanoのフロントエンド。人気がある</dd>
    <dt>caffe2</dt>
    <dd>caffeの改良版</dd>
    <dt>tensorflow.js</dt>
    <dd>tensorflowのjava script版</dd>
    </dl>
    </div> </div>
  
<h2>convolutional neural network(cnn, 畳み込みニューラルネットワーク)</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-cnn">[+]</label>
    <input type="checkbox" id="label-cnn"/>
    <div class="hidden_show">

  「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク。全結合でなく畳み込みを使うことでパラメータが少なくなる。
    <h4>例</h4>
      全結合：224*224*3*96*55*55
      畳み込みニューラルネットワーク：11*11*3*96*55*55

    </div></div>
  <h3>用語</h3>
    <div class="hidden_box">
    <label for="label-cnn-words">[+]</label>
    <input type="checkbox" id="label-cnn-words"/>
    <div class="hidden_show">

  <dl>
    <dt>全結合層</dt>
    <dd>全ての上位層から全ての下位層にネットワークが伝播する。責任の所在がわからなくなる。</dd>
    <dt>畳み込み層</dt>
    <dd>入力データの１部分に注目し、その部分画像の特徴を調べる層,カーネルと一部分の内積を求める。特徴はデータや損失関数により自動的に定まるが、入力層に近いところでは点や線といった低次元概念、出力層に近いところでは目や鼻といった高次元概念の注目をする。各特徴に１つのフィルターを使う</dd>
    <dt>プーリング層</dt>
    <dd>畳み込み層の出力を縮約し、データ量を削減する層、maxプーリングやaverageプーリングなどでデータの圧縮を実現する</dd>
    <dt>カーネル</dt>
    <dd>畳み込みに使用する重み行列(フィルタ)</dd>
    <dt>maxプーリング</dt>
    <dd>元行列の最大値を出力するプーリング</dd>
    <dt>サブサンプリング</dt>
    <dd></dd>
    <dt>averageプーリング</dt>
    <dd>元行列の平均値を出力するプーリング</dd>
    <dt>filters(keras)</dt>
    <dd>生成する特徴マップの数、filterはランダムに選ばれる</dd>
    <dt>kernel_size(keras)</dt>
    <dd>カーネル行列の大きさ</dd>
    <dt>strides(keras)</dt>
    <dd>カーネルを動かす距離</dd>
    <dt>padding(keras)</dt>
    <dd>入力画像の縮小を抑えるため、入力画像の周辺を0で埋めること</dd>
    <dt>pool_size(keras)</dt>
    <dd>一度にpoolingを適用する領域のサイズ</dd>
    <dt>global average pooling</dt>
    <dd>1つの特徴マップに 1 つのクラスを対応させることで分類を行う</dd>
    <dt>次元の祝福</dt>
    <dd>弁別可能性が高まることや、ミクロとマクロを結びつけることでシンプルに書けること。</dd>
  </dl>
  </div></div>

  <h3>3層パーセプトロンの関数近似としての完全性</h3>
    <div class="hidden_box">
    <label for="label-3perceptron">[+]</label>
    <input type="checkbox" id="label-3perceptron"/>
    <div class="hidden_show">

    ３層あればどんな関数も作成できることが証明できる。
    <h4>万能近似定理</h4>
    どんな関数を学習するかにかかわらず、大きなmlpであれば、その関数を表現できるが、訓練可能であることを保証しない。
    <h4>ノーフリーランチ定理</h4>
    <h4>層の数と表現力</h4>
      浅いネットワークであれば、一層あたりのニューロンが増えてしまう。
      </div></div>

  <h3>実装</h3>
    <div class="hidden_box">
    <label for="label-cnn-implementation">[+]</label>
    <input type="checkbox" id="label-cnn-implementation"/>
    <div class="hidden_show">

    <h4>python + keras</h4>
<!--==============================コード==============================-->
<pre> <code> # モデルの定義
model = sequential()

// 畳み込み層の挿入
model.add(conv2d(filters=32, kernel_size=(3, 3), input_shape=(28,28,1))) 

// 活性化関数の挿入
model.add(activation('relu')) 

// プーリング層の挿入
model.add(maxpooling2d(pool_size=(2, 2)))

// 入力にドロップアウトを適用(ランダムに入力ユニットを0とする)
model.add(dropout(0.25))
// 入力を平坦化
model.add(flatten())
// 全結合レイヤーの挿入
model.add(dense(128))

// アクティベーション層の導入(softmax)
model.add(activation('softmax'))

// 学習のためのモデルを設定
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])

// 固定回数の施行で学習
model.fit(x_train, y_train,
          batch_size=128,
          epochs=1,
          verbose=1,
          validation_data=(x_test, y_test)) </code> </pre>
<!--==================================================================-->
    </div></div>


<h2>recurrent neural network(rnn, 再帰ニューラルネットワーク)</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-rnn-outline">[+]</label>
    <input type="checkbox" id="label-rnn-outline"/>
    <div class="hidden_show">

    <h4>特徴</h4>
      フィードバック結合をもつ。漸化式のような構造を持つ。
      1時刻前の情報を使用することで、それまでの状態変化をすべて反映する。
    <h4>応用分野</h4>
      手書き文字認識、音声認識、手書き文字生成、系列学習、機械翻訳、画像脚注付け、構文解析、プログラムコード生成
    <h4>rnnの種類と応用例</h4>
      one-to-one  : 画像分類
      one-to-many : 中間層が時間発展で変わっていく : 画像脚注付き
      many-to-one : 入力が沢山で出力が１つ：感性分析(レビュー文を見せて星の数を予想する)
      many-to-many : 入力と出力がたくさん（通常のrnn)：機械翻訳
      many-to-many(逐次) : ビデオ分類

    <h4>ジョーダンネット</h4>
      (入力層＋context)＋中間層＋出力層。となっており、contextは1時刻前の出力層をコピーする。
    <h4>エルマンネット</h4>
      (入力層+context)＋中間層＋出力層となっており、contextは1時刻前の中間層（内部状態）をコピーする。

      </div></div>
  <h3>課題</h3>
    <div class="hidden_box">
    <label for="label-rnn-problem">[+]</label>
    <input type="checkbox" id="label-rnn-problem"/>
    <div class="hidden_show">

    <h4>long term dependency(長距離依存)</h4>
      いくつか前の情報に依存する時の対策は難しく、再帰結合係数行列の時間発展に関して指数的に作用（勾配爆発）

    <h4> the gradient exploring problem(勾配爆発問題)</h4>
      RNNの逆伝播において、重み行列を掛けていくことにより、勾配が指数関数的に大きくなってしまうこと。

    </div></div>

  <h3>応用技術</h3>
    <div class="hidden_box">
    <label for="label-rnn-problem">[+]</label>
    <input type="checkbox" id="label-rnn-problem"/>
    <div class="hidden_show">

    <h4> back propagation through time(bptt)</h4>
    　時間伝播において、過去のネットワークを含めて誤差逆伝播を考える。　
      時刻０からの過去の入力を考えたものは完全bpttと呼び、時刻幅を考えたものをtrancated(切断) bpttと呼ぶ

    <h4>teacher forcing(教師強制)</h4>
      ジョーダンネットにおいて、学習初期には、出力の精度が悪いので、教師情報を入れる。
      back propagation through time の代替として開発された。


    <h4>言語モデル</h4>
      次単語モデル予測課題。単語の総数が数十万なので、複数前の考慮ができない
      <ul>
              <li>0-gram(ヌルグラム) 事前の単語を考慮しない</li>
              <li>1-gram(ヌ二グラム) 1つ前の単語を考慮する。</li>
              <li>2-gram(ヌ二グラム) 2つ前と2つまえの単語を考慮する。</li>
      </ul>

      統計的言語モデルからニューラルネットワーク言語モデル（リカレント．．．nnlm、rnnlm)

      <h4>双方向リカレントニューラルネットワーク</h4>
        birnn、事後にデータが取得できた場合、次の時刻のデータから予測するということがあり得る
        例えば、英語の単語(the apple)のtheはappleを確認してからtheの発音を決定できる。
        
      <h4>sequence-to-sequence</h4>
        入力用と出力用の2つのrnn(lstm)を使用する。
        中間層の一工夫として、入力系列(文章)は逆順に入れると精度が上がることが報告されている。

      <h4>encoder-decoder(エンコーダデコーダ)</h4>
        ソースとターゲットで２つのリカレントネットワークを使用する。
        ソース言語の方で双方向リカレントニューラルネットワークを使用して、束ねてからターゲットにデータを入れる。
        ソースからの複数の出力のうちのどれを使うのかをattentionとして出力側が決定する。
        このことで、文章が長くなっても精度が下がらなくなった。

      <h4>勾配クリップ</h4>
        勾配の幅に制限を付けることで勾配爆発を抑えることで、学習が可能となる。

      <h4>long short-term memory (lstm)</h4>
        ゲートによって長距離依存ltdを解消。
        short term memory(アクティベーション(活性値))、long term memory(結合係数)のshort term memoryの影響をどうやって長引かせるのか？
        全体のネットワークの中の１つのニューロンを入力ゲート、出力ゲート、忘却ゲートを用いて再構築する。
        各ゲートの入出力も必要なので、結合係数は５倍になる。
        <h5>ゲートのコントロールはだれ</h5>
          一つ前の中間層、一つ前の出力、現時刻の入力で結合係数の差により、入力、出力、忘却のそれぞれのゲートの制御の差が出る。
      <h4>gru</h4>
        入力はそれ自体か内部状態と合わさったものが内部入力となり、リセットゲートにより決定される。
        過去の内部状態かそれに加え内部入力が合わさって作られたものかが新規内部状態になり、更新ゲートで決定される。
        内部状態によりリセットゲートが、内部状態により更新ゲートが決まる。
        
      <h4>メモリ拡張</h4>
        rnnに外部メモリを追加すると更に知的なことができる
        <dl>
          <dt>q and a</dt>
          <dd></dd>
          <dt>プログラムの自動作成</dt>
          <dd></dd>
          <dt>アルゴリズムの学習</dt>
          <dd></dd>
          <dt>ビデオq&a</dt>
          <dd></dd>
          <dt>メモリネット</dt>
          <dd></dd>
          <dt>ポインターネット</dt>
          <dd></dd>
        </dl>
    <h4>ニューラルチューリングマシン</h4>
      チューリングマシンでできることをニューラルネットでできるようになった。
    </div></div>

<h2>生成モデル</h2>
  <h3>evidence of lower bound (elbo, 変分下限)</h3>

  <h3>baysian nn (ベイズ流ニューラルネットワーク)</h3>

  <h3>deep generative models</h3>

  <h3>variational autoencoders (vae 変分オートエンコーダ)</h3>
    <div class="hidden_box">
    <label for="label-vae">[+]</label>
    <input type="checkbox" id="label-vae"/>
    <div class="hidden_show">

    生成モデルの枠組みで自己符号化器 (autoencoders) を解釈したもので、 生成モデルで言う潜在変数を観測データの符号と見做す. すなわち、観測データから潜在変数を推定する手続きが符号化であり、その逆が復号化である.
    グラフを使って似た画像を並べて、潜在変数を解釈することができる。
    <h4> ベイズ推論 </h4>
      条件付き確率の出力から、条件を算出する。

    <h4>変分ベイズ法 (変分推論、変分近似法)</h4>
      データ(x)と未知パラメータ(z)と潜在変数(θ)の確率分布がある場合、
      KL距離(KL Divergence)の最小化を解析的手法で求めることで最適値が求まる。

    <h4>リパラメトライゼーショントリック</h4>
      x -> z -> xを求める過程でzは確率変数でなく
      真値(平均)とノイズの組み合わせだとみなして解く。
      RNNにも応用できて、中間の文章が生成できる。

    </div></div>

  <h3>generative adversarial networks (GAN =敵対ネットワーク)</h3>
  <div class="hidden_box">
    <label for="label-gan">[+]</label>
    <input type="checkbox" id="label-gan"/>
    <div class="hidden_show">
    <h4>基本</h4>
      DiscrereminatorとGeneratorの2つのネットワークが競い合う。
      Generatorはノイズから画像を生成する。
      Generatorが作ったデータをDiscreminatorの入力とし、Discriminatorはデータを判断する。

    <h4>応用</h4>
      <ul>
        <li>眼鏡つき男性画像１　ー　眼鏡なし男性画像２　＋　眼鏡なし女性画像 = 眼鏡つき女性画像</li>
        <li>模型図から実画像を生成</li>
        <li>馬をシマウマに</li>
        <li>ゴッホ風、モネ風の画像生成</li>
        <li>自動着色</li>
        <li>光源を変更</li>
     </ul>

     </div></div>

  <h3>評価</h3>
    <div class="hidden_box">
    <label for="label-evaluate-vae">[+]</label>
    <input type="checkbox" id="label-evaluate-vae"/>
    <div class="hidden_show">

    <h4>動的評価</h4>
      通常は訓練データとテストデータは完全に分けて評価をするが、
      文脈においては、特定の単語（人名など）と偏ったデータになりがちなので、
      テストデータでも１回だけ学習をすること。

    </div></div>

<h2> NNによる強化学習</h2>
  <h3>基本</h3>
    <div class="hidden_box">
    <label for="label-reinforcement-nn">[+]</label>
    <input type="checkbox" id="label-reinforcement-nn"/>
    <div class="hidden_show">

    Q-LearningをDNNに採用。入力はテレビ画面。出力はジョイスティック(18通り)。
    教師信号はなし。
    画期的な技術というよりもコンピュータの能力、データ規模の拡大、
    アルゴリズムの改良、エコシステムの拡大

    </div></div>

  <h3>DQN</h3>
    <div class="hidden_box">
    <label for="label-dqn">[+]</label>
    <input type="checkbox" id="label-dqn"/>
    <div class="hidden_show">

    損失関数をQLearningに入れる。

    </div></div>

  <h3>改良版アルゴリズム</h3>
    <div class="hidden_box">
    <label for="label-rainbow">[+]</label>
    <input type="checkbox" id="label-rainbow"/>
    <div class="hidden_show">
     <dl>
       Rainbow, Hessel et al 2017
     </dl>
     <dd> ポリシーベース、状態価値関数、モデルベースの強化学習 </dd>

    </div></div>

<h2>参考文献、サイト</h2>
  <dd>
  <dt><cite>Akiba, Takuya, Shuji Suzuki, and Keisuke Fukuda "Extremely Large Minibatch Sgd: Training Resnet-50 on Imagenet in 15 Miniutes", 2017, arXiv Preprint.</cite>
  </dt>
  <dd></dd>
  Yee Wyne Teh NIPS 2017

  <dt> [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) 再解釈論文 (2016)</dt>
  <dd> 変分オートエンコーダを再解釈した論文</dd>

<div class = "end_of_page"></div>

  </main>
</body>
</html>
