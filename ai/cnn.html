<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai - convolution neural network</title>
<base href="../" />


<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
      macros: {
      x: "{\\times}",
      bm: ["{\\boldsymbol{#1}}",1],
      dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
      },
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->

<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - AI
      - convolutional neural network 
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：
</div>
</div>

<div class=site-header-margin> </div>

<!--*********************************本文*********************************-->
<h2>convolutional neural network(cnn, 畳み込みニューラルネットワーク)</h2>
  <h3>概要</h3>
    <div class="hidden_box">
    <label for="label-cnn">[+]　　　</label>
    <input type="checkbox" id="label-cnn"/>
    <div class="hidden_show">

  「畳み込み層」という層を使って特徴抽出を行うニューラルネットワーク。
  全結合でなく畳み込みを使うことでパラメータが少なくなる。
    <h4>ニューラルネットワークの問題</h4>
      <h5>計算量の問題</h5>
      例えば、
        全結合：224*224*3*96*55*55
        畳み込みニューラルネットワーク：11*11*3*96*55*55
      <h5>credit assignment problems(信用割当問題)</h5>
        全結合をすると、責任の所在が曖昧になる


    </div></div>
  <h3>用語</h3>
    <div class="hidden_box">
    <label for="label-cnn-words">[+]　　　</label>
    <input type="checkbox" id="label-cnn-words"/>
    <div class="hidden_show">

  <dl>
    <dt>全結合層</dt>
    <dd>全ての上位層から全ての下位層にネットワークが伝播する。責任の所在がわからなくなる。</dd>
    <dt>畳み込み層</dt>
    <dd>入力データの１部分に注目し、その部分画像の特徴を調べる層,カーネルと一部分の内積を求める。特徴はデータや損失関数により自動的に定まるが、入力層に近いところでは点や線といった低次元概念、出力層に近いところでは目や鼻といった高次元概念の注目をする。各特徴に１つのフィルターを使う</dd>
    <dt>プーリング層</dt>
    <dd>畳み込み層の出力を縮約し、データ量を削減する層、maxプーリングやaverageプーリングなどでデータの圧縮を実現する</dd>
    <dt>カーネル</dt>
    <dd>畳み込みに使用する重み行列(フィルタ)</dd>
    <dt>maxプーリング</dt>
    <dd>元行列の最大値を出力するプーリング</dd>
    <dt>サブサンプリング</dt>
    <dd></dd>
    <dt>averageプーリング</dt>
    <dd>元行列の平均値を出力するプーリング</dd>
    <dt>filters(keras)</dt>
    <dd>生成する特徴マップの数、filterはランダムに選ばれる</dd>
    <dt>kernel_size(keras)</dt>
    <dd>カーネル行列の大きさ</dd>
    <dt>strides(keras)</dt>
    <dd>カーネルを動かす距離</dd>
    <dt>padding(keras)</dt>
    <dd>入力画像の縮小を抑えるため、入力画像の周辺を0で埋めること</dd>
    <dt>pool_size(keras)</dt>
    <dd>一度にpoolingを適用する領域のサイズ</dd>
    <dt>global average pooling</dt>
    <dd>1つの特徴マップに 1 つのクラスを対応させることで分類を行う</dd>
    <dt>次元の祝福</dt>
    <dd>弁別可能性が高まることや、ミクロとマクロを結びつけることでシンプルに書けること。</dd>
  </dl>
  </div></div>

  <h3>3層パーセプトロンの関数近似としての完全性</h3>
    <div class="hidden_box">
    <label for="label-3perceptron">[+]　　　</label>
    <input type="checkbox" id="label-3perceptron"/>
    <div class="hidden_show">

    ３層あればどんな関数も作成できることが証明できる。
    <h4>万能近似定理</h4>
    どんな関数を学習するかにかかわらず、大きなmlpであれば、その関数を表現できるが、訓練可能であることを保証しない。
    <h4>ノーフリーランチ定理</h4>
      <q>コスト関数の局地を探索するあらゆるアルゴリズムは、全ての可能なコスト関数に適用した結果を平均すると同じ性能となる。</q>[1]という組み合わせ最適化の領域の定理。
    <h4>層の数と表現力</h4>
      浅いネットワークであれば、一層あたりのニューロンが増えてしまう。
      </div></div>

  <h3>実装</h3>
    <div class="hidden_box">
    <label for="label-cnn-implementation">[+]　　　</label>
    <input type="checkbox" id="label-cnn-implementation"/>
    <div class="hidden_show">

    <h4>python + keras</h4>
<!--==============================コード==============================-->
<pre> <code> # モデルの定義
model = sequential()

// 畳み込み層の挿入
model.add(conv2d(filters=32, kernel_size=(3, 3), input_shape=(28,28,1))) 

// 活性化関数の挿入
model.add(activation('relu')) 

// プーリング層の挿入
model.add(maxpooling2d(pool_size=(2, 2)))

// 入力にドロップアウトを適用(ランダムに入力ユニットを0とする)
model.add(dropout(0.25))
// 入力を平坦化
model.add(flatten())
// 全結合レイヤーの挿入
model.add(dense(128))

// アクティベーション層の導入(softmax)
model.add(activation('softmax'))

// 学習のためのモデルを設定
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])

// 固定回数の施行で学習
model.fit(x_train, y_train,
          batch_size=128,
          epochs=1,
          verbose=1,
          validation_data=(x_test, y_test)) </code> </pre>
<!--==================================================================-->
    </div></div>


<h3>参考文献、サイト</h3>
  <dd>
  <dt> <cite><a href="https://pdfs.semanticscholar.org/8bdf/dc2c2777b395c086810c03a8cdeccc55c4db.pdf">Wolpert, D.H., Macready, W.G. "No Free Lunch Theorems for Search", Technical Report SFI-TR-95-02-010,1995, Santa Fe Institute </a></cite></dt>
  <dd>ノーフリーランチ定理</dd>
  <dt><cite><a href="http://bookclub.kodansha.co.jp/product?item=0000147653">岡谷貴之,"深層学習(機械学習プロフェッショナルシリーズ)", 2015, 講談社</a></cite></dt>
  <dd>深層学習の基礎を網羅している本。</dd>
  <dt><cite>Akiba, Takuya, Shuji Suzuki, and Keisuke Fukuda "Extremely Large Minibatch Sgd: Training Resnet-50 on Imagenet in 15 Miniutes", 2017, arXiv Preprint.</cite>
  </dt>
  <dd></dd>
  Yee Wyne Teh NIPS 2017

  <dt> [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) 再解釈論文 (2016)</dt>
  <dd> 変分オートエンコーダを再解釈した論文</dd>

<div class = "end_of_page_margin"></div>
<div class = "end_of_page">
<a class="prev" href="ai/deep_learning.html" >前：深層学習の基本</a>
<a class="upper" href="index.html" >上：ホーム</a>
<a class="next" href="ai/rnn.html">次：再帰型ニューラルネットワーク</a>
</div>

  </main>
</body>
</html>
