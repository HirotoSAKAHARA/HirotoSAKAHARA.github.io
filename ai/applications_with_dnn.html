<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai - application with dnn</title>
<base href="../" />


<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
      macros: {
      x: "{\\times}",
      bm: ["{\\boldsymbol{#1}}",1],
      dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
      },
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true,
      tags: "ams",
      autoload: {
        color: [],
        colorV2: ['color']
      },
      packages: {'[+]': ['noerrors']}
    },
    chtml: {
      matchFontHeight: false,
      displayAlign: "left",
      displayIndent: "2em"
    },
    options: {
      renderActions: {
        /* add a new named action to render <script type="math/tex"> */
        find_script_mathtex: [10, function (doc) {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/);
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
            const text = document.createTextNode('');
            node.parentNode.replaceChild(text, node);
            math.start = {node: text, delim: '', n: 0};
            math.end = {node: text, delim: '', n: 0};
            doc.math.push(math);
          }
        }, '']
      }
    },
    loader: {
      load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - AI 
      - applications with dnn
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->

<h2>深層学習を使った応用事例</h2>
  <h3>深層学習を用いた自然言語処理</h3>
    <div class="hidden_box">
    <label for="label-nlp">[+]　　　</label>
    <input type="checkbox" id="label-nlp"/>
    <div class="hidden_show">

    <h4>利点</h4>
      深層学習を使わないベクトル表現では、ベクトルの次元が大きくなりすぎたり、意味を扱えなかったりするが、
      深層学習を使うと、誤差逆伝播法により、わずか数百次元のベクトルで単語の意味を表現できるようになる。

    <h4>word2vec</h4>
      "king" - "man" + "woman" = "Queen" <br>
      単語埋め込み(Embedding)モデル。単語のベクトルを用いて、単語の足し算、引き算が可能となる。同じように文法も解ける。
      BOWや単語文書行列では、それぞれの単語が0,1の入力層が数万個で表示されていたのが、まとめてベクトルとすると、
      ベクトルが学習によって意味を表すようになってきた。
      自分以外の周りの単語を予測するスキップグラムとその逆のCBOWで学習させると中間層に意味の表層ができる。

    <h4>doc2vec</h4>
      word2vecを応用した文章をベクトル化する技術で、文の語順も特徴として考慮に入れることができる。

    <h4>Sequence-to-Sequence <span class="small">: Encoder-Decoder、系列変換モデル</span></h4>
      ソースとターゲットで２つのRNN(LSTM)を使用する手法
      ソースに系列（文章など)を入れ、ソース側で双方向リカレントニューラルネットワークを使用して、
      束ねてベクトル情報に圧縮してターゲットに入力し、ターゲット側で新たな系列を生成する手法。

      ソースからの複数の出力のうちのどれを使うのかをattentionとして出力側が決定する。
      中間層の一工夫として、入力系列(文章)は逆順に入れると精度が上がることが報告されている。

    <h4>Attention Mechanism <span class="small">注意機構</span></h4>
      Sequence-to-Sequenceモデルの２つのRNN間の情報伝達を改善する手法で、長いネットワークにおいて、
      最初に入力された情報が復号化器まで伝播しづらくなるのを解決するために、直接的に入力情報を出力時に利用する。
      具体的には、各単語に対して、各時刻で出力されたベクトルの重み付き平均を計算し、それを利用する。



    </div></div>

<h3>参考文献、サイト</h3>
  <dd>
  <dt> <cite><a href="https://pdfs.semanticscholar.org/8bdf/dc2c2777b395c086810c03a8cdeccc55c4db.pdf">Wolpert, D.H., Macready, W.G. "No Free Lunch Theorems for Search", Technical Report SFI-TR-95-02-010,1995, Santa Fe Institute </a></cite></dt>
  <dd>ノーフリーランチ定理</dd>
  <dt><cite><a href="http://bookclub.kodansha.co.jp/product?item=0000147653">岡谷貴之,"深層学習(機械学習プロフェッショナルシリーズ)", 2015, 講談社</a></cite></dt>
  <dd>深層学習の基礎を網羅している本。</dd>
  <dt><cite>Akiba, Takuya, Shuji Suzuki, and Keisuke Fukuda "Extremely Large Minibatch Sgd: Training Resnet-50 on Imagenet in 15 Miniutes", 2017, arXiv Preprint.</cite>
  </dt>
  <dd></dd>
  Yee Wyne Teh NIPS 2017

  <dt> [[1606.05908] Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) 再解釈論文 (2016)</dt>
  <dd> 変分オートエンコーダを再解釈した論文</dd>

<div class = "end_of_page_margin"></div>
<div class = "end_of_page">
<a class="prev" href="ai/reinforcement_learning_with_dnn.html" >前：深層学習による強化学習</a>
<a class="upper" href="index.html" >上：ホーム</a>
<a class="next" href="ai/cautions.html">次：人工知能使用上の注意</a>
</div>

  </main>
</body>
</html>
