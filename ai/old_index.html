<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai</title>
<base href="../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
      tex: {
        macros: {
        x: "{\\times}",
        bm: ["{\\boldsymbol{#1}}",1],
        dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
        },
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true,
        tags: "ams",
        autoload: {
          color: [],
          colorV2: ['color']
        },
        packages: {'[+]': ['noerrors']}
      },
      chtml: {
        matchFontHeight: false,
        displayAlign: "left",
        displayIndent: "2em"
      },
      options: {
        renderActions: {
          /* add a new named action to render <script type="math/tex"> */
          find_script_mathtex: [10, function (doc) {
            for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            }
          }, '']
        }
      },
      loader: {
        load: ['[tex]/noerrors']
      }
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle">
      - AI
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：異常検知の項目を追加
</div>

 <div class="navigation">
<ul>
  <li><a href="index.html">home</a></li>
  <li><a href="mobile_robot/index.html">移動ロボット</a></li>
  <li><a href="ai/index.html">AI</a></li>
  <li><a href="math/index.html">数学</a></li>
  <li><a href="other/index.html">その他</a></li>
</ul>
</div>
</div>

<!--*********************************本文*********************************-->
<div class=site-header-margin>
</div>

<h1>人工知能(AI)</h1>

<h2>基本</h2>
  <h3>人工知能とは</h3>
    <div class="hidden_box">
    <label for="label_about_ai">[+]　　　</label>
    <input type="checkbox" id="label_about_ai"/>
    <div class="hidden_show">
    <h4>定義</h4>
      専門家の間ですら共有されている定義はない。
      しかし、<q>「人工知能」が、推論、認識、判断など、人間と同じ知的な処理能力を持つ機械（情報処理システム）であるという点については、
      大多数の研究者の意見は一致しているといってもよい」</q>[1]　

    <h4>人工知能の種類</h4>
      人工知能には様々な手法が含まれる。
      機械学習と呼ばれる手法は大量のデータによって予測を行う人工知能の一種であり、
      深層学習はその中でもニューラルネットと呼ばれるアルゴリズムの特殊なものである。
    <div>
      <img src="out\ai\kind_of_ai.png" alt="aiの種類"/>
    </div>

    <h4>機械学習とデータマイニング</h4>
      大量のデータを利用して特徴を見つけ出す手法には、機械学習とデータマイニングがある。
      これらは使用する技法は同じだが目的が異なる。
      機械学習の目的は発見したデータの特徴からあるデータに付随するデータの値を予測することであり、
      データマイニング目的は未知のデータの特徴を発見することである。
      データマイニングは英語ではknowledge-discovery in databasesの頭文字をとってKDDとも呼ばれる。
    </div></div>

  <h3>教師の有無による分類</h3>
    <div class="hidden_box">
    <label for="label_classify_by_teacher">[+]　　　</label>
    <input type="checkbox" id="label_classify_by_teacher"/>
    <div class="hidden_show">
      大きく分けて、教師あり学習、教師なし学習、強化学習にわけられる。教師データの解釈の仕方で分類が変わりうる。

    <h4>supervised learning<span class="small"> : 教師あり学習</span></h4>
      事前に与えられたデータをいわば例題とみなして、それをガイドに学習を行う。
      学習後は、入力に対して尤度が最大となる出力を求める。

    <h4>unsuperviesd learning<span class="small"> : 教師なし学習</span></h4>
      学習データを教えることによって、それぞれのデータの分類やデータ群の特徴の抽出を行う。

    <h4>reinforcement learning<span class="small"> : 強化学習</span></h4>
      行動の結果が行動の後にスカラ値の報酬として与えられる。
      報酬は即時に与えられるとは限らず、報酬は行動の改善の方法を与えない。
    </div></div>

  <h3>人工知能により解決するタスクの種類</h3>
    <div class="hidden_box">
    <label for="label_classify_by_task">[+]　　　</label>
    <input type="checkbox" id="label_classify_by_task"/>
    <div class="hidden_show">
      
    <h4>classification <span class="small"> : 分類</span></h4>
      教師あり学習の一種で、二値分類と多クラス分類がある。
      <h5>二値分類</h5>
      <q>入力\(\boldsymbol{x}\)を内容に応じて２種類に区別する問題</q>[2]
      <h5>多クラス分類</h5>
      <q>入力\(\boldsymbol{x}\)を内容に応じて有限個のクラスに分類する問題</q>[2]

    <h4>regression<span class="small"> : 回帰</span></h4>
      教師あり学習の一種で、訓練データをよく再現するような関数を定める問題。連続値をとる関数を主な対象とする。
      <h5>線形回帰</h5>
      \(z = w_0 + w_1u_1 + \cdots + w_uu_n + e\) のような、変数\(\boldsymbol{u}\)の線型結合で示される関数の係数\(\boldsymbol{w}\)を求める問題。
      多項回帰のように、変数自体は非線形の関数で表されても構わない。
      以下に分類される。
      <dl>
        <dt>単回帰</dt>
        <dd>変数が１つの１次式(\( z = wu + w_0 \) )の回帰。導出される1次式を回帰直線と呼ぶ。</dd>
        <dt>多項回帰</dt>
        <dd>変数が１つのn次式(\( y = w_1u + w_2u^2 + \cdot + w_nu^n + w_0 \))の回帰</dd>
        <dt>重回帰</dt>
        <dd>変数が複数ある式(\( y = w_1u_1 + w_2u_2 + \cdot + w_nu_n + w_0 \))の回帰</dd>
      </dl>
      <h5>非線形回帰</h5>
        観測から得られたデータがモデルパラメータの非線形結合であり、1つ以上の独立した変数に依存する関数によってモデル化される。

    <h4>clustering<span class="small"> : クラスタリング</span></h4>
      教師なし学習の一種で、ラベルのないデータ群を複数に分る問題。

    <h4>dimensionality reduction<span class="small"> : 次元削減(次元圧縮)</span></h4>
      教師なし学習の一種で、
      データの次元数を減らすこと。 計算の効率化を行ったり、データ群の特徴をつかむことが目標。
      線形の場合はデータ\(\boldsymbol{Y}\)を低次元の\(\boldsymbol{W}\)と
      \( \boldsymbol{X}\)を用いて\( \boldsymbol{Y} \simeq \boldsymbol{W^TX}\)と表せるが、
      非線形の次元削減手法も存在する。
      
    <h4>reinforcement learning<span class="small"> : 強化学習</span></h4>  
      各時刻に置ける最適な意思決定のルールを目的とする。
      報酬の期待値を最大するような逐次的意思決定ルール(ポリシー)を学習する。
    <h4>Recommendation <span class="small"> : 推薦</span></h4>
      情報フィルタリング技法の一種で、特定ユーザーが興味を持つと思われる情報を提示するもの。 

    <h4>bayesian optimization<span class="small"> : ベイズ最適化</span></h4>  
      教師なし学習の一種で、あるデータの存在領域を学習により狭めていく。

    </div></div>
    
    <h3>機械学習の基礎 <span class="small">(教師あり学習、教師なし学習、データマイニング)</span></h3>  
      <div class="hidden_box">
      <label for="label_procedure">[+]　　　</label>
      <input type="checkbox" id="label_procedure"/>
      <div class="hidden_show">
      <h4>教師あり学習、教師なし学習の手順</h4>
        教師あり学習、教師なし学習の学習の手順を示す。用語については次節で示す。
        データは学習に利用する訓練データと検証に利用する検証データ、テストデータに分割するが、
        これは、モデルが学習のし過ぎによりオーバーフィッティング状態となり、
        汎化性能が下がっていないことを確認するためである。
      <div>
        <img src="out\ai\learning_procedure.png" alt="学習手順"/>
      </div>

      <h4>CRISP-DM<span class="small"> : cross industry standard process for data mining-DM</span></h4>
        <div>
        CRISP-DMコンソーシアムによって提唱されたデータ分析プロジェクトのプロセスモデル。
        </div>
        
        <div>
        1.ビジネス理解で課題を明確にし、2.データ理解でデータを取得、3.データ準備でデータを整形し、4.モデリングで分析、
        その結果を5.評価して、6.適用、あるいは、もう一度サイクルを回し、考え直す。というやり方。
        </div>
        <img src="out\ai\CRISP-DM.png" alt="CRISP-DM"/>
    
      <h4>KDD<span class="small"> : Knowledge Discovery in Databases</span></h4>
        <div>
        データマイニングにより、データから知識を探索するためのプロセスモデル。CRISP-DMと比較し、よりデータ分析部分にフォーカスしており、以下の手順ですすめる。[6]
        </div>
        <div>
      <ol>
        <li> アプリケーションドメインの理解を構築し、消費者の視点でkddプロセスの目標を決定する。</li>
        <li> 発見がなされるデータ・セットを選択し、または変数やデータサンプルのサブセットに注目し、目的データ・セットを構築する。</li>
        <li> 適切なノイズ除去や時系列情報や変化を利用した欠損データ対策などのデータクレンジングと前処理を行う。</li>
        <li> タスクの目標に依存するデータの特徴を見つけるために、次元削減やデータ変換をし、変数の削減あるいはデータの不変量を見つける。</li>
        <li> kddプロセスの目標を特定のデータマイニング手法に合わせる。例えば、要約、分類、回帰、クラスタリングなど。</li>
        <li> データのパターンを探索するデータマイニングアルゴリズムの選択する。</li>
        <li> データマイニングを行う。</li>
        <li> マイニングされたパターンの解釈を行う。特に、モデルパターンの可視化や得られたパターンから与えられたデータの可視化を含む。</li>
        <li> 発見した知識を他のシステムと統合するか、あるいは単に興味ある人々のために文書化する。</li>
      </ol>

      <img src="out\ai\kdd.png" alt="kdd" width="100%"/>
      </div>  

      <h4>用語</h4>
        <h5>data ingestion<span class="small"> : データインジェスチョン</span></h5>
          csv, excel, xml, 紙, 音声などの様々な媒体、形式で保存されている様々な形式のデータを
          同じ形式で利用できるよう加工すること。通常はデータベースに格納する。

        <h5>data cleansing<span class="small"> : データクレンジング</span></h5>
          データを検査し、不正なデータや、同じデータ、文法異常のデータなどを修正、破棄することで、
          データの質（有効性、正確さ、完全さ、一貫性、統一性）を上げること。

        <h5>training dataset<span class="small"> : 訓練データ</span></h5>
          モデルを適合するためのデータのサンプル

        <h5>validation dataset<span class="small"> : 検証データ</span></h5>
          訓練データで適合したモデルを評価し、
          チューニングの要否やチューニングの方向性を決めるためのデータのサンプル。
          
        <h5>test dataset<span class="small"> : テストデータ</span></h5>
          訓練データで適合され、検証データを使いチューニングされた
          最終的なモデルの評価のために使用されるデータのサンプル。

        <h5>generalization performance<span class="small"> : 汎化性能</span></h5>
          テストデータなど、未学習のデータでの性能。

        <h5>overfitting <span class="small"> : 過学習, 過剰適合</span></h5>
          訓練データを学習させすぎた結果、訓練データの特徴のみに特化しすぎ、汎化性能が低くなってしまうこと。

        <h5>underfitting <span class="small"> : 未学習</span></h5>
          学習が足りず、訓練データのみならず、テストデータでも性能が出ないこと。


      </div></div>

    <h3>機械学習の基礎 <span class="small">(強化学習)</span></h3>  
      <div class="hidden_box">
      <label for="label_reinforcement_learning">[+]　　　</label>
      <input type="checkbox" id="label_reinforcement_learning"/>
      <div class="hidden_show">
      <h4>概要</h4>
        エージェントが行動し、行動によって環境が変化し、
        環境から状態と報酬が与えられるようなシナリオで、
        累積報酬の最大化を達成できるようなポリシーを学習することを目的とする。
        <div>
          <img src="out\ai\reinforcement_learning.png" alt="学習手順"/>
        </div>
      <h4>特徴</h4>
        エージェントが探索し、探索の後で報酬が与えられるということはほかの学習と大きく異なる。
        行動により環境が変化するために、探索と活用のトレードオフが発生する。
  
      <h4>MDP <span class="small">: Marcov Decision Process、マルコフ決定過程</span></h4>
        状態の組と、行動の組、初期状態が各状態での遷移確率とある状態からある状態に遷移したときの報酬の期待値が定義されている状態で、エージェントがとるポリシーを決定する。強化学習は、マルコフ決定過程におけるポリシーの最適化問題と言える。(下図はwikipediaの図：<a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B#/media/%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Markov_Decision_Process_example.png">３つの状態と２つの行動を持つ簡単なMDPの例</a>をplantumlで書いたもの)
        <div>
          <img src="out\ai\marcov_decision_process.png" alt="マルコフ決定過程"/>
        </div>

      <h4>用語</h4>
        <h5>エージェント</h5>
        制御器や意思決定者</dd>

        <h5>環境</h5>
        制御対称システム</dd>

        <h5>行動</h5>
        エージェントが環境を変化させる行為

        <h5>報酬</h5>
        その行動が良かったか悪かったかを示すスカラ値。行動への具体的なヒントにならない。
        行動の行った後、遅れて示される。

        <h5>状態</h5> 
        環境が現在どうなっているかを表すパラメータ。

        <h5>ポリシー</h5> 
        報酬の期待値を最大にするような逐次的意思決定ルール。

        <h5>状態価値関数</h5> 
        ある状態から、あるポリシーで行動を続けた時の報酬の総和。

        <h5>行動価値関数</h5> 
        ある状態から、ある行動をとった後、あるポリシーで行動を続けた時の報酬の総和。

        <h5>価値関数</h5>
        あるポリシーを行ったときの、報酬の期待値。 状態価値関数vと行動価値関数qがある。

        <h5>最適価値関数</h5>
        価値関数の中で最大値となるもの。一般にはベルマン方程式となり、計算するのは非常に難しい。

        <h5>収益</h5>
        即時報酬＋Σ(割引率*未来の報酬)。

        <h5>TD学習</h5>
        モンテカルロ法と動的計画法を合わせた方法で、経験的に学習を行いつつ、
        途中で評価値を更新していく学習手法。Q学習やActor-Critic法、TD(\(\lambda\))法はこの学習法の一つ。

        <h5>探索と利用のジレンマ</h5>
        ある行動が最適かどうかは別の行動をしないとわからないが、最良の行動と決定している別の行動ばかりしていると良い行動を選べない。

    </div></div>

    <h3>その他基本的なトピック</h3>  
      <div class="hidden_box">
      <label for="label_basic_topics">[+]　　　</label>
      <input type="checkbox" id="label_basic_topics"/>
      <div class="hidden_show">

      <h4>基本用語</h4>
       <h5>sota</h5>
          あるタスクにおいて、その時の最高性能が出せるアルゴリズム

        <h5>boostrap method :<span class="small">ブートストラップ法</span></h5>
          ある限られた標本集団から母集団の性質を推定するための方法で、
          繰り返しを許してランダムにn個の標本を取得し、
          その標本から得られる推定値の分布からパラメータの確率分布や誤差を推定する方法。

        <h5>ensemble learning<span class="small"> : アンサンブル学習</span></h5>
          より良い予測性能を出すために、複数のアルゴリズムを用いること。

        <h5>bagging <spam class="small"> : バギング</spam></h5>
          訓練サンプル集合からブーストラップサンプル集合を複数作成し、
          それぞれのサンプル集合に対し独立なモデルを複数学習させ、
          すべてのモデルの予測値をもとに、最終的な予測値を決定する方法。
          モデル固有の誤差が相殺されるため、性能が安定する。

        <h5>boosting <spam class="small"> : ブースティング</spam></h5>
          一連の弱い学習機をまとめることで強い学習機を生成する機械学習のメタアルゴリズム。
          弱い学習機とは真の分類と若干の相関がある分類器で、強い学習機とは真の分類と良く相関する学習機のこと。
          複数の弱い学習機に学習させた後、学習に失敗したデータの重みづけが見直され、
          別の弱い学習機によって再学習を行う。
          難しいデータを正しく分類した学習機を重みを高め、学習機を統合する。


        <h5>階層的クラスタリング</h5>
          データの一部をクラスタリングし、クラスタリングしたクラスを一つのデータととらえ、残りのデータとともに次のクラスタリングを行う。
        <h5>非階層的クラスタリング</h5>
          すべてのデータを一気にクラスタリングする。データ量が多い場合に有効な手法。
        <h5>正例・負例</h5>
          ２クラス分類の学習用データで値が１となってるクラスを正例、値が−１あるいは０となっているクラスを負例と呼ぶ。
        
      </div></div>


<h2>データ前処理の手法</h2>
  <h3>データ量が少ない場合への対応</h3>
    <div class="hidden_box">
    <label for="label-data-extension">[+]　　　</label>
    <input type="checkbox" id="label-data-extension"/>
    <div class="hidden_show">

    <h4>data augmentation<span class="small"> : データ拡張、データ水増し</span></h4>
      <h5>画像処理</h5>
        回転、平行移動、拡大・縮小、反転、アフィン変換、剪断変換、ホモグラフィ変換、色変換、光量変換、ノイズ追加などを利用する。

      <h5>mixup</h5>
        データ間を線形に補間したものを使用する。この時同時にラベルも学習する。
        分類問題のときは、0でも1でもないデータができる。

      <h5>random erasing</h5>
        画像の一部をランダムに欠損させる。

    <h4>transfer learning<span class="small"> : 転移学習</span></h4>
      別の学習に使用されたモデルの重み付けをそのまま利用したモデルを用意し、少ないデータで学習する。

    </div></div>


  <h3>データ欠損への対応</h3>
    <div class="hidden_box">
    <label for="label8-1">[+]　　　</label>
    <input type="checkbox" id="label8-1"/>
    <div class="hidden_show">

    <h4>データ欠損のパターン</h4>
    <dl>
      <dt>MCAR <span class="small"> : missing completely at random</span></dt>
      <dd>あるデータ値が欠損する確率が、全データと無関係。</dd>
      <dt>MAR<span class="small"> : missing at random</span></dt>
      <dd>ある項目が欠損する確率が、欠損している項目以外の観測されたデータ項目のみから推測できる。例として女性は一定の確率で年齢を答えない場合など。</dd>
      <dt>NMAR<span class="small"> : not missing at random</span></dt>
      <dd>ある項目が欠損する確率がその項目そのものに依存し、また、その項目以外のデータ項目からも、欠損している項目の欠損率を推測できない。例としては、男女変わらず、年齢が高くなるほど年齢について答えなくなる場合など。この場合の欠損は代入法では解決できない。</dd>
    </dl>
    <h4>リストワイズ削除</h4>
        欠損値が含まれるデータ行をすべて削除する。

    <h4>ペアワイズ削除</h4>
      欠損の少ない列を残し、その列において欠損値が含まれるデータ行をすべて削除する。

    <h4>平均値代入法</h4>
      欠損値をその列、あるいは行の平均値によって穴埋めする方法。分散や誤差を知りたい場合は使えない。
      
    <h4>回帰代入法</h4>
      欠損値を除いた状態で回帰分析を行い、推定値を補完する。

    <h4>確率的回帰代入法</h4>
      回帰代入法により推定した値に誤差をランダムに加えて補完する。

    <h4>ホットデック法</h4>
      欠損値を含むデータ行（レシピエント）に欠損値以外の属性の似ているデータ行（ドナー)の値を使って補完。
      ドナーは最近傍法などで探索。

    <h4>多重代入法</h4>
      観測されたデータから欠損値を予測して補完する完全なデータセットを複数作成し、
      各セット毎に分析モデルを構築、最後に各分析モデルを統合する。

    </div></div>

    <h3>外れ値への対応</h3>
    <div class="hidden_box">
    <label for="label8-2">[+]　　　</label>
    <input type="checkbox" id="label8-2"/>
    <div class="hidden_show">

    <h4>LOF<span class="small"> : local outlier factor</span></h4>
      データの密度に基づいて外れ値を検知する。
      <ul>
        <li>近くにデータ点が少ないのが外れ値であると考える</li>
        <li>k個の近傍点を使ってデータの密度を推定する</li>
        <li>上記の密度が、周囲と相対的に低い点を外れ値と判定する</li>
      </ul>
    <h4>isolation forest</h4>
      軸毎にランダムにデータ分割を行う空間分割ツリーを作成した時に、
      早々にデータ分割ができなくなるデータを外れ値とする。
      <ul>
        <li>距離や密度に依存しないため、それらの指標を計算するコストが不要</li>
        <li>計算が複雑でなく、省メモリである</li>
        <li>大規模データであっても計算をスケールさせやすい</li>
      </ul>
    </div></div>

  <h3>不均衡データへの対応</h3>
    <div class="hidden_box">
    <label for="label8-3">[+]　　　</label>
    <input type="checkbox" id="label8-3"/>
    <div class="hidden_show">

    <h4>オーバーサンプリング</h4>
      頻度が少ない値を含むデータを複製して増加させる。SMOTE(Synthetic minority over-sampling technique)などの手法がある。
    <h4>アンダーサンプリング</h4>
      頻度が多い値を含むデータを削減する。ENN(Edited Nearest Neighbours)などの手法がある。
    <h4>アンダーサンプリングとアンダーサンプリングの組み合わせ</h4>
      SMOTE-ENNなどの手法がある。
      
    </div></div>

  <h3>データ形式の変換</h3>
    <div class="hidden_box">
    <label for="label8-31">[+]　　　</label>
    <input type="checkbox" id="label8-31"/>
    <div class="hidden_show">

    <h4>連続値のカテゴリ化</h4>
      連続値をカテゴリ値に変換する。

    <h4>カテゴリデータのダミー変数化</h4>
      カテゴリデータを各カテゴリを示す変数に分け、排他となる２値で表す。

    <h4>横持ちデータと縦持ちデータの変換</h4>
    人が情報を認識しやすい横持ちデータと、
    情報量が変化してもデータ構造を変更する必要のない縦持ちデータがあり、
    データ分析モデルによりどちらかの形式に変換する必要がある。
    <table>
      <caption>横持ちデータ</caption>
      <tr>
        <th>id</th>
        <th>item1</th>
        <th>item2</th>
        <th>item3</th>
      </tr>
      <tr>
        <td>id1</td>
        <td>value 11</td>
        <td>value 12</td>
        <td>value 13</td>
      </tr>
      <tr>
        <td>id2</td>
        <td>value 21</td>
        <td>value 22</td>
        <td>value 23</td>
      </tr>
    </table>

    <table>
      <caption>縦持ちデータ</caption>
      <tr>
        <th>id</th>
        <th>items</th>
        <th>value</th>
      </tr>
      <tr>
        <td>id1</td>
        <td>item1</td>
        <td>value11</td>
      </tr>
      <tr>
        <td>id1</td>
        <td>item2</td>
        <td>value12</td>
      </tr>
      <tr>
        <td>id1</td>
        <td>item2</td>
        <td>value13</td>
      </tr>
      <tr>
        <td>id2</td>
        <td>item1</td>
        <td>value21</td>
      </tr>
      <tr>
        <td>id2</td>
        <td>item2</td>
        <td>value22</td>
      </tr>
      <tr>
        <td>id2</td>
        <td>item3</td>
        <td>value23</td>
      </tr>

    </table>

    </div></div>

  <h3>雑音除去</h3>
    <div class="hidden_box">
    <label for="label8-4">[+]　　　</label>
    <input type="checkbox" id="label8-4"/>
    <div class="hidden_show">

    <h4>平準化</h4>
    ローパスフィルタを通してスパイクを除去する。

    <h4>ICA</h4>

    </div></div>

    <h3>normalization<span class="small"> : 正規化</span></h3>
    <div class="hidden_box">
    <label for="label8-5">[+]　　　</label>
    <input type="checkbox" id="label8-5"/>
    <div class="hidden_show">

    <h4>センタリング</h4>
    平均値を求めた後全データから平均値を引き、平均を0にする

    <h4>standarization<span class="small"> : 標準化</span></h4>
    特徴を平均0、分散1にすることで、特徴ごとのデータ分布を近づける。
    ImageDataGeneratorを使う場合は以下のようになる。
<pre> <code># ジェネレーターの生成
data_generator = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)

# 標準化
g = data_generator.flow(X_train, y_train, shuffle=False)
X_batch, y_batch = g.next() </code> </pre>

    <h4>Box-Cox変換</h4>
    以下の式に示すように\(x_i\)を\(y_i\)に変換することで、データを正規分布に近づける。
    線形獣回帰分析など、正規分布を仮定しているようなモデルの精度を高めることができる。
    $$
      y_i^\lambda = 
        \begin{cases}
          \frac{x_i^\lambda - 1}{\lambda} & (\lambda \neq 0) \\
          \log{(x_i)} & (\lambda = 0)
        \end{cases}
    $$


    <h4>whitening<span class="small"> : 白色化</span></h4>
        特徴間の相関をなくし、分散を一定にする。
<pre> <code># ジェネレーターの生成
datagen = ImageDataGenerator(featurewise_center=True, zca_whitening=True)

# 標準化
g = datagen.flow(X_train, y_train, shuffle=False)
X_batch, y_batch = g.next() </code> </pre>

    <h4>batch normalization<span class="small"> : バッチ正規化 </span></h4>
    ミニバッチ学習の際にバッチごとに標準化を行うこと。
    Kerasの実装は以下の通り。
<pre> <code> model.add(BatchNormalization()) </code> </pre>

    <h4>重み正規化</h4>
      入力xではなく、重み係数wを正規化(平均０分散１）
    <h4>層正規化</h4>
      層内の全ニューロンを正規化(平均０分散１）する。入力データが１でも正則化可能。

    <h4>ZCA<span class="small"> : zero-phase component analysis, ゼロ位相成分分析</span></h4>
      白色化の線形変換に使用する行列を対称行列に制限する。

    <h4>LRN<span class="small"> : local response  normalization, 局所的応答正規化</span></h4>
      同一位置(ピクセル)において複数の特徴マップ間で正規化する。[4]

    <h4>GCN<span class="small"> : global contrast normalization, 大域コントラスト正規化</span></h4>
      特徴マップ全体でコントラストを正規化する。

    <h4>LCN<span class="small"> : local contrast normalization, 局所コントラスト正規化</span></h4>
      特徴マップの局所領域内でコントラストを正規化する。
    </div></div>


  <h3>次元削減</h3>
    <div class="hidden_box">
    <label for="label8-6">[+]　　　</label>
    <input type="checkbox" id="label8-6"/>
    <div class="hidden_show">

    <h4>PCA<span class="small"> : principal component analysis, 主成分分析</span></h4>
      主成分分析を行い、不要な次元を削減することで、学習パラメータの大幅削減を行う。

    <h4>SVC<span class="small"> : singular value decomposition, 特異値分解</span></h4>
      特異値分解を行い、不要な次元を削減することで、学習パラメータの大幅削減を行う。
        
      特異値分解とは、行列\(A\)を\( A = U\sum V^{T} \)と分解することである。ここで\(\sum\)の次元は\(A\)と等しく、\(D\)を\(A\)の固有値の対角行列とすると、\( \sum =
   \left(
    \begin{array}{cc}
      D & 0 \\
      0 & 0 
    \end{array}
  \right)
 \)と表される(0の数で次元を合わせる）。\(V \)は特異ベクトルを並べた右特異ベクトル、Uは\(A\)、\(\sum\)、\(V\)から求まる左特異ベクトルである。
    </div></div>

  <h3>自然言語処理におけるデータ前処理</h3>
    <div class="hidden_box">
    <label for="label8-7">[+]　　　</label>
    <input type="checkbox" id="label8-7"/>
    <div class="hidden_show">

    <h4>表記揺れへの対応</h4>
      <h5>表記ゆれ</h5>
        同じ文章の中で、同じ意味の言葉が異なって表記されていること(大文字・小文字、半角・全角、ひらがな、カタカナなど）

      <h5>正規化</h5>
        表記ゆれを防ぐために、ルールベースで文字や数字を変換すること。正規表現を使うことが多い。

    <h5>Padding</h5>
      入力文の長さの不一致があると、行列演算ができなくなる。そこで、長すぎる文を削り、
      短すぎる文には０を埋める処理をして、文章の長さを統一する。

    <h5>ID化</h5>
      単語をID化することで、ニューラルネットに入力として与えれるようにする。

    </div></div>

   
<h2>学習の手法</h2>
  <h3><a href="ai/neural_network.html">neural network<span class="small"> : NN, ニューラルネットワーク</span></a></h3>

    <h3>SVN<span class="small"> : support vector machine, サポートベクターマシン</span></h3>
    <div class="hidden_box">
    <label for="label_svn">[+]　　　</label>
    <input type="checkbox" id="label_svn"/>
    <div class="hidden_show">
      <div>分類、回帰に使われる手法。分類に使われる場合はSupport Vector Classification(SVC)とも</div>

    </div> </div>

  <h3>k nearest neighbor<span class="small"> : k近傍法, 最近傍法</span></h3>
    <div class="hidden_box">
    <label for="label_knn">[+]　　　</label>
    <input type="checkbox" id="label_knn"/>
    <div class="hidden_show">
    <h4>ターゲットタスク</h4>
      分類、回帰、異常検知
    <h4>アルゴリズム</h4>
        ある空間上の入力点のラベルを、隣接のk個の学習済みの点のラベルから決定する。
        \(k = 1 \)の時、最近傍法となる。
        隣接の点群の中の最多のラベルを選ぶ事で分類問題に、
        ラベルの平均や重み付き平均を取ることで回帰問題に適用できる。
    <h4>特徴</h4>
        関数が想定されていないノンパラメトリックの手法で、
        教師データをそのまま丸暗記する怠惰学習(lazy learner)である。
        また、空間の各軸である特徴尺度の選択と距離の計算方法が分類性能に大きな影響を与え、
        最近傍探索アルゴリズムが計算速度に大きな影響を与える。
    <h4>長所</h4>
      <ul>
        <li>単純。</li>
      </ul>
    <h4>短所</h4>
      <ul>
        <li>データの局所的構造に左右されやすい。</li>
        <li>不適切な特徴を入れてしまうことで、正確さが著しく損なわれる。</li>
        <li>ノイズ耐性とクラス境界の明確さがトレードオフとなり、kの決定が難しい。</li>
      </ul>
    <h4>異常検知タスクにおける異常度</h4>
      $$
        \ln\frac{}{\pi_0 N^1(x)}{\pi_1 N^0(x)}
      $$
      \(\pi_0\)は全標本中の正常データの割合、
      \(\pi_1\)は全標本中の異常データの割合、
      \(N^0(x)\)は\(x\)のk近傍中の正常データの割合、
      \(N^1(x)\)は\(x\)のk近傍中の異常データの割合。
    </div></div>

  <h3 id="perceptron">simple perceptron<span class="small"> : 単純パーセプトロン</span></h3>

    <div class="hidden_box">
    <label for="label_perceptron">[+]　　　</label>
    <input type="checkbox" id="label_perceptron"/>
    <div class="hidden_show">

    <h4>ターゲットタスク</h4>
    分類
    <h4>アルゴリズム</h4>
    複数の入力に対し、それぞれに重みを加えたものを足し合わせ、
    その和が0以上の場合は1、以下の場合は0を出力するアルゴリズム。

    <div>
      <img src="out\ai\perceptron.png" alt="パーセプトロン"/>
    </div>

    <h4>長所</h4>
      <ul>
        <li>単純。</li>
      </ul>
    
    <h4>短所</h4>
      <ul>
        <li>１本の判別直線で分離できない問題は解けない。</li>
      </ul>
    </div></div>

  <h3>logistic regression<span class="small"> : ロジスティック回帰</span></h3>
    <div class="hidden_box">
    <label for="label_logistic_regression">[+]　　　</label>
    <input type="checkbox" id="label_logistic_regression"/>
    <div class="hidden_show">

    <h4>ターゲットタスク</h4>
    二値分類、確率やダミー変数について分析、予測するのに使われる。
    <h4>アルゴリズム</h4>
    成功確率を\(p\)とすると、オッズ = (成功確率) / (失敗確率) = \( \frac{p}{1-p} \)となる。
    オッズの対数をとったもの(ロジットという)が入力の線形結合で表せるとする。
    $$
      \ln(\frac{p_i}{1-p_i}) = \alpha + \beta_1 x_{1,i} + \cdots + \beta_k x_{k,i}
    $$
    左辺を\(p_i\)にして整理すると以下の式で表される。
    $$
    p_i = \frac{1}{1 + e^{- ( \alpha + \beta_1 x_{1,i} + \cdots + \beta_k x_{k, i})}}
    $$
    これは単純パーセプトロンの以下の式と等価である。
    $$
    p_i = \varsigma (\alpha + \beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})
    $$
    ここで\(\varsigma\)はシグモイド関数。

    つまり、ロジスティック回帰とは、単純パーセプトロンの出力にシグモイド関数を付けて出力を確率にしたもの。

    <h4>長所</h4>
      <ul>
      <li>非常に簡単</li>
      </ul>
    <h4>短所</h4>
       <ul>
      <li>直線分割しかできない</li>
      </ul>
    </div> </div>

  <h3>naive bayes<span class="small"> : ナイーブベイズ, 単純ベイズ</span></h3>
    <div class="hidden_box">
    <label for="label_naive_bayse">[+]　　　</label>
    <input type="checkbox" id="label_naive_bayse"/>
    <div class="hidden_show">
      <h4>ターゲットタスク</h4>
      分類。特にテキスト分類、推薦、センチメント分析、異常検知。
      <h4>アルゴリズム</h4>
        以下に示すベイズの定理を元にした学習アルゴリズムで、
        ある事象\(y\)が起きた時にある事象\(x_k\)が起きていた確率をそれぞれの\(k\)について求め、
        確率が最大となる\(x_k\)を選択する。
        $$
        p(x_k|y) = \frac{p(y|x_k)p(x_k)}{p(y)}
        $$
        ここで\(p(x)\)は\(x\)が起きる確率、\(p(y|x)\)は\(x\)が起きたときに\(y\)が起きる条件付き確率。
      <h4>長所</h4>
      	<ul>
		      <li>単純</li>
		      <li>高速</li>
		      <li>少ないトレーニングデータでも性能が出る</li>
	      </ul>
      <h4>短所</h4>
      	<ul>
		      <li>各特徴量が独立という強い仮定が必要なので、問題によっては成り立たない。</li>
	      </ul>
    </div> </div>

  <h3>random forest<span class="small"> : ランダムフォレスト</span></h3>
    <div class="hidden_box">
    <label for="label_random_forest">[+]　　　</label>
    <input type="checkbox" id="label_random_forest"/>
    <div class="hidden_show">
    <h4>ターゲットタスク</h4>
    分類、回帰、クラスタリング。

    <h4>アルゴリズム</h4>
      複数の決定木の結果を統合し、最終結果を導く。
    <div>
      <img src="out\ai\random_forest.png" alt="ランダムフォレスト"/>
    </div>

    <h4>決定木のアルゴリズム</h4>
      決定木は各軸(特徴量)において、それぞれの基準により枝を分割する事を繰り返し、ツリーを作る。
    	<div>
      	  <img src="out\ai\decision_tree.png" alt="決定木"/>
    	</div>

    <h4>CART</h4>
      決定木の１つであるCARTについて説明をする。
      CARTはジニ不純度によりノードを２つに分割し、
      （分割前のジニ不純度)-(分割後のジニ不純度の合計)
      が最も高くなるような分割を行う。

    <h4>ジニ不純度</h4>
      $$
        I_G(t) = 1 - \sum^c_{i = 1}(\frac{n_i}{N})^2
      $$
      ここで\(c\)は目的変数のクラス数、\(t\)は現在のノード、
      \(N\)はノード内のデータ数、
      \(n_i\)はクラス\(i\)に属するノード内のデータの数。

    <h4>そのほかの決定木</h4>
      ID3, C4.5, CHAID等がある。分割基準や分岐の数、枝刈りの有無等が異なる。
      分割基準として使われるものとしては、ジニ不純度のほか、エントロピーがある。

    <h4>エントロピー</h4>
      $$
      	I_H(t) = - \sum^c_{i = 1}(\frac{n_i}{N})\log{(\frac{n_i}{N})}
      $$

    <h4>長所</h4>
      	<ul>
		      <li>出力結果の説明が比較的簡単。</li>
		      <li>並列化が容易</li>
	      </ul>
      <h4>短所</h4>
      	<ul>
		      <li>軸に平行な線しか引けないので、対応できない場合がある。</li>
	      </ul>
  </div></div>

  <h3>ada boost<span class="small"> : アダブースト</span></h3>
    <div class="hidden_box">
    <label for="label_boosting">[+]　　　</label>
    <input type="checkbox" id="label_boosting"/>
    <div class="hidden_show">
      <h4>ターゲットタスク</h4>
      二値分類
      <h4>アルゴリズム</h4>
      弱い学習機とデータの重みづけによって強い学習機を作る。
      教師データは1か-1とする。
      <div>
        <img src="out\ai\ada_boost.png" alt="ada boost"/>
      </div>

      <h4>長所</h4>
      	<ul>
		      <li>精度が高い。</li>
	      </ul>
      <h4>短所</h4>
      	<ul>
		      <li>ノイズや異常値に追従しやすい。</li>
		      <li>並列化ができず、計算速度が遅い。</li>
	      </ul>
    </div> </div>

  <h3>least squares method : <span class="small">最小二乗法</span></h3>
    <div class="hidden_box">
    <label for="label_LSM">[+]　　　</label>
    <input type="checkbox" id="label_LSM"/>
    <div class="hidden_show">

    <h4>ターゲットタスク</h4>
    線形回帰
    <h4>アルゴリズム</h4>
    <div>入力の組\(\bm{U}\)、出力の組\(\bm{z}\)が与えられた時、
    \(\bm{z} \simeq \bm{Uw}\)となる係数の組\(\bm{w}\)を、
    残差平方和 \( \|\bm{Uw} - \bm{z}\| \) を最小にするという条件で求める。</div>
    <div>正規方程式 \( \bm{U^{\mathrm{T}}Uw} = \bm{U^\mathrm{T}z} \)により、
    より直接的には以下の式により$\bm{w}$を算出する。</div>
    $$
      \bm{w} = (\bm{U}^{\mathrm{T}}\bm{U})^{-1}\bm{U}^{\mathrm{T}} \bm{z}
    $$

    </div></div>
  
  <h3>k means clustering<span class="small"> : k平均クラスタリング</span></h3>
    <div class="hidden_box">
    <label for="label-k-means-clustering">[+]　　　</label>
    <input type="checkbox" id="label-k-means-clustering"/>
    <div class="hidden_show">
    <h4>ターゲットタスク</h4>
    クラスタリング。
    <h4>アルゴリズム</h4>
    <div>
      <img src="out\ai\k_means_clustering.png" alt="k平均法"/>
    </div>
     <h4>長所</h4>
      	<ul>
		      <li>データ量が膨大でも高速で計算できる。</li>
	      </ul>
      <h4>短所</h4>
      	<ul>
		      <li>結果が初期値に大きく依存する。</li>
		      <li>分割数の決定は別の考察が必要。</li>
	      </ul>

      <h4>エルボー法</h4>
        最適なkを決定するための方法で
        kが大きくなればなるほどSSEは小さくなる傾向にあるが、必要以上に大きくしてもSSEが小さくならないことを利用し、
        各kにおいてSSEを計算し、折れ曲がる値を最適値とみなす手法。
        SSEについては評価の項を参照のこと。

    </div></div>

  <h3>DBSCAN <span class="small"> : density-based spatial clustering of applications with noise</span></h3>
    <div class="hidden_box">
    <label for="label-dbscan">[+]　　　</label>
    <input type="checkbox" id="label-dbscan"/>
    <div class="hidden_show">

    <h4>ターゲットタスク</h4>
    クラスタリング。
    <h4>アルゴリズム</h4>
      <h5>概要</h5>
        以下の方法でコア点、到達可能点、外れ値に分類する。
        <ul>
          <li> 点\(p\)を含め、点\(p\)から距離\(\varepsilon \)以内に少なくともminPts個の点があれば、点\(p\)をコア点とする。</li>
          <li> 点\(q\)がコア点\(p\)から距離から距離\(\varepsilon \)以内にあれば、点\(q\)は点\(p\)から直接到達可能である。</li>
          <li> 点\(p = \) 点\(p_1\)、点\(q\) = 点\(p_n\)の時、点\(p_{i+1}\)が点\(p_i\)から直接到達可能であり、
                 結果点\(p_n\)が点\(p_1\)から順にたどり着ける場合、点\(q\)は点\(p\)から到達可能である。
                 この時、点\(q\)以外はコア点である(\(q\)はコア点か到達可能点である)。</li>
          <li> どの点からもたどり着けない点は外れ点かノイズ点である。</li>
        </ul>
      <h5>手順</h5>
      <ol>
        <li>パラメータ\( \varepsilon \)、minPtsを決定する。</li>
        <li>全ての点について、その点から 距離\(\varepsilon \)以内にある隣接点を見つけ、minPts以上の隣接点がある点をコア点とする。</li>
        <li>隣接グラフ上のコア点群の連結グラフを見つける。この時コア点でない点は考慮しない。</li>
        <li>コア点でない点が近くのクラスタから距離\(\varepsilon \)以内にあれば、その点を近くのクラスタに割当する。そうでない点はノイズ点とする。</li>
      </ol>

    <h4>長所</h4>
      <ul>
		    <li>データのクラスタ数を事前に必要としない。</li>
		    <li>任意の形状のクラスタを見つけることができる。</li>
		    <li>ハズレ値に対しロバストである。</li>
	    </ul>
    <h4>短所</h4>
      <ul>
        <li>データの内容が不明な場合、パラメータ、特に\(\varepsilon\)を設定するのが困難</li>
	    </ul>
    </div></div>


  <h3>PCA<span class="small"> : principal component analysis, 主成分分析 </span></h3>
    <div class="hidden_box">
    <label for="label-pca">[+]　　　</label>
    <input type="checkbox" id="label-pca"/>
    <div class="hidden_show">
    <h4>ターゲットタスク</h4>
    次元削減。
    <h4>アルゴリズム</h4>
    あるデータ\(\bm{X}\)の共分散\(\sum = E[\bm{(X-E[X])(X-E[X])^T}]\)を求める。
    次に\(\sum\)の固有値と固有ベクトルを求め、固有値の大きいほうから必要な分だけ
    主成分を抽出する。
    抽出した主成分の固有ベクトルを並べた行列を元の行列にかけることで、次元を削減する。
    <h4>長所</h4>
      	<ul>
		      <li>計算が早い</li>
		      <li>もとデータの次元が多い場合でも解析できる。</li>
	      </ul>
    <h4>短所</h4>
      	<ul>
		      <li>異なる特徴が混じったデータでは誤った削減を行う。</li>
	      </ul>

    </div></div>

  <h3>ICA<span class="small"> : independent component analysis, 独立成分分析</span></h3>
    <div class="hidden_box">
    <label for="label-ica">[+]　　　</label>
    <input type="checkbox" id="label-ica"/>
    <div class="hidden_show">

    <h4>内容</h4>
      ブラインドソース分離(多変量の信号を複数の加法的な成分に分離する)
      ための計算手法。PCAでは合わさったデータの主成分が出力され、分離できない
      ネゲントロピー(負のエントロピー)を考える
    <h4>応用例</h4>
      MEG、画像の雑音除去、保険データ

    </div></div>

  <h3>matrix factorization<span class="small"> : 行列因子化</span></h3>
    <div class="hidden_box">
    <label for="label-MF">[+]　　　</label>
    <input type="checkbox" id="label-MF"/>
    <div class="hidden_show">

    <h4>NMF<span class="small"> : non-negative matrix factorization, 非負行列因子化</span></h4>
      正の値だけをとるMatrix Factorization。こちらが先に発表され、 改良版としてMatrix Factorizationが発表された。
      行列Xを行列Wと行列Hで近似する方法。
      画像のパーツパーツを抽出できる。

    <h4>matrix factorization<span class="small"> : 行列因子化</span></h4>
      協調フィルタリングとして働く。Netflix映画のおすすめができるようになった。
      目標関数はRSMEで、正則化により過学習を抑制。NMFとあまり変わらない。
      
    </div></div>
    
    
  <h3>t-SNE<span class="small"> : t-stocastic neighbor embedding</span></h3>
    <div class="hidden_box">
    <label for="label-tSNE">[+]　　　</label>
    <input type="checkbox" id="label-tSNE"/>
    <div class="hidden_show">

    t分布を用いた確率的な視覚化手法で、多様体(manifold)の次元圧縮。
    多様体とは、近傍ではユークリッド距離系となっているが、
    全体としてはユークリッド距離系となっていない。
    抽象的に言えば、スイスロール上のデータを広げて２次元にして表示できる。

    </div></div>

  <h3>q-learning<span class="small"> : Q学習</span></h3>
    <div class="hidden_box">
    <label for="label-q-learning">[+]　　　</label>
    <input type="checkbox" id="label-q-learning"/>
    <div class="hidden_show">

      動的で複雑な環境に対する学習で、価値反復と方策反復により学習を行う。
      行動価値観数(Q, A)に基づき、時刻の行動は行動方策に従って選択されるQ(S, A)を更新する。
      <h4>マルコフ決定過程</h4>
        <h5>マルコフ性、マルコフ性</h5>
          一つ前の状態の条件付き確率（一つ前の状態の条件付き確率）。それまでの状態は考慮されない。
        <h5>POMDP</h5>
          部分観測可能なマルコフ決定過程：POMDP
        <h5>マルコフ過程（マルコフ連鎖)</h5>
          状態Sと行為Aと遷移確率Pと報酬Rと(割引率γ)の組 <lt> S, A, P, R, γ <gt>

    </div></div>
  <h3>actor-critic</h3>
    <div class="hidden_box">
    <label for="label-actor_critic">[+]　　　</label>
    <input type="checkbox" id="label-actor_critic"/>
    <div class="hidden_show">

    </div></div>

  <h3>TD(\(\lambda\))</h3>
    <div class="hidden_box">
    <label for="label-tdlambda">[+]　　　</label>
    <input type="checkbox" id="label-tdlambda"/>
    <div class="hidden_show">

    </div></div>

  <h3 id="perceptron">各種法に共通する技術</h3>
    <div class="hidden_box">
    <label for="label-common-techniques">[+]　　　</label>
    <input type="checkbox" id="label-common-techniques"/>
    <div class="hidden_show">

    <h4>正則化</h4>
      回帰問題において、過学習を行った場合に係数が大きくなりがちであることを利用し、
      損失関数にペナルティ項を設定することで、汎化性能の高いモデルの生成を目指す方法。
      <h5>L1正則化</h5>
        あるモデルの評価関数を次式のように設定する。L1正則化の式を使うと、
        説明変数が０になりやすく、次元削減に利用される。
        $$
        \min{f(x)} + \lambda\sum^n_{i=1}|w_i|
        $$
        ここで\(\min{f(x)}\)は通常の損失関数。
        この正則化を利用した回帰をラッソ回帰と呼ぶ。

      <h5>L2正則化</h5>
        モデルの評価関数を以下のように設定する。L2正則化ではモデルの過学習を防ぐために用いられる。
        $$
        \min{f(x)} + \frac{\lambda}{2}\sum^n_{i=1}|w_i|^2
        $$
        この正則化を利用した回帰をリッジ回帰と呼ぶ。

      <h5>Elastic Net回帰</h5>
        L1正則化とL2正則化を組み合わせた回帰手法

    </div></div>


<h2>評価</h2>
  <h3>標準問題</h3>
    <div class="hidden_box">
    <label for="label7-1">[+]　　　</label>
    <input type="checkbox" id="label7-1"/>
    <div class="hidden_show">

    <dl>
      <dt>MNIST(エムニスト)</dt>
      <dd>手書き数字のデータセット、28x28で１チャンネルのデータとなっている</dd>
      <dt>CIFAR-10</dt>
      <dd>10種類のオブジェクトが映った画像の10x10のデータセットの分類</dd>
      <dt>ImageNet(<a href="http : //www.image-net.org">http : //www.image-net.org/</a>)</dt>
      <dd>データが公開されているので登録すればダウンロード可能</dd>
      <dt>MS-COCO(<a href="http : //cocodataset.org/#home">http : //cocodataset.org/#home</a>)</dt>
      <dd>画像に対して５つのキャプションが与えられる。表現の多様性をチェックする</dd>    </dl>
    </div></div>
  <h3>分類の評価</h3>
    <div class="hidden_box">
    <label for="label7-2">[+]　　　</label>
    <input type="checkbox" id="label7-2"/>
    <div class="hidden_show">

    <h4>Confusion Matrix<span class="small">混合行列</span></h4>
    <table>
      <tr>
        <th></th>
        <th>実際は正</th>
        <th>実際は負</th>
      </tr>
      <tr>
        <th>予測が正</th>
        <th>TP<span class="small"> :True Positive</span></th>
        <th>FP<span class="small"> :False Positive、偽陽性、第一種過誤</span></th>
      </tr>
      <tr>
        <th>予測が負</th>
        <th>FN<span class="small"> :False Negative、偽陰性、第二種過誤</span></th>
        <th>TN<span class="small"> :True Negative</span></th>
      </tr>
    </table>

    <h4>評価指標</h4>
      <h5>Accuracy<span class="small"> : 正解率、精度</span></h5>
        全予測の中で正答した割合。
        $$
          \frac{{\rm TP +  TN}}{{\rm TP + TN + FP + FN}}
        $$

      <h5>Recall<span class="small"> : 正常標本精度、再現率、検出率</span></h5>
        実際は正である標本の中で、正と判断した割合。
        正の標本と負の標本の数が違いすぎる場合は役に立たない。
        $$
          \frac{{\rm TP}}{{\rm TP +  FN}}
        $$

      <h5>誤報率</h5>
        実際は正である標本の中で、負と判定した割合。
        $$
          1 - \frac{{\rm TP}}{{\rm TP + FN}}
        $$
      <h5>Precision<span class="small"> : 適合率</span></h5>
        正と予測した中で実際に正である割合。
        $$
          \frac{{\rm TP}}{{\rm TP + FP}}
        $$
  
      <h5>Specificity<span class="small"> : 異常標本精度、特異率、異常網羅率</span></h5>
        実際は負である標本の中で、負と判断した割合。
        $$
          \frac{{\rm TN}}{{\rm FP + TN}}
        $$

      <h5>F-measure<span class="small"> : F値</span></h5>
        適合率と再現率のバランス。
        $$
          \frac{2 \times {\rm Precision \times Recall}}{ {\rm Precision + Recall}}
        $$
    <h4>項目応答理論</h4>
    例えば、被験者の能力に対して、各テスト項目の正答確率がモデル化できるとすると、そのモデルと実際結果を比較し、それぞれの項目が妥当かどうかを確認できる。
    </dl>
    </div></div>

  <h3>回帰の評価</h3>
    <div class="hidden_box">
    <label for="label-evaluation-of-regression">[+]　　　</label>
    <input type="checkbox" id="label-evaluation-of-regression"/>
    <div class="hidden_show">
    <h4>決定係数</h4>
      一般に\(R^2\)で表され、重回帰分析の結果を読み取る指標の一つで１に近いほど分析の精度が高いことを示す。
      0.8程度あれば十分にモデリングできていると言える。
      $$
        R^2 = 1 - \frac{\sum^n_{i=1}(y_i - \hat{y_i})^2}{\sum^n_{i=1}(y_i - \bar{y})^2}
      $$
      ここで、\(y_i\)はi番目の実測値、\(\hat{y_i})\)はi番目の予測値、\(\bar{y}\)は実測値の平均値である。

    <h4>自由度調整済みの決定係数</h4>
      決定係数は説明変数の数を多く取れば取るほど良い値となる傾向があるため、
      説明変数の数を\(p\)、標本数を\(N\)として調整を行った決定係数。
  
      $$
        R^2 = 1 - \frac{\sum^n_{i=1}(y_i - \hat{y_i})^2 / (N - p - 1)}{\sum^n_{i=1}(y_i - \bar{y})^2 / (N - 1)}
      $$

    <h4>バイアスバリアンス分解</h4>
    <dl>
      <dt>バイアス</dt>
      <dd>予測値と真値の誤差、アンダーフィッティング時に大きくなりがち</dd>
      <dt>バリアンス</dt>
      <dd>予測値と真値の分散、オーバーフィッティング時に大きくなりがち</dd>
    </dl>
    </div></div>


  <h3>クラスタリングの評価</h3>
    <div class="hidden_box">
    <label for="label-eval-clustering">[+]　　　</label>
    <input type="checkbox" id="label-eval-clustering"/>
    <div class="hidden_show">

    <h4>SSE<span class="small"> : sum of squared error、クラスタ内誤差平方和</span></h4>
      クラスタ\(i\)のSSEを\(E_i\)は以下の式で表される。
      $$
        E_i = \sum^n_{j=1}(x_j- \bar{x})^2
      $$
      ここで\(x_i\)はクラスタに含まれる点の座標、\(\bar{x}\)はクラスタの重心。
      クラスタリングアルゴリズムの性能は全クラスタのSSEを合わせたものとなり、以下の式で表される。
      $$
        E_total = \sum_i^m{E_i}
      $$
      ここでｍはクラスタ数。

    </div></div>

  <h3>NNの評価</h3>
    <div class="hidden_box">
    <label for="label7-4">[+]　　　</label>
    <input type="checkbox" id="label7-4"/>
    <div class="hidden_show">
    <dl>
      <dt>BLEU (<a href="https : //www.aclweb.org/anthology/P02-1040.pdf">BLEU :  a Method for Automatic Evaluation of Machine Translation </a>)</dt>
      <dd>機械翻訳の指標。過去の単語から次の単語の出現確率を算出するn-gramを改良して使用する。</dd>
      <dt>ROUGE(<a href="https : //www.aclweb.org/anthology/W04-1013.pdf">ROUGE :  A Package for Automatic Evaluation of Summaries </a>)</dt>
      <dd> 自動要約システムのための指標。正しい要約と自動要約システムの中のオーバーラップされた言葉の数の割合を調べる。</dd>
      <dt>METEOR(<a href="https : //www.cs.cmu.edu/~alavie/papers/BanerjeeLavie2005-final.pdf">METEOR :  An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</a>)</dt>
      <dd> 機械翻訳の指標。ユニグラムの正確性と再現性の調和平均を元に計算される。正確性より再現性により重きを置く。</dd>
        <dt>CIDEr (<a href="https : //arxiv.org/pdf/1411.5726.pdf">CIDEr :  Consensus-based Image Description Evaluation</a>)</dt>
      <dd> 画像の自動キャプション付の指標。</dd>
    </dl>
    </div></div>
 
<h2>各分野における応用技術</h2>
  <h3>自然言語処理 <span class="small">NLP, natural language processing</span></h3>
    <div class="hidden_box">
    <label for="label7-3">[+]　　　</label>
    <input type="checkbox" id="label7-3"/>
    <div class="hidden_show">
    <h4>基本用語</h4>
      <h5>トークン</h5>
        自然言語解析における文章の最小単位となる文字や文字列
      <h5>タイプ</h5>
        単語の種類を表す。
      <h5>文章</h5>
        まとまった内容を表す文。
      <h5>文書</h5>
        複数の文書からなるデータ１件分
      <h5>コーパス</h5>
        自然言語の文書を大量に集めたデータで、国立国語研究所などの様々な機関がまとめたデータのほか、青空文庫などもコーパスと言える。
      <h5>シソーラス</h5>
        単語の上位／下位関係、部分／全体関係、同義関係、類義関係などによって、単語を分類し、体系づけた類語辞典、辞書。
      <h5>形態素</h5>
        意味を持つ最小の単位。
      <h5>単語</h5>
        単一または複数の形態素から構成される小さな単位
      <h5>わかち書き</h5>
        文章において語の区切りに空白を挟んで記述すること。
      <h5>表層</h5>
        原文の記述のこと。
      <h5>原形</h5>
        活用する前の記述の事。
      <h5>特徴</h5>
        文章や文書から抽出された情報のこと。
      <h5>辞書</h5>
        単語のリストの事。
      
    <h4>形態素解析 <span class="small">Morphological Analysis</span></h4>
      文法ルールや解析辞書データに基づいて文章を単語に分割し、それぞれに品詞を付与する処理。様々なライブラリが提供されている。
     <dl>
      <dt><a href="http://taku910.github.io/mecab/">MeCab</a></dt>
      <dd>  京都大学情報学研究科−日本電信電話株式会社コミュニケーション科学基礎研究所 共同研究ユニットプロジェクトを通じて開発されたオープンソース形態素解析エンジンで、言語, 辞書,コーパスに依存しない汎用的な設計を 基本方針としている。</dd>
      <dt><a href="http://mocobeta.github.io/janome/">Janome</a></dt>
      <dd> Janome(蛇の目)はpure pythonで書かれた、辞書内包(mecab-ipadic-2.7.0-20070801)の形態素解析器で、依存ライブラリなしでインストールできるpythonライブラリ<dd>
      <dt><a href="https://chasen-legacy.osdn.jp/">Chasen</a></dt>
      <dd> 茶筅システムは、奈良先端科学技術大学院大学情報科学研究科自然言語処理学講座(松本研究室)が、広く自然言語処理研究に資するため無償のソフトウェアとして開発されたもの。</dd>
      <dt><a href="http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN">JUMAN</a></dt>
      <dd>JUMANは、京都大学 大学院情報学研究科 知能情報学専攻 知能メディア講座 言語メディア分野（黒橋・河原・村脇研究室）によって、計算機による日本語の解析の研究を目指す多くの研究者に共通に使える形態素解析ツールを提供するために開発された。</dd>
    </dl>
     
    <h4>Ngram</h4>
      与えられたテキストから、それぞれの語に対して隣接するn個の語の列を作って解析する方法。
      単語単位の場合もある。
      <ul>
              <li>0-gram(ヌルグラム) 事前の単語を考慮しない</li>
              <li>1-gram(ヌ二グラム) 1つ前の単語を考慮する。</li>
              <li>2-gram(ヌ二グラム) 2つ前と2つまえの単語を考慮する。</li>
      </ul>

      統計的言語モデルからニューラルネットワーク言語モデル（リカレント．．．nnlm、rnnlm)

    <h4>BOW <span class="small"> : Bags of Words</span></h4>
      文章中の各単語の分布を記録したベクトルとして表現する。ベクトル表現には以下のようなものがある。
      <h5>カウント表現</h5>
        文章中の各単語の出現数を記録する。

      <h5>バイナリ表現</h5>
        文章中に各単語が存在するかどうかを記録する。

      <h5>tf-idf表現</h5>
        単語の出現頻度である\(tf\)(term frequency)と単語の希少性を示す\(idf\)(inverse document frequency:逆文書頻度)に基づいて計算される。
        多くの文書に出現する一般的な言葉の重要度を下げる。
        $$
          tfidf_{i,j} = tf_{i,j}\cdot idf_i
        $$
        $$
          tf_{i,j} = \frac{n_{i,j}}{\sum_k{n_{k,j}}}
        $$
        $$
          idf_i = \log{\frac{|D|}{|{d: d \ni t_i}|}}
        $$
          ここで、\(n_{i_j}\)は文書\(d_j\)における単語\(t_i\)の出現回数、\(\sum_k{n_{k,j}}\)は文書\(d_j\)における全ての単語の出現回数の和。\(|D|\)は総文書数、\(|{d: d \ni t_i}|\)は単語\(t_i\)を含む文書数。

      <h5>one hot vector表現</h5>
         犬、猫、うさぎのような数値でない値を取る説明変数において、犬:(1,0,0),猫(0,1,0),うさぎ(0,0,1)のような対応する次元が1でその他が0のベクトルで表す。
         次元数は言葉の種類の総数となる。

    <h4>類似度</h4>
      ２つのデータがどれだけ似ているかの指標
      <h5>cos類似度</h5>
        ベクトルがどれだけ近いかを判断する指標
        $$
          cos(\vec{q}, \vec{d}) = \frac{\vec{q}\vec{d}}{|\vec{q}||\vec{d}|} = \frac{\sum_{i=1}q_id_i}{\sqrt{\sum_{i=1}q_i^2}\sqrt{\sum_{i=1}d_i^2}}
        $$
      <h5>Jaccard類似度</h5>
        集合動詞の類似度を判断する指標
      <h5>ピアソンの積率相関係数</h5>
      <q>2つの量的変数間の直線的関連の変数を表す係数。n組のデータ(\(x_1, y_1\)),(\(x_2, y_2\)) \(\cdots\) (\(x_n, y_n\))があり、それぞれの平均を\(\bar{x}, \bar{y}\)とした時、ピアソンの積率相関係数は以下の式で表される。</q>[6]
      $$
        r_{xy} = \frac{{\displaystyle \sum_{i = 1}^n (x_i - \overline{x})
        (y_i - \overline{y})}}{\sqrt{{\displaystyle \sum_{i = 1}^n 
        (x_i - \overline{x})^2}} \sqrt{{\displaystyle \sum_{i = 1}^n 
        (y_i - \overline{y})^2}}} = \frac{s_{xy}}{s_xs_y}
      $$
      <h5>ケンドールの順位相関係数</h5>
        対応する２つの変量\(p_i(x_i, y_i)\)がある時、\(p_s\)と\(p_t\) (\(s \lt t\)) のときの、\(x_s\)と\(x_t\), \(y_s\)と\(y_t\)の大小の向きに関する類似度を計算する相関係数。
      <h5>スピアマンの順位相関係数</h5>
         変量の値をその大きさ順に順位に変換し、あるいは順位しかわかっていないデータの順位により、ピアソンの相関係数を計算したもの。

    <h4>単語文書行列<span class="small">term-document matrix</span></h4>
      文書に出現する単語の頻度を表形式で表したもの。tf, idfにより単語の重みを変えると、重みあり単語文書行列になる。
        
    <h4>トピックモデル</h4>
      文章が複数のトピックで成り立っており、文書中の単語はそれらのトピックにより確率的に生成されていると仮定するモデル。
      <dl>
        <dt>潜在的ディリクレ配分法</dt>
        <dd>
          トピックモデルにおいて、トピック分布にディリクレ分布を仮定し、ベイズ推定する手法。
          ある現象が起きたときに次の現象が起きる確率であるガンマ分布を全ての単語で計算し、
          確率密度関数を作成する。 適応的にパラメータが変わるノンパラメトリックモデルである。
        </dd>
      </dl>
    </div></div>
 
  <h3>画像認識</h3>
    <div class="hidden_box">
    <label for="label-image">[+]　　　</label>
    <input type="checkbox" id="label-image"/>
    <div class="hidden_show">

    <h4>VQ(Vector Quantaization、ベクトル量子化)</h4>
    典型的な画像を組み合わせた画像を作ることができる。

    <h4>Regions with CNN features (R-CNN)</h4>
      画像に対してオーバーラップを許して何が写っているのかを示す。
      バウンディングボックスと呼ばれる画像領域を切り出して、正規化して、CNNに入れる。
      関心領域の切り出し方法は従来の方法
    <h4>fast-RCNN</h4>
      画像の切り出しを１つのネットワークで行い実時間画像処理が可能に。
    <h4>FCN</h4>
      セマンティックセグメンテーション。細かい切り出しができる
    <h4>YOLO (you lok only onece)</h4>
      バウンディングボックスの切り分けと認識が一回でできる。
    <h4>SSD (Single Shot Multibox Detector)</h4>
      バウンディングボックスの切り分けと認識が一回でできる。
    <h4> SegNet</h4>
      実時間での画像領域の細かい切り出しができるようになった。

    <h4>画像キャプション付け</h4>
        画像処理と自然言語処理を組み合わせた問題。
        <h5>NIC</h5>

    </div></div>

  <h3>音声認識</h3>
    <div class="hidden_box">
    <label for="label-sound">[+]　　　</label>
    <input type="checkbox" id="label-sound"/>
    <div class="hidden_show">

    <h4>WaveNet</h4>
      音声認識、音声生成、テキスト読み上げ。
      delayted(スキップを伴う)学習を伴う。
      従来モデルであるparametoricモデルとconcatenativeのモデルと比べると1.5倍ぐらいの点数。

     </div></div>

  <h3>ロボティクス</h3>
    <div class="hidden_box">
    <label for="label-robotics">[+]　　　</label>
    <input type="checkbox" id="label-robotics"/>
    <div class="hidden_show">

    <h4>Competitive Self Play (Open AI)</h4>
      報酬系だけを変えることによって、相撲だけでなく、サッカーのPKなどができるようになる。
      そのスキルを別のスキルに転移することもできる。

    <h4>Alpha Go zero</h4>
      囲碁の定石を学習させないことでより囲碁が強くなった。

    </div></div>

  <h3>異常検知</h3>
    <div class="hidden_box">
    <label for="label-failure-detection">[+]　　　</label>
    <input type="checkbox" id="label-failure-detection"/>
    <div class="hidden_show">
    <h4>異常検知の分類</h4>
      <h5>outliner detection<span class="small"> : 外れ値検出</span></h5>
        他の値の傾向から大きく外れた値の検出。
      <h5>change detection<span class="small"> : 変化検知</span></h5>
        観測値の傾向、振る舞いが変化したタイミングの検知
      <h5>discord discovery<span class="small"> : 異常部位検出</span></h5>
        観測値の振る舞いが他の場所の振る舞いと大きく変化している部位を検知する。

    <h4>外れ値検知手法</h4>
      <h5>ネイマン・ピアソン決定則</h5>
        \(y=1\)の時を異常時の分布、\(y=0\)の時を正常時の分布とした時、以下の式は異常度を示し、
        この値がしきい値を超えた場合に異常であるとみなす決定方法。
        $$
          \ln\frac{p(x' | y = 1, D}{p(x'| y = 0, D)}
        $$
      <h5>ホテリングのT二乗法</h5>
        信頼できるデータの平均値と共分散を求め、マハラビノス距離を計算する。
        マハラビノス距離\(d\)の2乗が一定以上であれば、異常とみなす。マハラビノス距離\(d\)は以下の式で表される。
        $$
          d = \sqrt{\sum^M_{i=1}(\frac{x_i - \hat{u_i}}{\hat{\sigma_i}})^2}
        $$
        ここで\(x_i\)は各軸の値、\(\hat{u_i}\)は各軸の平均値、\(\hat{\sigma_i}\)は各軸の標準偏差である。<br />
        
        また、行列式で表すと以下の式となる。
        $$
          d = \sqrt{(X - \mu)^T{\sum}^{-1}(X - \mu)}
        $$
        ここで、\(X\)は特徴ベクトル、 \(\mu\)は特徴ベクトルの母平均、\({\sum}^{-1}\)は逆共分散行列（精度行列）である。


    </div></div>

  <h3>ネットワーク分析</h3>


<h2>AIの注意点</h2>
  <h3>偏見への問題</h3>
    <div class="hidden_box">
    <label for="label-caution1">[+]　　　</label>
    <input type="checkbox" id="label-caution1"/>
    <div class="hidden_show">
    黒人の女性をゴリラとして認識された例がある。学習データが白人に偏ってしまっている。
    取り込められるデータが偏ると、出力が偏ってしまう。
    </div></div>
  <h3>ブラックボックス</h3>
    <div class="hidden_box">
    <label for="label-caution2">[+]　　　</label>
    <input type="checkbox" id="label-caution2"/>
    <div class="hidden_show">
    パラメータが改ざんされてもわからない、危険な使い方をしていてもわからない可能性がある。
    個人の勉強と専門家を用意するべき。
    </div></div>
  <h3>fooling CNN</h3>
    <div class="hidden_box">
    <label for="label-caution3">[+]　　　</label>
    <input type="checkbox" id="label-caution3"/>
    <div class="hidden_show">
  　画像を少しだけ変えると、認識が変わる例がある。
  　我々の認識とDNNの認識方法は異なっている。
  <h4>One Pixcel Attack</h4>
    元画像に１画素だけ異なる所を付け加えると認識が変わる。
    </div></div>

<h2>その外(未整理)</h2>
    <div class="hidden_box">
    <label for="label-sonohoka">[+]　　　</label>
    <input type="checkbox" id="label-sonohoka"/>
    <div class="hidden_show">

  <dl>
    <dt>怠惰学習</dt>
    <dd>事前にモデルを構築しない学習法</dd>
    <dt>勾配降下法</dt>
    <dd><a href="math/index.html">数学-最適化問題</a>へ移動</dd>

    <dt>Empirical Risk Minimization(ERM, 経験損失最小化)</dt>
   <dd>損失関数とデータについて求めた経験損失を最小化するようにモデルを選ぶ学習の基準</dd>

    <dt>Competitive Learning(競合学習)</dt>
    <dd>
    教師なし学習で頻繁に行われる学習で、最も入力データに反応したニューロンのみ更新したりするようなニューロン同士で競わせるような学習.</dd>
    <dt>NIC</dt>
    <dd>
    写真から何をしているのかを言語化する。
    </dd>
    <dt>識別モデル</dt>
    <dd>
      Xをデータ、Yをラベル(従属変数)とするとXとYの関係を直接記述する。
    </dd>

    <dt>生成モデル</dt>
    <dd>
    クラスに対する条件付き分布をもとめることで、クラスに属するデータがどういうものであるかを知ることができる。
    ここから逆算して、入力に人工的なデータを作り出すことができる。
    ベイズモデルは生成モデルであり、代表的なものとしてはナイーブベイズモデルがある。
    <dt>TPU</dt>
    <dd>

    </dd>

   <dt>minimax法</dt>
    <dd>ゲーム木において、相手は自分にとって最悪手、自分は最善手を選ぶとして最善手を算出する方法</dd>
    <dt>αβ法</dt>
    <dd>minimax法でβカット(自分の手を計算する時に、現在の候補より評価値が低い手のノードを探索しないこと)とαカット(相手の手を計算するときに、現在の候補より評価値の高いノードを探索しないこと)を行う手法</dd>
    <dt>AIC、赤池情報量基準</dt>
    <dd>情報量基準(パラメータで記述されたモデルのクラスからモデルを選択する基準)の１つ。 T*log(s^2)+2K、ここでTは標本の大きさ、s^2はモデル誤差校の分散推定量、Kはモデルに含まれる係数の数</dd>
    <dt>BIC、ベイジアン情報量基準</dt>
    <dd>情報量基準(パラメータで記述されたモデルのクラスからモデルを選択する基準)の１つ。 T*log(s^2)+K*log(T)、 ここでTは標本の大きさ、s^2はモデル誤差校の分散推定量、Kはモデルに含まれる係数の数</dd>

    <dt>ImageDataGenerator(keras)</dt>
    <dd>
      ランダムにデータの反転、圧縮、移動、回転などを行うことができる。
      ミニバッチでデータを作成しながら、学習することで、メモリも少なくてすむ。
    </dd>
  </dl>
  </div></div>

<h2>参考文献、サイト</h2>
<div>
  <ol>
    <li><cite><a href="https : //www.shoeisha.co.jp/book/detail/9784798157559">一般社団法人日本ディープラーニング協会監修, 浅川 伸一, 江間 有沙, 工藤 郁子, 巣龍 悠輔, 瀬谷 啓介, 松井 孝之, 松尾 豊."深層学習教書 ディープラーニング G検定（ジェネラリスト） 公式テキスト", 2018, 翔泳社</li>
    <li><cite><a href="https : //www.shoeisha.co.jp/book/detail/9784798157559">岡谷 貴之."深層学習", 2015, 講談社</li>
    <li><cite><a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B">"マルコフ決定過程", wikipedia</a>
    <li><cite><a href="http://may46onez.hatenablog.com/entry/2016/01/08/142843">gco(id:may46onez)."theanoでLocal Response Normalization(LRN)を使う (備忘録とか日常とか)",2016, Hatena Blog</li>
    <li><cite><a href="https://www.aaai.org/Papers/KDD/1996/KDD96-014.pdf">Usama Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, "Knowledge Discovery and Data Mining: Towards a Unifying Framework",1996, KDD-96 Proceedings</li>
    <li><cite><a href="https://bellcurve.jp/statistics/glossary/1233.html">BellCurve, "ピアソンの積率相関係数", 統計WEB</a>
    <li><cite><a href="https : //daihen.aidemy.jp/">"Aidemy"</li>
  </ol>
</div>

<div class = "end_of_page">
<a class="prev" href="ai/index.html" >前：ai/index</a>
<a class="upper" href="ai/index.html" >上：ai/index</a>
<a class="next" href="ai/neural_network.html">次：深層学習</a>
</div>



</main>
</body>
</html>
