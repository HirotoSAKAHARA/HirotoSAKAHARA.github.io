<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - ai</title>

<link rel="stylesheet" href="../style.css">
<!-- 数式を使う宣言-->
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "AMS" }},
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    },
    "HTML-CSS": { matchFontHeight: false },
    displayAlign: "left",
    displayIndent: "2em"
  });
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->
<div class="title_space">
  <div class="title">
    <a href="../index.html">hsmemo</a>
    <span class="subtitle">
      - AI
    </span>
  </div>
</div>

<div class="last_modified">
<script type="text/javascript"><!--
   document.write('最終更新日:' + document.lastModified);
// --></script>
<!--...........................更新内容............................-->
：回帰と分類などの整理 
</div>

 <div class="navigation">
<ul>
  <li><a href="../index.html">home</a></li>
  <li><a href="../mobileRobot/index.html">移動ロボット</a></li>
  <li><a href="../ai/index.html">AI</a></li>
  <li><a href="../math/index.html">数学</a></li>
  <li><a href="../other/index.html">その他</a></li>
</ul>
</div>

<!--*********************************本文*********************************-->
<h1>AI</h1>

<h2>教師あり学習</h2>

  <h3>回帰</h3>
    <h4>線形回帰</h4>
    <dl>
      <dt>線形回帰</dt>
      <dd>Y = a0 + a1x1 + ... + anxn + e となる関係を示したもの</dd>
      <dt>単回帰</dt>
      <dd>変数が１つの１次式</dd>
      <dt>多項回帰</dt>
      <dd>変数が１つだが、ｎ次式</dd>
      <dt>重回帰</dt>
      <dd>変数が複数</dd>
    </dl>
  
    <ul>
      <li>解析的解法と探索的解法</li>
      <li>線形回帰</li>
      <li>回帰直線</li>
      <li>正規方程式</li>
    </ul>

    <h4>ロジスティック回帰</h4>
    確率やダミー変数について分析、予測するのに使われる。

    <h4>サポートベクターマシン SVC, SVM</h4>

  <h3>分類</h3>
    <h4>最近傍法,k近傍法</h4>
        空間上の点を抽出したとき、隣接の学習済みの点のラベル情報から抽出点のラベル情報を決定する
        関数が想定されていないノンパラメトリックで
        教師データをそのまま丸暗記する怠惰学習(lazy learner)である。
        kの選択や特徴尺度の選択などのパラメータ選択が必要である。
        また、最近傍探索アルゴリズムが計算速度において重要となる

    <h4>ナイーブベイズ</h4>
        
    <h4>boosting, ブースティング</h4>
   　　機械学習のメタアルゴリズムで、ほかの学習アルゴリズムと組み合わせて利用する。adaBoostが有名
       前の分類器の間違いに応じてデータの重みを調整する。
   　　真の分類と若干の相関がある弱い分類器を組み合わせることにより、強い学習機を生成できるかを示す。


<h2>教師なし学習</h2>
  <h3>概要</h3>
    学習データを教えることによって、それぞれの点の特徴の分類を行う。
  <h3>分類(クラスタリング)</h3>
    <h4>Self Organizing Map (SOM, 自己組織化)</h4>
      <dl>
      <dt>マルスバーグの自己組織化</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>コホネンの自己組織化</dt>
      <dd>
        ランダムなベクトルからなる行列を用意する。入力ノード群を用意し、
        そのノードに一番近い点とそのノードの隣接点に暴露することを繰り返すことで分類ができる。
        (結果的に入力ノードが消える場合もある）
      </dd>
      <dt>リンスカーの受容野形成の自己組織化</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>オジャのSOMの自己組織化</dt>
      <dd>
        Todo:内容
      </dd>
      </dl>
    <h4>k means クラスタリング</h4>
  
  <h3>次元圧縮</h3>
    <h4>PCA 主成分分析</h4>
    <h4>Independent component analysis (ICA 独立成分分析)</h4>
    　多変量の信号を複数の加法的な成分に分離するための計算手法。
    <h4>LDA 潜在ディレクリ配置</h4>
    <h4>t-SNE</h4>

<h2>自然言語処理</h2>
  <h3>トピックモデル</h3>
    <dl>
      <dt>潜在的ディリクレ配分法</dt>
      <dd>トピックモデルにおいて、トピック分布にディリクレ分布を仮定し、ベイズ推定する手法のこと</dd>
    </dl>

<h2><a href="CNN.html">Convolutional Neural Network(CNN, 畳み込みニューラルネットワーク)</a></h2>

<h2>ディープラーニング</h2>
  <dl>
    <dt>パーセプトロン手法(単純パーセプトロン)</dt>
    <dd>バイアス +Σ（入力 × 重み）= a を計算し、aが0以上の場合は1、以下の場合は0を出力</dd>
    <dt>誤差伝搬</dt>
    <dd>内容</dd>
    <dt>自己符号化</dt>
    <dd>内容</dd>

    <dt>ドロップアウト</dt>
    <dd>途中の学習を取る</dd>

  </dl>

  <h3>次元の呪い</h4>
  AIC, BIC

  <h3>視覚化</h3>
    <h4>クラスタリング</h4>
    <ul>
      <li>k Nearest Neighbor</li>
    </ul>

<h2>スパースモデリング</h2>

<h2>評価</h2>
  <h3>標準問題</h3>
  <dl>
    <dt>MNIST(エムニスト)</dt>
    <dd>手書き数字のデータセット、28x28で１チャンネルのデータとなっている</dd>
    <dt>CIFAR-10</dt>
    <dd>10種類のオブジェクトが映った画像の10x10のデータセットの分類</dd>
  </dl>

  <h3>分類の種類</h3>
  <dl>
    <dt>正解率, 精度, accuracy</dt>
    <dd>(TP + TN) / (TP + TN + FP + FN)</dd>
    <dt>適合率, precision</dt>
    <dd>
      <div>trueのと予測した中で実際にtrueである割合</div>
      <div>(TP) / (TP + FP)
    </dd>
    <dt>再現率, 検出率, recall</dt>
    <dd>
      <div>実際にtrueの値のうち正であると予測されたもの</div>
      <div> (TP) / (TP + FN)</div>
    </dd>
    <dd>
    <dt>F値, F-measure</dt>
    <dd>
      <div>正解率と再現率のバランス</div>
      <div>(2 * precision * recall) / (precision + recall)</div>
    </dd>
  </dl>
  <h3>バイアスバリアンス分解</h3>
   <dl>
    <dt>バイアス</dt>
    <dd>予測値と真値の誤差、アンダーフィッティング時に大きくなりがち</dd>
    <dt>バリアンス</dt>
    <dd>予測値と真値の分散、オーバーフィッティング時に大きくなりがち</dd>
  </dl>
 
<h2>データクレンジング</h2>
  <h3>水増し</h3>
    学習の際に必要なデータを水増しする。
    <dl>
      <dt>ImageDataGenerator(keras)</dt>
      <dd>
        ランダムにデータの反転、圧縮、移動、回転などを行うことができる。
      </dd>
    </dl>
  <h3>正規化</h3>
    <h4>標準化</h4>
    特徴を平均0、分散1にすることで、特徴ごとのデータ分布を近づける。
    ImageDataGeneratorを使う場合は以下のようになる。
<pre> <code># ジェネレーターの生成
data_generator = ImageDataGenerator(samplewise_center=True, samplewise_std_normalization=True)

# 標準化
g = data_generator.flow(X_train, y_train, shuffle=False)
X_batch, y_batch = g.next() </code> </pre>
    <h4>白色化</h4>
    特徴間の相関をなくすこと。
<pre> <code># ジェネレーターの生成
datagen = ImageDataGenerator(featurewise_center=True, zca_whitening=True)

# 標準化
g = datagen.flow(X_train, y_train, shuffle=False)
X_batch, y_batch = g.next() </code> </pre>

    <dl>
      <dt>バッチ正規化</dt>
      <dd>
      ミニバッチ学習の際にバッチごとに標準化を行うこと。
      Kerasの実装は以下の通り。
<pre> <code> model.add(BatchNormalization()) </code> </pre>

      </dd>
      <dt>主成分分析</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>特異値分解</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>ゼロ位相成分分析</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>局所的応答正規化</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>帯域コントラスト正規化</dt>
      <dd>
        Todo:内容
      </dd>
      <dt>局所的コントラスト正規化</dt>
      <dd>
        Todo:内容
      </dd>
    </dl>



<h2>その外</h2>
  <dl>
    <dt>勾配降下法</dt>
    <dd><a href="../math/index.html">数学-最適化問題</a>へ移動</dd>

    <dt>項目応答理論</dt>
    <dd>例えば、被験者の能力に対して、各テスト項目の正答確率がモデル化できるとすると、そのモデルと実際結果を比較し、それぞれの項目が妥当かどうかを確認できる</dd>

    <dt>Empirical Risk Minimization(ERM, 経験損失最小化)</dt>
    <dd>損失関数とデータについて求めた経験損失を最小化するようにモデルを選ぶ学習の基準</dd>

    <dt>Competitive Learning(競合学習)</dt>
    <dd>
    教師なし学習で頻繁に行われる学習で、最も入力データに反応したニューロンのみ更新したりするようなニューロン同士で競わせるような学習.</dd>
    <dt>転移学習</dt>
    <dd>
      学習済みのモデルを使って新たなモデルの学習を行うこと。VGG16などがある。
    </dd>
  </dl>

<h2>参考</h2>
  <ul>
  <li><cite><a href="http://bookclub.kodansha.co.jp/product?item=0000147655">岩田 具治."トピックモデル、機械学習プロフェッショナルシリーズ".2018,講談社</a> </cite></li>
  </ul>

</main>
</body>
</html>
