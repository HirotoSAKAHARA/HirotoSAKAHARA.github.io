<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - ai - deep learning </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 数式を使う宣言-->
<script>
  window.MathJax = {
    tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
    },
    chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
    },
    options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
    },
    loader: {
load: ['[tex]/noerrors']
    }
  };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>

</head>

<body>
<main>

<!--***************************ヘッダー領域****************************-->

<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 深層学習</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日:' + document.lastModified);
</script>
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>深層学習の基礎</h1>
    <h2>概要</h2>
        <div><div class="hidden_show">
        脳の中の神経細胞を模擬したアルゴリズムで、
        シナプスの結合によりネットワークを形成した人口ニューロンが持つ結合加重ベクトルを学習によりを変化させ、
        様々な問題を解決できるようなモデルを作る。

        </div></div>
    <h2>基本用語</h2>
        <div><div class="hidden_show">
        <h3>Artificial Neural Network(ANN, 人工ニューラルネットワーク)</h3>
            コンピュータの中のニューラルネットワーク, BNNの対比。

        <h3>Biologinal Neural Network(BNN, 生物学的ニューラルネットワーク)</h3>
            人間や動物が生物学的に持っているニューラルネットワーク 。  

        <h3>ニューロン</h3>
            <a class="local" href="03_ai/04_learning_methods/08_simple_perceptron.html">単純パーセプトロン</a>
            の出力に活性化関数と呼ばれる非線形の関数を加え、複雑な問題を解けるように工夫したもの。
            多層パーセプトロンに使用する。


            <div class="img"><img src="out\ai\neuron.png" alt="ニューロン"/></div>

        <h3>活性化関数</h3>
            <h4>英語名、略称、別名など</h4>
                activation function
            <h4>内容</h4>
                各ニューロンの出力は上位層の出力の重み付き和とバイアスと呼ばれる定数に、
                非線形の関数を加えたものとなる。 この非線形の関数が活性化関数であり、
                モデルを非線形化することによって、ニューラルネットワークの表現力を増大させる。
                また、出力ニューロンにおいては、タスクの種類によって設計を考慮する必要がある。

        <h3>多層パーセプトロン</h3>
            <h4>英語名、略称、別名など</h4>
                multi layer perceptron、順伝播型ニューラルネットワーク、Feedforward Newral Network
            <h4>内容</h4>
                ニューロンを組み合わせることで、複雑な問題を解けるよう改良したもの。
                ニューロンを層状に並べたユニットが隣接層間でのみ結合した構造をもち、情報が入力側から出力側へ一方向に伝播する。
                <div class="img"><img src="out\ai\multi_layer_perceptron.png" alt="ニューロン"/></div>

        <h3>深層学習</h3>
            <h4>英語名、略称、別名など</h4>
                deep learning
            <h4>内容</h4>
                多層パーセプトロン等のニューラルネットワークの中で、３層以上のもの。
                入力層\(x\)、1層以上の中間層(隠れ層)\(h\)、出力層\(y\)からなる。
    
        <h3>誤差関数</h3>
            <h4>英語名、略称、別名など</h4>
                error function、
                コスト関数、cost function、
                損失関数、loss function、
                目的関数、objective function
            <h4>内容</h4>
                教師データと出力の差を定量的に示すための関数で、タスクの種類によって使われる関数が変わる。

        <h3>(誤差関数の)勾配</h3>
            $$
            \nabla E := \frac{\partial E}{\partial \bm{w}} = \left[ \frac{\partial E}{\partial w_1},\cdots, \frac{\partial E}{\partial w_M}\right]
            $$


        </div></div>
    <h2>学習の手順</h2>
        <div><div class="hidden_show">
        ニューラルネットワークの学習の手順を以下に示す。
        <ol>
            <li>重みの初期値を決定する。</li>
            <li>入力層にデータを与える</li>
            <li>各ニューロンにおいて、入力と重み、活性化関数から、次の層の入力を算出する。</li>
            <li>2を繰り返し、出力を算出する。</li>
            <li>出力と教師データ、コスト関数から、誤差を算出する。</li>
            <li>誤差と各層の重みから、コスト関数の勾配を見積もる。</li>
            <li>コスト関数の勾配などから、重みを変更する。</li>
            <li>終了の判断し、終了でなければ初めに戻る。</li>
        </ol>


        </div></div>
    <h2>課題</h2>
        <div><div class="hidden_show">
        <h3>the gradient vanishing problem(勾配消失問題)</h3>
            誤差勾配の計算時には、各層の微分が行われ、かつ下層の勾配が上層に影響を与える。
            活性化関数で一般的であったシグモイド関数を使うと、一回の計算で微分値が1/4以下になるので、
            層を増やすと上層での勾配がなくなり、学習が行われない。

        <h3>次元の呪い</h3>
            深層学習は必要以上にパラメータがあることが多いが、
            一般に推定すべきパラメータが多くなると、次元が多くなって、
            パラメータ空間の探索が難しくなり、
            過学習などが起きやすくなること。

    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="03_ai/05_reinforcement_learning/23_td_lambda.html">TD(\(\lambda\))</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="03_ai/06_deep_learning/02_hidden_layer.html">隠れ層の設計</a>
</div>
</main>
</body>
</html>
