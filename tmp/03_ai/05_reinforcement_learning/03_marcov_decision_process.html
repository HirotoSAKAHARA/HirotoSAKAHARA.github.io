<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - ai - reinforcement learning </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 強化学習</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>マルコフ決定過程</h1>  
    <h2>この記事の目的</h2>
        <div><div class="hidden_show">
        マルコフ決定過程について記述し、強化学習の問題設定を理解するのに必要な概念を取得する。


        </div></div>
    <h2>マルコフ過程</h2>
        <div><div class="hidden_show">
        <h3>基本用語</h3>
            <h4>マルコフ性</h4>
                次のステップの状態は現在の状態と行動により確率的に決定され、
                過去の挙動とは無関係であるという性質。 
            <h4>時間ステップ</h4>
                動作の回数は離散化されて時間ステップ\(t \;(\in \mathbb{N})\)と表現される。
            <h4>報酬</h4>
                各時刻の状態と行動によって与えられる値\(r_t\)。
            <h4>エピソード</h4>
                タスクの開始から終了までにかかる時間。

        <h3>マルコフ過程に含まれる要素</h3>
            <h4>状態集合</h4>
                エピソード内で取りうる全ての状態を(\({\rm s_1}, \ldots \))とすると、
                状態集合\(\mathcal{S}\)は以下で示される。
                $$
                \mathcal{S} = \{{\rm s}_1, \ldots, {\rm s}_{|\mathcal{S}|}\}
                $$
                また、時間ステップ\(t\)における状態の実現値を\(s_t\)と示す。
            <h4>行動集合</h4>
                エピソード内で取りうる全ての行動を(\({\rm a_1}, \ldots \))とすると、
                行動集合\(\mathcal{A}\)は以下で示される。
                $$
                \mathcal{A} = \{{\rm a}_1, \ldots, {\rm a}_{|\mathcal{A}|}\}
                $$ 
                状態\({\rm s}\)で取りうる行動集合は\(\mathcal{A}({\rm s})\)と示す。
                また、時間ステップ\(t\)における行動の実現値を\(a_t\)と示す。
            <h4>初期状態分布</h4>
                エピソード開始時点でどの状態になるかを示した確率関数
                $$
                p_{s_0} \;\; \colon \mathcal{S} \rightarrow [0,1] \;\; | \;\; \sum_{s \in \mathcal{S}}p_{s_0}(s) = 1 
                $$

            <h4>状態遷移確率</h4>
            環境中のある状態\(s\)において、エージェントが行動\(a\)をした時の次の状態が\(s'\)となる確率。
                $$
                p_T(s'|s, a) \; \colon \; \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow [0, 1] \;\; | \;\; \sum_{s' \in \mathcal{S}} p_T(s'|s, a) = 1, \forall(s, a) \in \mathcal{S} \times \mathcal{A} 
                $$

            <h4>報酬関数</h4>
            ある状態\(s\)から行動\(a\)を取ったときに
            次の状態が\(s'\)となった場合の報酬を一意に決定する関数。
                $$
                r = g(s, a, s')
                $$

        </div></div>
    <h2>マルコフ決定過程</h2>
        <div><div class="hidden_show">
        <h3>概要</h3>
            マルコフ性を満たす強化学習のこと。最適方策の探索と利用を目標とする。

        <h3>基本用語</h3>    
            <h4>収益</h4>
                <h5>定義</h5>
                最終的に合計でどのぐらいの報酬を得られたかを示す値。以下の式で表される。
                $$
                C_t = \sum_{\tau=0}^\infty \gamma^{\tau} R_{t+1+\tau}
                $$
                ここで\(\gamma\)は割引率、\(R_t\)は時間ステップ\(t\)の収益の確率変数。
                <h5>累積報酬和</h5>
                    時間ステップ\(t\)から一定期間の報酬を合計\((\gamma = 1)\)するのでなく、
                    するのでなく割引率をかけてから合計する方法。
            <h4>価値関数</h4>
                ある方策を行い続けたときの、収益の期待値。
                状態価値関数\(v\)と行動価値関数\(q\)がある。 

            <h4>状態価値</h4>
                ある状態をスタートした時点で、ある方策で今後の行動行い続けたと仮定した場合の報酬の期待値。

            <h4>状態価値関数</h4> 
                状態価値を示す式。以下で表される。
                $$
                V^{\pi}_t(s) = \bm{E}^{\pi}(C_{t+1}|s_t = s)
                $$ 
                ここで、\(C_t\)は時間ステップ\(t\)における報酬、\(\pi\)は方策、\({\bm s}\)は初期状態である。

            <h4>行動価値</h4> 
                ある状態である行動を起こした時にスタートした時点で、
                ある方策で今後の行動行い続けたと仮定した場合の報酬の期待値。

            <h4>行動価値関数</h4> 
                行動価値を示す式。以下で表される。
                $$
                Q^{\pi}_t(s) = \bm{E}^{\pi}(C_{t+1}|s_t = s, a_t = a)
                $$ 
                ここで、\(C_t\)は時間ステップ\(t\)における報酬、\(\pi\)は方策、\({\bm s}\)は初期状態である。 

            <h4>最適価値関数、最適方策</h4> 
                状態価値観数によって、ある方策\(\pi_1\)における状態価値関数\(V^{\pi_1}\)と
                別の方策\(\pi_2\)における状態価値観数\(V^{\pi_2}\)を比較することができる。
                ある状態\(s\)において、
                \(V^*(s) = \max(V^{\pi_1}(s),\ldots, V^{\pi_{|\mathcal{\pi}|}}(s))\)
                を満たす\(V*\)を最適価値関数といい、\(\pi^*\)を最適方策という。 
                最適価値関数には最適状態価値関数と最適行動価値関数がある。
                一般にはベルマン方程式となり、計算するのは非常に難しい。

        <h3>ベルマン方程式</h3>    
            ある時点での状態\(s\)とそこから行動した結果移行する可能性のある状態\(s'\)との間に
            価値関数の関係式が成り立てば良い。という発想で導出された方程式。
            価値関数の再帰構造から、以下の式が得られる。
            $$
            \begin{eqnarray*}
            V^\pi(s) &=& \mathbb{E^\pi}(R_0 + \gamma C_1 | S_0 = s) \\
            &=& \sum_{a in \mathcal{A}} \pi(a | s)( \sum_{s' in \mathcal{S}} P_T(s'|s, a) (g(s, a, s') + \gamma V^\pi (s')
            \end{eqnarray*}
            $$
             $$
            Q^\pi(s, a) = 
            \sum_{s' in \mathcal{S}} (P(s'|s, a)g(s, a, s') + \sum_{a' \in \mathcal{A}} \gamma P(a' | s')Q^\pi(s', a')))

            $$ 
        <h3>モンテカルロ法による状態価値推定</h3>    
            <h4>概要</h4>
                状態\(s\)から出発し、方策\(\pi\)に従い終端状態までエピソードを進めることを何度も繰り返し、
                最終的に得られた報酬を逆算し、現在の価値を取得する。
                方策には一般的に確率変数が入っているため、
                何度も繰り返し、平均値を取ることで偏りを減らす。

            <h4>欠点</h4>
            <ul>
                <li>
                    収益を計算する必要があるため、
                    １つのエピソードが終了するまで学習ができない。
                </li>
                <li>
                    方策が変わるたびに全て学び直しをする必要がある。
                </li>
                <li>
                    計算のためには各状態\(s\)における期待値を保持し続けないといけない。
                </li>
            </ul

        <h3>ベルマン最適方程式</h3>    
            最適状態価値関数、最適行動価値関数もベルマン方程式（状態価値関数、行動価値関数を再帰的な式で表現したもの）と同じく、
            再帰的な式に変形することができる。これらをベルマン最適方程式と呼ぶ。
            $$
             V^*(s) = \max_{a \in \mathcal{A}} sum_{s' in \mathcal{S}} p_t(s' | s, a )(g(s,r,s') + \gamma V*(s))
            $$
            $$
            Q^*(s, a) =  \sum_{s' in \mathcal{S}} p_t(s' | s, a )(g(s,r,s') + \gamma \max_{a \in \mathcal{A}} Q^*(s, a))
            $$  

        </div></div>
    <h2>参考文献、サイト</h2>
        <div><div class="hidden_show">
        <ol>

            <li><a href="http://bookclub.kodansha.co.jp/product?item=0000275420">森村哲郎, "強化学習", 2019, 講談社</a></li>
        </ol>
        </div></div> 
    </div></div> 
    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="03_ai/05_reinforcement_learning/02_multi_armed_bandit.html">多腕バンディット問題</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="03_ai/05_reinforcement_learning/04_dynamic_programming.html">動的計画法</a>
</div>
</main>
</body>
</html>       
