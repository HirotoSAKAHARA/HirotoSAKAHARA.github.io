 <!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - ai - reinforcement learning </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 強化学習</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>TD手法</h1>  
    <h2>この記事の目的</h2>
        <div><div class="hidden_show">
            TD手法について理解し、実際の状態遷移確率がわからない状況において、学習していく方法について理解する。
        </div></div>
    <h2>概要</h2>
        <div><div class="hidden_show">
            TD手法とは、Time Differenceのことで、最終結果をみずに現在の推定値を利用して次の推定値を更新していく方法。
        </div></div>
    <h2>Sarsa</h2>
        <div><div class="hidden_show">
        <h3>概要</h3>
            On-policy、つまり未来の行動が現戦略にしたがって動作すると想定してTD学習を行う方法。
            動作の推定を行い、実際に得られた報酬\(r\)と見積もりによる推定値\(\gamma Q(s', a') - Q(s,a)\)の差を推定値に足すことで、
            推定値の精度を上げていく。


            <h3>アルゴリズム</h3>                                                                                           <ol>
                <li>
                    全ての\(s\)、\(a\)において行動価値関数\(Q(s, a)\を任意に初期化する。
                    ただし終端状態\({\rm terminal state}\)におけるQ(s_{\rm terminal state} = 0\)とする。
                </li>
                <li> 各エピソードにおいて、sが終端状態となるまで以下を繰り返す。
                    <ol>
                        <li>状態\(s\)を初期化する</li>
                        <li>行動価値関数\(Q\)から導かれる方策を利用して状態\(s\)から行動\(a\)を選択する。</li>
                        <li>以下を繰り返す
                            <ol>
                                <li>行動\(a\)を行って遷移した状態\(s'\)と報酬\(r\)を観測する。</li>
                                <li>行動価値関数\(Q\)から導かれる方策を利用して状態\(s'\)から行動\(a'\)を選択する。</li>
                                <li>
                                    $$
                                    Q(s, a) = Q(s, a) + \alpha ( r + \gamma Q(s', a') - Q(s, a))
                                    $$
                                </li>
                                <li>\(s = s', a = a'\)とする。</li>
                            </ol>
                        </li>
                    </ol>
                </li>
            </ol>
            Q関数から導かれる方策とは、例えば\(\epsilon-greedy\)を使うとすると、ある確率でQ関数による決定的方策となり、残りの確率では、ランダムに行動を選択される方策となる。
            

        </div></div>
    <h2>Q学習</h2>
        <div><div class="hidden_show">
        <h3>概要</h3>
            Off-policy、つまり未来の行動が現戦略と関係なく動作すると想定してTD学習を行う方法。
            動作した時に、実際に得られた報酬\(r\)と過去の見積もりをによる最適行動時の推定値\(\gamma \max_{a' \in \mathcal{A}} Q(s', a') - Q(s,a)\)の差を推定値に足すことで、
            推定値の精度を上げていく。 
             <h3>アルゴリズム</h3>                                                                                           <ol>
                <li>
                    全ての\(s\)、\(a\)において行動価値関数\(Q(s, a)\を任意に初期化する。
                    ただし終端状態\({\rm terminal state}\)におけるQ(s_{\rm terminal state} = 0\)とする。
                </li>
                <li> 各エピソードにおいて、sが終端状態となるまで以下を繰り返す。
                    <ol>
                        <li>状態\(s\)を初期化する</li>
                        <li>以下を繰り返す
                            <ol>
                                <li>行動価値関数\(Q\)から導かれる方策を利用して状態\(s\)から行動\(a\)を選択する。</li>
                                <li>行動\(a\)を行って遷移した状態\(s'\)と報酬\(r\)を観測する。</li>
                                <li>
                                    $$
                                    Q(s, a) = Q(s, a) + \alpha ( r + \gamma \max_{a' \in \mathcal{A}} Q(s', a') - Q(s, a))
                                    $$
                                </li>
                                <li>\(s = s'\)とする。</li>
                            </ol>
                        </li>
                    </ol>
                </li>
            </ol>
            Q関数から導かれる方策とは、例えば\(\epsilon-greedy\)を使うとすると、ある確率でQ関数による決定的方策となり、残りの確率では、ランダムに行動を選択される方策となる。


        </div></div>
    <h2>参考文献、サイト</h2>
        <div><div class="hidden_show">
        <ol>
            <li><a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Richard S. Sutton and Andrew G. Barto, "Reinforcement Learning Second Edition An Introduction", 2018, MIT Press </a></li>
            <li><a href="https://aidemy.net">"Aidemy"</a></li>
            <li><a href="https://qiita.com/ABmushi/items/83a639506fcbc4050ce8">@ABmushi, "強化学習のOn-PolicyとOff-Policy", 2018, Qiita</a></li> 
        </ol>
        </div></div> 
    </div></div> 
    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="03_ai/05_reinforcement_learning/04_dynamic_programming.html">動的計画法</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="03_ai/05_reinforcement_learning/22_actor_critic.html">アクタークリティック</a>
</div>
</main>
</body>
</html>
