<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - ai - base </title>
<base href="../../" />

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 人工知能について</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：分類の項にあった記述を移動
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>強化学習</h1>  
    <h2>概要</h2>
        <div><div class="hidden_show">
          九州大学の木村先生のホームページより引用する。『<cite>強化学習とは，試行錯誤を通じて環境に適応する学習制御の枠組である． 教師付き学習(Supervised learning)とは異なり，状態入力に対する正しい行動 出力を明示的に示す教師が存在しない．かわりに報酬というスカラーの情報を 手がかりに学習するが，報酬にはノイズや遅れがある．そのため，行動を実行した直後の報酬をみるだけでは，学習主体はその行動が正しかったかどうかを判断できないという困難を伴う．</cite> 』[1]
          箇条書きにすると以下のような学習の枠組みである。
          <ul>
              <li>学習対象は、試行錯誤的に行動しなければいけない。</li>
              <li>行動に対しては、結果の良し悪しを示す報酬のみが与えられる。</li>
              <li>ただし、報酬は遅れやノイズがあり、行動の直後の報酬を見ただけでは学習ができない。</li>
          </ul>


        </div></div>
    <h2>参考文献、サイト</h2>
        <div><div class="hidden_show">
    <ol>
        <li><a href="http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html">"強化学習とは？(what is Reinforcement Learning?)",木村 元、宮崎 和光、小林 重信</a></li>
    </ol> 
    </div></div> 
    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="03_ai/01_base/03_supervised_and_unsupervised_learning.html">教師あり学習・教師なし学習</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="03_ai/02_preprocessing/01_preprocessing.html">前準備の基礎</a>
</div>
</main>
</body>
</html>
