<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - others - natural language processing </title>
<base href="../">

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : その他 - 自然言語処理</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>深層学習を使った自然言語処理</h1>
    
    <h2>概要</h2>
        <div><div class="hidden_show">
        深層学習を使わないベクトル表現では、ベクトルの次元が大きくなりすぎたり、意味を扱えなかったりするが、
        深層学習を使うと、誤差逆伝播法により、わずか数百次元のベクトルで単語の意味を表現できるようになる。

        </div></div>
    <h2>word2vec</h2>
        <div><div class="hidden_show">
    
  "king" - "man" + "woman" = "Queen" <br>
単語埋め込み(Embedding)モデル。単語のベクトルを用いて、単語の足し算、引き算が可能となる。同じように文法も解ける。
BOWや単語文書行列では、それぞれの単語が0,1の入力層が数万個で表示されていたのが、まとめてベクトルとすると、
ベクトルが学習によって意味を表すようになってきた。
自分以外の周りの単語を予測するスキップグラムとその逆のCBOWで学習させると中間層に意味の表層ができる。

        </div></div>
    <h2>doc2vec</h2>
        <div><div class="hidden_show">
    
  word2vecを応用した文章をベクトル化する技術で、文の語順も特徴として考慮に入れることができる。

        </div></div>
    <h2>Sequence-to-Sequence <span class="small">: Encoder-Decoder、系列変換モデル</span></h2>
        <div><div class="hidden_show">
    
  ソースとターゲットで２つのRNN(LSTM)を使用する手法
ソースに系列（文章など)を入れ、ソース側で双方向リカレントニューラルネットワークを使用して、
束ねてベクトル情報に圧縮してターゲットに入力し、ターゲット側で新たな系列を生成する手法。

ソースからの複数の出力のうちのどれを使うのかをattentionとして出力側が決定する。
中間層の一工夫として、入力系列(文章)は逆順に入れると精度が上がることが報告されている。

        </div></div>
    <h2>Attention Mechanism <span class="small">注意機構</span></h2>
        <div><div class="hidden_show">
    
  Sequence-to-Sequenceモデルの２つのRNN間の情報伝達を改善する手法で、長いネットワークにおいて、
最初に入力された情報が復号化器まで伝播しづらくなるのを解決するために、直接的に入力情報を出力時に利用する。
具体的には、各単語に対して、各時刻で出力されたベクトルの重み付き平均を計算し、それを利用する。


    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="html/topic_model.html">トピックモデル</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="html/preprocessing_for_natural_language_processing.html">自然言語処理におけるデータ前処理</a>
</div>
</main>
</body>
</html>
