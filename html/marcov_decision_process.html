 <!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>hsmemo - - ai - reinforcement learning </title>
<base href="../">

<link rel="stylesheet" href="style.css">
<!-- 式・使う宣言-->
<script>
    window.MathJax = {
tex: {
macros: {
x: "{\\times}",
bm: ["{\\boldsymbol{#1}}",1],
dd: ["{\\frac{\\partial #1}{\\partial #2}}",2]
},
inlineMath: [['$','$'], ['\\(','\\)']],
processEscapes: true,
tags: "ams",
autoload: {
color: [],
colorV2: ['color']
},
packages: {'[+]': ['noerrors']}
},
chtml: {
matchFontHeight: false,
displayAlign: "left",
displayIndent: "2em"
},
options: {
renderActions: {
/* add a new named action to render <script type="math/tex"> */
find_script_mathtex: [10, function (doc) {
for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
const display = !!node.type.match(/; *mode=display/);
const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
const text = document.createTextNode('');
node.parentNode.replaceChild(text, node);
math.start = {node: text, delim: '', n: 0};
math.end = {node: text, delim: '', n: 0};
doc.math.push(math);
}
}, '']
}
},
loader: {
load: ['[tex]/noerrors']
}
    };
</script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script">
</script>
</head>
<body>
<main>


<!--***************************ヘッダー領域****************************-->
<div class="site-header">
<div class="title_space">
  <div class="title">
    <a href="index.html">hsmemo</a>
    <span class="subtitle"> : 人工知能 - 強化学習</span>
  </div>

</div>

<div class="last_modified">
<script type="text/javascript">
   document.write('最終更新日 : ' + document.lastModified);
</script>
<!--...........................更新内容............................-->
：分類の項にあった記述を移動
</div>
</div>

<div class=site-header-margin> </div>
<!--*********************************本文*********************************-->
<h1>マルコフ決定過程</h1>  
    <h2>この記事の目的</h2>
        <div><div class="hidden_show">
       強化学習の基礎である確率過程などを通し、逐次意思決定の問題設定の典型例を紹介する。
       k
        </div></div>
    <h2>概要</h2>
        <div><div class="hidden_show">
    
        エージェントが行動し、行動によって環境が変化し、
        環境から状態と報酬が与えられるようなシナリオで、
        累積報酬の最大化を達成できるようなポリシーを学習することを目的とする。
        行動の結果が行動の後にスカラ値の報酬として与えられる。
        報酬は即時に与えられるとは限らず、報酬は行動の改善の方法を与えない。
        英語ではreinforcement learning(RL)。
        <div class="img"><img src="out\ai\reinforcement_learning.png" alt="学習手順"/></div>
        </div></div>
    <h2>特徴</h2>
        <div><div class="hidden_show">
        エージェントが探索し、探索の後で報酬が与えられるということはほかの学習と大きく異なる。
        行動により環境が変化するために、探索と活用のトレードオフが発生する。
        </div></div>
    <h2>MDP <span class="small">: Marcov Decision Process、マルコフ決定過程</span></h2>
        <div><div class="hidden_show">
    
        状態の組と、行動の組、初期状態が各状態での遷移確率とある状態からある状態に遷移したときの報酬の期待値が定義されている状態で、エージェントがとるポリシーを決定する。強化学習は、マルコフ決定過程におけるポリシーの最適化問題と言える。(下図はwikipediaの図：<a href="https://ja.wikipedia.org/wiki/%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8B#/media/%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB:Markov_Decision_Process_example.png">３つの状態と２つの行動を持つ簡単なMDPの例</a>をplantumlで書いたもの)
        <div class="img"><img src="out\ai\marcov_decision_process.png" alt="マルコフ決定過程"/></div>

        </div></div>
    <h2>基本用語</h2>
        <div><div class="hidden_show">
    
        <h3>エージェント</h3>
            制御器や意思決定者</dd>

        <h3>環境</h3>
            制御対称システム</dd>

        <h3>行動</h3>
            エージェントが環境を変化させる行為

        <h3>報酬</h3>
            その行動が良かったか悪かったかを示すスカラ値。
            行動への具体的なヒントにならない。
            行動の行った後、遅れて示される。

        <h3>状態</h3> 
            環境が現在どうなっているかを表すパラメータ。

        <h3>ポリシー</h3> 
            報酬の期待値を最大にするような逐次的意思決定ルール。

        <h3>状態価値関数</h3> 
            ある状態から、あるポリシーで行動を続けた時の報酬の総和。

        <h3>行動価値関数</h3> 
            ある状態から、ある行動をとった後、
            あるポリシーで行動を続けた時の報酬の総和。

        <h3>価値関数</h3>
            あるポリシーを行ったときの、報酬の期待値。
            状態価値関数vと行動価値関数qがある。

        <h3>最適価値関数</h3>
            価値関数の中で最大値となるもの。
            一般にはベルマン方程式となり、計算するのは非常に難しい。

        <h3>収益</h3>
            即時報酬＋Σ(割引率*未来の報酬)。

        <h3>TD学習</h3>
            モンテカルロ法と動的計画法を合わせた方法で、経験的に学習を行いつつ、
            途中で評価値を更新していく学習手法。Q学習やActor-Critic法、TD(\(\lambda\))法はこの学習法の一つ。

        <h3>探索と利用のジレンマ</h3>
            ある行動が最適かどうかは別の行動をしないとわからないが、
            最良の行動と決定している別の行動ばかりしていると良い行動を選べない。

    </div></div>
<div class="end_of_page_margin"></div>
<div class="end_of_page">
<a class="prev" href="html/multi_armed_bandit.html">多腕バンディット問題</a>
<a class="upper" href="index.html">上：ホーム</a>
<a class="next" href="html/q_learning.html">Q学習</a>
</div>
</main>
</body>
</html>       
